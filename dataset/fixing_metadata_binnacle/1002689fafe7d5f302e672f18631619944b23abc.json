{"seed":2469397708,"processedDockerfileHash":"bb154734e0b2e7808bf12f08c6d3a4c5","fixedSmells":["use-no-install-recommends","do-not-use-apt-get-update-alone","pin-package-manager-versions-apt-get","pin-package-manager-versions-pip","pin-package-manager-versions-npm","pin-package-manager-versions-gem","pin-package-manager-versions-apk","use-copy-instead-of-add","use-wget-instead-of-add","do-not-have-secrets","have-a-healthcheck","have-a-user"],"successfullyFixedSmells":["do-not-use-apt-get-update-alone","pin-package-manager-versions-apt-get","have-a-healthcheck"],"processedDockerfile":"#\n#   Licensed to the Apache Software Foundation (ASF) under one or more\n#   contributor license agreements.  See the NOTICE file distributed with\n#   this work for additional information regarding copyright ownership.\n#   The ASF licenses this file to You under the Apache License, Version 2.0\n#   (the \"License\"); you may not use this file except in compliance with\n#   the License.  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#   Unless required by applicable law or agreed to in writing, software\n#   distributed under the License is distributed on an \"AS IS\" BASIS,\n#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#   See the License for the specific language governing permissions and\n#   limitations under the License.\n#\n#   WARNING: THIS DOCKERFILE IS NOT INTENDED FOR PRODUCTION USE OR DEPLOYMENT.\n#\n#   Base image for the whole Docker file\nARG APT_DEPS_IMAGE=\"airflow-apt-deps\"\nARG PYTHON_BASE_IMAGE=\"python:3.6-slim\"\n#  ###########################################################################################################\n#   This is the base image with APT dependencies needed by Airflow. It is based on a python slim image\n#   Parameters:\n#      PYTHON_BASE_IMAGE - base python image (python:x.y-slim)\n#  ###########################################################################################################\nFROM ${PYTHON_BASE_IMAGE} AS airflow-apt-deps\nSHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-e\", \"-u\", \"-x\", \"-c\"]\n#   Need to repeat the empty argument here otherwise it will not be set for this stage\n#   But the default value carries from the one set before FROM\nARG PYTHON_BASE_IMAGE\nENV PYTHON_BASE_IMAGE=\"${PYTHON_BASE_IMAGE}\"\nARG AIRFLOW_VERSION=\"2.0.0.dev0\"\nENV AIRFLOW_VERSION=\"$AIRFLOW_VERSION\"\n#   Print versions\nRUN echo \"Base image: ${PYTHON_BASE_IMAGE}\"\nRUN echo \"Airflow version: ${AIRFLOW_VERSION}\"\n#   Make sure noninteractie debian install is used and language variables set\nENV DEBIAN_FRONTEND=\"noninteractive\" \\\n    LANGUAGE=\"C.UTF-8\" \\\n    LANG=\"C.UTF-8\" \\\n    LC_ALL=\"C.UTF-8\" \\\n    LC_CTYPE=\"C.UTF-8\" \\\n    LC_MESSAGES=\"C.UTF-8\"\n#   By increasing this number we can do force build of all dependencies\nARG DEPENDENCIES_EPOCH_NUMBER=\"1\"\n#   Increase the value below to force renstalling of all dependencies\nENV DEPENDENCIES_EPOCH_NUMBER=\"${DEPENDENCIES_EPOCH_NUMBER}\"\n#   Install curl and gnupg2 - needed to download nodejs in the next step\nRUN apt-get update \\\n && (apt-get update ;apt-get install --no-install-recommends curl=7.88.1-7ubuntu1 gnupg2=2.2.40-1ubuntu2 -y ) \\\n && apt-get autoremove -yqq --purge \\\n && apt-get clean \\\n && rm -rf /var/lib/apt/lists/*\n#   Install basic apt dependencies\nRUN curl -sL https://deb.nodesource.com/setup_10.x | bash - \\\n && apt-get update \\\n && (apt-get update ;apt-get install --no-install-recommends apt-utils=2.6.0 build-essential=12.9ubuntu3 curl=7.88.1-7ubuntu1 dirmngr=2.2.40-1ubuntu2 freetds-bin=1.3.17+ds-2 freetds-dev=1.3.17+ds-2 git=1:2.39.2-1ubuntu1 gosu=1.14-1 libffi-dev=3.4.4-1 libkrb5-dev=1.20.1-1build1 libpq-dev=15.2-1 libsasl2-2=2.1.28+dfsg-10 libsasl2-dev=2.1.28+dfsg-10 libsasl2-modules=2.1.28+dfsg-10 libssl-dev=3.0.8-1ubuntu1 locales=2.37-0ubuntu2 netcat nodejs=18.13.0+dfsg1-1ubuntu2 rsync=3.2.7-1 sasl2-bin=2.1.28+dfsg-10 sudo=1.9.13p1-1ubuntu2 -y ) \\\n && apt-get autoremove -yqq --purge \\\n && apt-get clean \\\n && rm -rf /var/lib/apt/lists/*\n#   Install MySQL client from Oracle repositories (Debian installs mariadb)\nRUN KEY=\"A4A9406876FCBD3C456770C88C718D3B5072E1F5\" \\\n && GNUPGHOME=\"$( mktemp -d ;)\" \\\n && export GNUPGHOME \\\n && for KEYSERVER in $( shuf -e ha.pool.sks-keyservers.net hkp://p80.pool.sks-keyservers.net:80 keyserver.ubuntu.com hkp://keyserver.ubuntu.com:80 pgp.mit.edu ;); do gpg --keyserver \"${KEYSERVER}\" --recv-keys \"${KEY}\" \\\n && break || true ; done \\\n && gpg --export \"${KEY}\" | apt-key add - \\\n && gpgconf --kill all rm -rf \"${GNUPGHOME}\" ; apt-key list > /dev/null \\\n && echo \"deb http://repo.mysql.com/apt/ubuntu/ trusty mysql-5.6\" | tee -a /etc/apt/sources.list.d/mysql.list \\\n && apt-get update \\\n && (apt-get update ;apt-get install --no-install-recommends libmysqlclient-dev=8.0.32-0ubuntu4 mysql-client=8.0.32-0ubuntu4 -y ) \\\n && apt-get autoremove -yqq --purge \\\n && apt-get clean \\\n && rm -rf /var/lib/apt/lists/*\nRUN adduser airflow \\\n && echo \"airflow ALL=(ALL) NOPASSWD: ALL\" > /etc/sudoers.d/airflow \\\n && chmod 0440 /etc/sudoers.d/airflow\n#  ###########################################################################################################\n#   This is an image with all APT dependencies needed by CI. It is built on top of the airlfow APT image\n#   Parameters:\n#       airflow-apt-deps - this is the base image for CI deps image.\n#  ###########################################################################################################\nFROM airflow-apt-deps AS airflow-ci-apt-deps\nSHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-e\", \"-u\", \"-x\", \"-c\"]\nENV JAVA_HOME=\"/usr/lib/jvm/java-8-openjdk-amd64/\"\nARG APT_DEPS_IMAGE\nENV APT_DEPS_IMAGE=\"${APT_DEPS_IMAGE}\"\nRUN echo \"${APT_DEPS_IMAGE}\"\n#   Note the ifs below might be removed if Buildkit will become usable. It should skip building this\n#   image automatically if it is not used. For now we still go through all layers below but they are empty\nRUN if [[ \"${APT_DEPS_IMAGE}\" == \"airflow-ci-apt-deps\" ]] ; then mkdir -pv /usr/share/man/man1 \\\n && mkdir -pv /usr/share/man/man7 \\\n && apt-get update \\\n && (apt-get update ;apt-get install --no-install-recommends gnupg=2.2.40-1ubuntu2 krb5-user=1.20.1-1build1 ldap-utils=2.6.3+dfsg-1~exp1ubuntu2 less=590-1.2 lsb-release=12.0-1ubuntu1 net-tools=2.10-0.1ubuntu3 openjdk-8-jdk=8u362-ga-0ubuntu2 openssh-client=1:9.0p1-1ubuntu8 openssh-server=1:9.0p1-1ubuntu8 postgresql-client=15+248 python-selinux sqlite3=3.40.1-1 tmux=3.3a-3 unzip=6.0-27ubuntu1 vim=2:9.0.1000-4ubuntu2 -y ) \\\n && apt-get autoremove -yqq --purge \\\n && apt-get clean \\\n && rm -rf /var/lib/apt/lists/* ; fi\nENV HADOOP_DISTRO=\"cdh\" \\\n    HADOOP_MAJOR=\"5\" \\\n    HADOOP_DISTRO_VERSION=\"5.11.0\" \\\n    HADOOP_VERSION=\"2.6.0\" \\\n    HIVE_VERSION=\"1.1.0\"\nENV HADOOP_URL=\"https://archive.cloudera.com/${HADOOP_DISTRO}${HADOOP_MAJOR}/${HADOOP_DISTRO}/${HADOOP_MAJOR}/\"\nENV HADOOP_HOME=\"/tmp/hadoop-cdh\" \\\n    HIVE_HOME=\"/tmp/hive\"\nRUN if [[ \"${APT_DEPS_IMAGE}\" == \"airflow-ci-apt-deps\" ]] ; then mkdir -pv ${HADOOP_HOME} \\\n && mkdir -pv ${HIVE_HOME} \\\n && mkdir /tmp/minicluster \\\n && mkdir -pv /user/hive/warehouse \\\n && chmod -R 777 ${HIVE_HOME} \\\n && chmod -R 777 /user/ ; fi\n#   Install Hadoop\n#   --absolute-names is a work around to avoid this issue https://github.com/docker/hub-feedback/issues/727\nRUN if [[ \"${APT_DEPS_IMAGE}\" == \"airflow-ci-apt-deps\" ]] ; then HADOOP_URL=${HADOOP_URL}hadoop-${HADOOP_VERSION}-${HADOOP_DISTRO}${HADOOP_DISTRO_VERSION}.tar.gz \\\n && HADOOP_TMP_FILE=/tmp/hadoop.tar.gz \\\n && curl -sL ${HADOOP_URL} > ${HADOOP_TMP_FILE} \\\n && tar xzf ${HADOOP_TMP_FILE} --absolute-names --strip-components 1 -C ${HADOOP_HOME} \\\n && rm ${HADOOP_TMP_FILE} ; fi\n#   Install Hive\nRUN if [[ \"${APT_DEPS_IMAGE}\" == \"airflow-ci-apt-deps\" ]] ; then HIVE_URL=${HADOOP_URL}hive-${HIVE_VERSION}-${HADOOP_DISTRO}${HADOOP_DISTRO_VERSION}.tar.gz \\\n && HIVE_TMP_FILE=/tmp/hive.tar.gz \\\n && curl -sL ${HIVE_URL} > ${HIVE_TMP_FILE} \\\n && tar xzf ${HIVE_TMP_FILE} --strip-components 1 -C ${HIVE_HOME} \\\n && rm ${HIVE_TMP_FILE} ; fi\nENV MINICLUSTER_URL=\"https://github.com/bolkedebruin/minicluster/releases/download/\"\nENV MINICLUSTER_VER=\"1.1\"\n#   Install MiniCluster TODO: install it differently. Installing to /tmp is probably a bad idea\nRUN if [[ \"${APT_DEPS_IMAGE}\" == \"airflow-ci-apt-deps\" ]] ; then MINICLUSTER_URL=${MINICLUSTER_URL}${MINICLUSTER_VER}/minicluster-${MINICLUSTER_VER}-SNAPSHOT-bin.zip \\\n && MINICLUSTER_TMP_FILE=/tmp/minicluster.zip \\\n && curl -sL ${MINICLUSTER_URL} > ${MINICLUSTER_TMP_FILE} \\\n && unzip ${MINICLUSTER_TMP_FILE} -d /tmp \\\n && rm ${MINICLUSTER_TMP_FILE} ; fi\nENV PATH=\"\\\"${PATH}:/tmp/hive/bin\\\"\"\n#  ###########################################################################################################\n#   This is the target image - it installs PIP and NPM dependencies including efficient caching\n#   mechanisms - it might be used to build the bare airflow build or CI build\n#   Parameters:\n#      APT_DEPS_IMAGE - image with APT dependencies. It might either be base deps image with airflow\n#                       dependencies or CI deps image that contains also CI-required dependencies\n#  ###########################################################################################################\nFROM ${APT_DEPS_IMAGE} AS main\nSHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-e\", \"-u\", \"-x\", \"-c\"]\nWORKDIR /opt/airflow\nRUN echo \"Airflow version: ${AIRFLOW_VERSION}\"\nARG APT_DEPS_IMAGE\nENV APT_DEPS_IMAGE=\"${APT_DEPS_IMAGE}\"\nARG AIRFLOW_USER=airflow\nENV AIRFLOW_USER=\"${AIRFLOW_USER}\"\nARG HOME=/home/airflow\nENV HOME=\"${HOME}\"\nARG AIRFLOW_HOME=${HOME}/airflow\nENV AIRFLOW_HOME=\"${AIRFLOW_HOME}\"\nARG AIRFLOW_SOURCES=/opt/airflow\nENV AIRFLOW_SOURCES=\"${AIRFLOW_SOURCES}\"\nRUN mkdir -pv ${AIRFLOW_HOME} mkdir -pv ${AIRFLOW_HOME}/dags mkdir -pv ${AIRFLOW_HOME}/logs \\\n && chown -R ${AIRFLOW_USER}.${AIRFLOW_USER} ${AIRFLOW_HOME}\n#   Increase the value here to force reinstalling Apache Airflow pip dependencies\nARG PIP_DEPENDENCIES_EPOCH_NUMBER=\"1\"\nENV PIP_DEPENDENCIES_EPOCH_NUMBER=\"${PIP_DEPENDENCIES_EPOCH_NUMBER}\"\n#   Optimizing installation of Cassandra driver\n#   Speeds up building the image - cassandra driver without CYTHON saves around 10 minutes\nARG CASS_DRIVER_NO_CYTHON=\"1\"\n#   Build cassandra driver on multiple CPUs\nARG CASS_DRIVER_BUILD_CONCURRENCY=\"8\"\nENV CASS_DRIVER_BUILD_CONCURRENCY=\"${CASS_DRIVER_BUILD_CONCURRENCY}\"\nENV CASS_DRIVER_NO_CYTHON=\"${CASS_DRIVER_NO_CYTHON}\"\n#   By default PIP install run without cache to make image smaller\nARG PIP_NO_CACHE_DIR=\"true\"\nENV PIP_NO_CACHE_DIR=\"${PIP_NO_CACHE_DIR}\"\nRUN echo \"Pip no cache dir: ${PIP_NO_CACHE_DIR}\"\n#   PIP version used to install dependencies\nARG PIP_VERSION=\"19.0.1\"\nENV PIP_VERSION=\"${PIP_VERSION}\"\nRUN echo \"Pip version: ${PIP_VERSION}\"\nRUN pip install pip==${PIP_VERSION} --upgrade\n#   We are copying everything with airflow:airflow user:group even if we use root to run the scripts\n#   This is fine as root user will be able to use those dirs anyway.\n#   Airflow sources change frequently but dependency configuration won't change that often\n#   We copy setup.py and other files needed to perform setup of dependencies\n#   This way cache here will only be invalidated if any of the\n#   version/setup configuration change but not when airflow sources change\nCOPY --chown=airflow:airflow setup.py ${AIRFLOW_SOURCES}/setup.py\nCOPY --chown=airflow:airflow setup.cfg ${AIRFLOW_SOURCES}/setup.cfg\nCOPY --chown=airflow:airflow airflow/version.py ${AIRFLOW_SOURCES}/airflow/version.py\nCOPY --chown=airflow:airflow airflow/__init__.py ${AIRFLOW_SOURCES}/airflow/__init__.py\nCOPY --chown=airflow:airflow airflow/bin/airflow ${AIRFLOW_SOURCES}/airflow/bin/airflow\n#   Airflow Extras installed\nARG AIRFLOW_EXTRAS=\"all\"\nENV AIRFLOW_EXTRAS=\"${AIRFLOW_EXTRAS}\"\nRUN echo \"Installing with extras: ${AIRFLOW_EXTRAS}.\"\n#   First install only dependencies but no Apache Airflow itself\n#   This way regular changes in sources of Airflow will not trigger reinstallation of all dependencies\n#   And this Docker layer will be reused between builds.\nRUN pip install --no-use-pep517 -e \".[${AIRFLOW_EXTRAS}]\"\nCOPY --chown=airflow:airflow airflow/www/package.json ${AIRFLOW_SOURCES}/airflow/www/package.json\nCOPY --chown=airflow:airflow airflow/www/package-lock.json ${AIRFLOW_SOURCES}/airflow/www/package-lock.json\nWORKDIR ${AIRFLOW_SOURCES}/airflow/www\n#   Install necessary NPM dependencies (triggered by changes in package-lock.json)\nRUN gosu ${AIRFLOW_USER} npm ci\nCOPY --chown=airflow:airflow airflow/www/ ${AIRFLOW_SOURCES}/airflow/www/\n#   Package NPM for production\nRUN gosu ${AIRFLOW_USER} npm run prod\n#   Always apt-get update/upgrade here to get latest dependencies before\n#   we redo pip install\nRUN : \\\n && apt-get upgrade -y --no-install-recommends \\\n && apt-get clean \\\n && rm -rf /var/lib/apt/lists/*\n#   Cache for this line will be automatically invalidated if any\n#   of airflow sources change\nCOPY --chown=airflow:airflow . ${AIRFLOW_SOURCES}/\nWORKDIR ${AIRFLOW_SOURCES}\n#   Always add-get update/upgrade here to get latest dependencies before\n#   we redo pip install\nRUN : \\\n && apt-get upgrade -y --no-install-recommends \\\n && apt-get clean \\\n && rm -rf /var/lib/apt/lists/*\n#   Additional python deps to install\nARG ADDITIONAL_PYTHON_DEPS=\"\"\nRUN if [[ -n \"${ADDITIONAL_PYTHON_DEPS}\" ]] ; then pip install ${ADDITIONAL_PYTHON_DEPS} ; fi\nCOPY --chown=airflow:airflow ./scripts/docker/entrypoint.sh /entrypoint.sh\nUSER ${AIRFLOW_USER}\nWORKDIR ${AIRFLOW_SOURCES}\nENV PATH=\"${HOME}:${PATH}\"\nEXPOSE 8080/tcp\nENTRYPOINT [\"/usr/local/bin/dumb-init\", \"--\", \"/entrypoint.sh\"]\nCMD [\"--help\"]\n# Please add your HEALTHCHECK here!!!\n","originalDockerfile":"#\n#  Licensed to the Apache Software Foundation (ASF) under one or more\n#  contributor license agreements.  See the NOTICE file distributed with\n#  this work for additional information regarding copyright ownership.\n#  The ASF licenses this file to You under the Apache License, Version 2.0\n#  (the \"License\"); you may not use this file except in compliance with\n#  the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\n#  WARNING: THIS DOCKERFILE IS NOT INTENDED FOR PRODUCTION USE OR DEPLOYMENT.\n#\n#  Base image for the whole Docker file\nARG APT_DEPS_IMAGE=\"airflow-apt-deps\"\nARG PYTHON_BASE_IMAGE=\"python:3.6-slim\"\n# ###########################################################################################################\n#  This is the base image with APT dependencies needed by Airflow. It is based on a python slim image\n#  Parameters:\n#     PYTHON_BASE_IMAGE - base python image (python:x.y-slim)\n# ###########################################################################################################\nFROM ${PYTHON_BASE_IMAGE} AS airflow-apt-deps\nSHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-e\", \"-u\", \"-x\", \"-c\"]\n#  Need to repeat the empty argument here otherwise it will not be set for this stage\n#  But the default value carries from the one set before FROM\nARG PYTHON_BASE_IMAGE\nENV PYTHON_BASE_IMAGE=\"${PYTHON_BASE_IMAGE}\"\nARG AIRFLOW_VERSION=\"2.0.0.dev0\"\nENV AIRFLOW_VERSION=\"$AIRFLOW_VERSION\"\n#  Print versions\nRUN echo \"Base image: ${PYTHON_BASE_IMAGE}\"\nRUN echo \"Airflow version: ${AIRFLOW_VERSION}\"\n#  Make sure noninteractie debian install is used and language variables set\nENV DEBIAN_FRONTEND=\"noninteractive\" \\\n    LANGUAGE=\"C.UTF-8\" \\\n    LANG=\"C.UTF-8\" \\\n    LC_ALL=\"C.UTF-8\" \\\n    LC_CTYPE=\"C.UTF-8\" \\\n    LC_MESSAGES=\"C.UTF-8\"\n#  By increasing this number we can do force build of all dependencies\nARG DEPENDENCIES_EPOCH_NUMBER=\"1\"\n#  Increase the value below to force renstalling of all dependencies\nENV DEPENDENCIES_EPOCH_NUMBER=\"${DEPENDENCIES_EPOCH_NUMBER}\"\n#  Install curl and gnupg2 - needed to download nodejs in the next step\nRUN apt-get update \\\n && apt-get install --no-install-recommends curl gnupg2 -y \\\n && apt-get autoremove -yqq --purge \\\n && apt-get clean \\\n && rm -rf /var/lib/apt/lists/*\n#  Install basic apt dependencies\nRUN curl -sL https://deb.nodesource.com/setup_10.x | bash - \\\n && apt-get update \\\n && apt-get install --no-install-recommends apt-utils build-essential curl dirmngr freetds-bin freetds-dev git gosu libffi-dev libkrb5-dev libpq-dev libsasl2-2 libsasl2-dev libsasl2-modules libssl-dev locales netcat nodejs rsync sasl2-bin sudo -y \\\n && apt-get autoremove -yqq --purge \\\n && apt-get clean \\\n && rm -rf /var/lib/apt/lists/*\n#  Install MySQL client from Oracle repositories (Debian installs mariadb)\nRUN KEY=\"A4A9406876FCBD3C456770C88C718D3B5072E1F5\" \\\n && GNUPGHOME=\"$( mktemp -d ;)\" \\\n && export GNUPGHOME \\\n && for KEYSERVER in $( shuf -e ha.pool.sks-keyservers.net hkp://p80.pool.sks-keyservers.net:80 keyserver.ubuntu.com hkp://keyserver.ubuntu.com:80 pgp.mit.edu ;); do gpg --keyserver \"${KEYSERVER}\" --recv-keys \"${KEY}\" \\\n && break || true ; done \\\n && gpg --export \"${KEY}\" | apt-key add - \\\n && gpgconf --kill all rm -rf \"${GNUPGHOME}\" ; apt-key list > /dev/null \\\n && echo \"deb http://repo.mysql.com/apt/ubuntu/ trusty mysql-5.6\" | tee -a /etc/apt/sources.list.d/mysql.list \\\n && apt-get update \\\n && apt-get install --no-install-recommends libmysqlclient-dev mysql-client -y \\\n && apt-get autoremove -yqq --purge \\\n && apt-get clean \\\n && rm -rf /var/lib/apt/lists/*\nRUN adduser airflow \\\n && echo \"airflow ALL=(ALL) NOPASSWD: ALL\" > /etc/sudoers.d/airflow \\\n && chmod 0440 /etc/sudoers.d/airflow\n# ###########################################################################################################\n#  This is an image with all APT dependencies needed by CI. It is built on top of the airlfow APT image\n#  Parameters:\n#      airflow-apt-deps - this is the base image for CI deps image.\n# ###########################################################################################################\nFROM airflow-apt-deps AS airflow-ci-apt-deps\nSHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-e\", \"-u\", \"-x\", \"-c\"]\nENV JAVA_HOME=\"/usr/lib/jvm/java-8-openjdk-amd64/\"\nARG APT_DEPS_IMAGE\nENV APT_DEPS_IMAGE=\"${APT_DEPS_IMAGE}\"\nRUN echo \"${APT_DEPS_IMAGE}\"\n#  Note the ifs below might be removed if Buildkit will become usable. It should skip building this\n#  image automatically if it is not used. For now we still go through all layers below but they are empty\nRUN if [[ \"${APT_DEPS_IMAGE}\" == \"airflow-ci-apt-deps\" ]] ; then mkdir -pv /usr/share/man/man1 \\\n && mkdir -pv /usr/share/man/man7 \\\n && apt-get update \\\n && apt-get install --no-install-recommends gnupg krb5-user ldap-utils less lsb-release net-tools openjdk-8-jdk openssh-client openssh-server postgresql-client python-selinux sqlite3 tmux unzip vim -y \\\n && apt-get autoremove -yqq --purge \\\n && apt-get clean \\\n && rm -rf /var/lib/apt/lists/* ; fi\nENV HADOOP_DISTRO=\"cdh\" \\\n    HADOOP_MAJOR=\"5\" \\\n    HADOOP_DISTRO_VERSION=\"5.11.0\" \\\n    HADOOP_VERSION=\"2.6.0\" \\\n    HIVE_VERSION=\"1.1.0\"\nENV HADOOP_URL=\"https://archive.cloudera.com/${HADOOP_DISTRO}${HADOOP_MAJOR}/${HADOOP_DISTRO}/${HADOOP_MAJOR}/\"\nENV HADOOP_HOME=\"/tmp/hadoop-cdh\" \\\n    HIVE_HOME=\"/tmp/hive\"\nRUN if [[ \"${APT_DEPS_IMAGE}\" == \"airflow-ci-apt-deps\" ]] ; then mkdir -pv ${HADOOP_HOME} \\\n && mkdir -pv ${HIVE_HOME} \\\n && mkdir /tmp/minicluster \\\n && mkdir -pv /user/hive/warehouse \\\n && chmod -R 777 ${HIVE_HOME} \\\n && chmod -R 777 /user/ ; fi\n#  Install Hadoop\n#  --absolute-names is a work around to avoid this issue https://github.com/docker/hub-feedback/issues/727\nRUN if [[ \"${APT_DEPS_IMAGE}\" == \"airflow-ci-apt-deps\" ]] ; then HADOOP_URL=${HADOOP_URL}hadoop-${HADOOP_VERSION}-${HADOOP_DISTRO}${HADOOP_DISTRO_VERSION}.tar.gz \\\n && HADOOP_TMP_FILE=/tmp/hadoop.tar.gz \\\n && curl -sL ${HADOOP_URL} > ${HADOOP_TMP_FILE} \\\n && tar xzf ${HADOOP_TMP_FILE} --absolute-names --strip-components 1 -C ${HADOOP_HOME} \\\n && rm ${HADOOP_TMP_FILE} ; fi\n#  Install Hive\nRUN if [[ \"${APT_DEPS_IMAGE}\" == \"airflow-ci-apt-deps\" ]] ; then HIVE_URL=${HADOOP_URL}hive-${HIVE_VERSION}-${HADOOP_DISTRO}${HADOOP_DISTRO_VERSION}.tar.gz \\\n && HIVE_TMP_FILE=/tmp/hive.tar.gz \\\n && curl -sL ${HIVE_URL} > ${HIVE_TMP_FILE} \\\n && tar xzf ${HIVE_TMP_FILE} --strip-components 1 -C ${HIVE_HOME} \\\n && rm ${HIVE_TMP_FILE} ; fi\nENV MINICLUSTER_URL=\"https://github.com/bolkedebruin/minicluster/releases/download/\"\nENV MINICLUSTER_VER=\"1.1\"\n#  Install MiniCluster TODO: install it differently. Installing to /tmp is probably a bad idea\nRUN if [[ \"${APT_DEPS_IMAGE}\" == \"airflow-ci-apt-deps\" ]] ; then MINICLUSTER_URL=${MINICLUSTER_URL}${MINICLUSTER_VER}/minicluster-${MINICLUSTER_VER}-SNAPSHOT-bin.zip \\\n && MINICLUSTER_TMP_FILE=/tmp/minicluster.zip \\\n && curl -sL ${MINICLUSTER_URL} > ${MINICLUSTER_TMP_FILE} \\\n && unzip ${MINICLUSTER_TMP_FILE} -d /tmp \\\n && rm ${MINICLUSTER_TMP_FILE} ; fi\nENV PATH=\"\\\"${PATH}:/tmp/hive/bin\\\"\"\n# ###########################################################################################################\n#  This is the target image - it installs PIP and NPM dependencies including efficient caching\n#  mechanisms - it might be used to build the bare airflow build or CI build\n#  Parameters:\n#     APT_DEPS_IMAGE - image with APT dependencies. It might either be base deps image with airflow\n#                      dependencies or CI deps image that contains also CI-required dependencies\n# ###########################################################################################################\nFROM ${APT_DEPS_IMAGE} AS main\nSHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-e\", \"-u\", \"-x\", \"-c\"]\nWORKDIR /opt/airflow\nRUN echo \"Airflow version: ${AIRFLOW_VERSION}\"\nARG APT_DEPS_IMAGE\nENV APT_DEPS_IMAGE=\"${APT_DEPS_IMAGE}\"\nARG AIRFLOW_USER=airflow\nENV AIRFLOW_USER=\"${AIRFLOW_USER}\"\nARG HOME=/home/airflow\nENV HOME=\"${HOME}\"\nARG AIRFLOW_HOME=${HOME}/airflow\nENV AIRFLOW_HOME=\"${AIRFLOW_HOME}\"\nARG AIRFLOW_SOURCES=/opt/airflow\nENV AIRFLOW_SOURCES=\"${AIRFLOW_SOURCES}\"\nRUN mkdir -pv ${AIRFLOW_HOME} mkdir -pv ${AIRFLOW_HOME}/dags mkdir -pv ${AIRFLOW_HOME}/logs \\\n && chown -R ${AIRFLOW_USER}.${AIRFLOW_USER} ${AIRFLOW_HOME}\n#  Increase the value here to force reinstalling Apache Airflow pip dependencies\nARG PIP_DEPENDENCIES_EPOCH_NUMBER=\"1\"\nENV PIP_DEPENDENCIES_EPOCH_NUMBER=\"${PIP_DEPENDENCIES_EPOCH_NUMBER}\"\n#  Optimizing installation of Cassandra driver\n#  Speeds up building the image - cassandra driver without CYTHON saves around 10 minutes\nARG CASS_DRIVER_NO_CYTHON=\"1\"\n#  Build cassandra driver on multiple CPUs\nARG CASS_DRIVER_BUILD_CONCURRENCY=\"8\"\nENV CASS_DRIVER_BUILD_CONCURRENCY=\"${CASS_DRIVER_BUILD_CONCURRENCY}\"\nENV CASS_DRIVER_NO_CYTHON=\"${CASS_DRIVER_NO_CYTHON}\"\n#  By default PIP install run without cache to make image smaller\nARG PIP_NO_CACHE_DIR=\"true\"\nENV PIP_NO_CACHE_DIR=\"${PIP_NO_CACHE_DIR}\"\nRUN echo \"Pip no cache dir: ${PIP_NO_CACHE_DIR}\"\n#  PIP version used to install dependencies\nARG PIP_VERSION=\"19.0.1\"\nENV PIP_VERSION=\"${PIP_VERSION}\"\nRUN echo \"Pip version: ${PIP_VERSION}\"\nRUN pip install pip==${PIP_VERSION} --upgrade\n#  We are copying everything with airflow:airflow user:group even if we use root to run the scripts\n#  This is fine as root user will be able to use those dirs anyway.\n#  Airflow sources change frequently but dependency configuration won't change that often\n#  We copy setup.py and other files needed to perform setup of dependencies\n#  This way cache here will only be invalidated if any of the\n#  version/setup configuration change but not when airflow sources change\nCOPY --chown=airflow:airflow setup.py ${AIRFLOW_SOURCES}/setup.py\nCOPY --chown=airflow:airflow setup.cfg ${AIRFLOW_SOURCES}/setup.cfg\nCOPY --chown=airflow:airflow airflow/version.py ${AIRFLOW_SOURCES}/airflow/version.py\nCOPY --chown=airflow:airflow airflow/__init__.py ${AIRFLOW_SOURCES}/airflow/__init__.py\nCOPY --chown=airflow:airflow airflow/bin/airflow ${AIRFLOW_SOURCES}/airflow/bin/airflow\n#  Airflow Extras installed\nARG AIRFLOW_EXTRAS=\"all\"\nENV AIRFLOW_EXTRAS=\"${AIRFLOW_EXTRAS}\"\nRUN echo \"Installing with extras: ${AIRFLOW_EXTRAS}.\"\n#  First install only dependencies but no Apache Airflow itself\n#  This way regular changes in sources of Airflow will not trigger reinstallation of all dependencies\n#  And this Docker layer will be reused between builds.\nRUN pip install --no-use-pep517 -e \".[${AIRFLOW_EXTRAS}]\"\nCOPY --chown=airflow:airflow airflow/www/package.json ${AIRFLOW_SOURCES}/airflow/www/package.json\nCOPY --chown=airflow:airflow airflow/www/package-lock.json ${AIRFLOW_SOURCES}/airflow/www/package-lock.json\nWORKDIR ${AIRFLOW_SOURCES}/airflow/www\n#  Install necessary NPM dependencies (triggered by changes in package-lock.json)\nRUN gosu ${AIRFLOW_USER} npm ci\nCOPY --chown=airflow:airflow airflow/www/ ${AIRFLOW_SOURCES}/airflow/www/\n#  Package NPM for production\nRUN gosu ${AIRFLOW_USER} npm run prod\n#  Always apt-get update/upgrade here to get latest dependencies before\n#  we redo pip install\nRUN apt-get update \\\n && apt-get upgrade -y --no-install-recommends \\\n && apt-get clean \\\n && rm -rf /var/lib/apt/lists/*\n#  Cache for this line will be automatically invalidated if any\n#  of airflow sources change\nCOPY --chown=airflow:airflow . ${AIRFLOW_SOURCES}/\nWORKDIR ${AIRFLOW_SOURCES}\n#  Always add-get update/upgrade here to get latest dependencies before\n#  we redo pip install\nRUN apt-get update \\\n && apt-get upgrade -y --no-install-recommends \\\n && apt-get clean \\\n && rm -rf /var/lib/apt/lists/*\n#  Additional python deps to install\nARG ADDITIONAL_PYTHON_DEPS=\"\"\nRUN if [[ -n \"${ADDITIONAL_PYTHON_DEPS}\" ]] ; then pip install ${ADDITIONAL_PYTHON_DEPS} ; fi\nCOPY --chown=airflow:airflow ./scripts/docker/entrypoint.sh /entrypoint.sh\nUSER ${AIRFLOW_USER}\nWORKDIR ${AIRFLOW_SOURCES}\nENV PATH=\"${HOME}:${PATH}\"\nEXPOSE 8080/tcp\nENTRYPOINT [\"/usr/local/bin/dumb-init\", \"--\", \"/entrypoint.sh\"]\nCMD [\"--help\"]\n","injectedSmells":[],"originalDockerfileHash":"1bbbc8d2698a53763701a753933004cc","successfullyInjectedSmells":[],"originalDockerfileUglified":"#\n#   Licensed to the Apache Software Foundation (ASF) under one or more\n#   contributor license agreements.  See the NOTICE file distributed with\n#   this work for additional information regarding copyright ownership.\n#   The ASF licenses this file to You under the Apache License, Version 2.0\n#   (the \"License\"); you may not use this file except in compliance with\n#   the License.  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#   Unless required by applicable law or agreed to in writing, software\n#   distributed under the License is distributed on an \"AS IS\" BASIS,\n#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#   See the License for the specific language governing permissions and\n#   limitations under the License.\n#\n#   WARNING: THIS DOCKERFILE IS NOT INTENDED FOR PRODUCTION USE OR DEPLOYMENT.\n#\n#   Base image for the whole Docker file\nARG APT_DEPS_IMAGE=\"airflow-apt-deps\"\nARG PYTHON_BASE_IMAGE=\"python:3.6-slim\"\n#  ###########################################################################################################\n#   This is the base image with APT dependencies needed by Airflow. It is based on a python slim image\n#   Parameters:\n#      PYTHON_BASE_IMAGE - base python image (python:x.y-slim)\n#  ###########################################################################################################\nFROM ${PYTHON_BASE_IMAGE} AS airflow-apt-deps\nSHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-e\", \"-u\", \"-x\", \"-c\"]\n#   Need to repeat the empty argument here otherwise it will not be set for this stage\n#   But the default value carries from the one set before FROM\nARG PYTHON_BASE_IMAGE\nENV PYTHON_BASE_IMAGE=\"${PYTHON_BASE_IMAGE}\"\nARG AIRFLOW_VERSION=\"2.0.0.dev0\"\nENV AIRFLOW_VERSION=\"$AIRFLOW_VERSION\"\n#   Print versions\nRUN echo \"Base image: ${PYTHON_BASE_IMAGE}\"\nRUN echo \"Airflow version: ${AIRFLOW_VERSION}\"\n#   Make sure noninteractie debian install is used and language variables set\nENV DEBIAN_FRONTEND=\"noninteractive\" \\\n    LANGUAGE=\"C.UTF-8\" \\\n    LANG=\"C.UTF-8\" \\\n    LC_ALL=\"C.UTF-8\" \\\n    LC_CTYPE=\"C.UTF-8\" \\\n    LC_MESSAGES=\"C.UTF-8\"\n#   By increasing this number we can do force build of all dependencies\nARG DEPENDENCIES_EPOCH_NUMBER=\"1\"\n#   Increase the value below to force renstalling of all dependencies\nENV DEPENDENCIES_EPOCH_NUMBER=\"${DEPENDENCIES_EPOCH_NUMBER}\"\n#   Install curl and gnupg2 - needed to download nodejs in the next step\nRUN apt-get update \\\n && apt-get install --no-install-recommends curl gnupg2 -y \\\n && apt-get autoremove -yqq --purge \\\n && apt-get clean \\\n && rm -rf /var/lib/apt/lists/*\n#   Install basic apt dependencies\nRUN curl -sL https://deb.nodesource.com/setup_10.x | bash - \\\n && apt-get update \\\n && apt-get install --no-install-recommends apt-utils build-essential curl dirmngr freetds-bin freetds-dev git gosu libffi-dev libkrb5-dev libpq-dev libsasl2-2 libsasl2-dev libsasl2-modules libssl-dev locales netcat nodejs rsync sasl2-bin sudo -y \\\n && apt-get autoremove -yqq --purge \\\n && apt-get clean \\\n && rm -rf /var/lib/apt/lists/*\n#   Install MySQL client from Oracle repositories (Debian installs mariadb)\nRUN KEY=\"A4A9406876FCBD3C456770C88C718D3B5072E1F5\" \\\n && GNUPGHOME=\"$( mktemp -d ;)\" \\\n && export GNUPGHOME \\\n && for KEYSERVER in $( shuf -e ha.pool.sks-keyservers.net hkp://p80.pool.sks-keyservers.net:80 keyserver.ubuntu.com hkp://keyserver.ubuntu.com:80 pgp.mit.edu ;); do gpg --keyserver \"${KEYSERVER}\" --recv-keys \"${KEY}\" \\\n && break || true ; done \\\n && gpg --export \"${KEY}\" | apt-key add - \\\n && gpgconf --kill all rm -rf \"${GNUPGHOME}\" ; apt-key list > /dev/null \\\n && echo \"deb http://repo.mysql.com/apt/ubuntu/ trusty mysql-5.6\" | tee -a /etc/apt/sources.list.d/mysql.list \\\n && apt-get update \\\n && apt-get install --no-install-recommends libmysqlclient-dev mysql-client -y \\\n && apt-get autoremove -yqq --purge \\\n && apt-get clean \\\n && rm -rf /var/lib/apt/lists/*\nRUN adduser airflow \\\n && echo \"airflow ALL=(ALL) NOPASSWD: ALL\" > /etc/sudoers.d/airflow \\\n && chmod 0440 /etc/sudoers.d/airflow\n#  ###########################################################################################################\n#   This is an image with all APT dependencies needed by CI. It is built on top of the airlfow APT image\n#   Parameters:\n#       airflow-apt-deps - this is the base image for CI deps image.\n#  ###########################################################################################################\nFROM airflow-apt-deps AS airflow-ci-apt-deps\nSHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-e\", \"-u\", \"-x\", \"-c\"]\nENV JAVA_HOME=\"/usr/lib/jvm/java-8-openjdk-amd64/\"\nARG APT_DEPS_IMAGE\nENV APT_DEPS_IMAGE=\"${APT_DEPS_IMAGE}\"\nRUN echo \"${APT_DEPS_IMAGE}\"\n#   Note the ifs below might be removed if Buildkit will become usable. It should skip building this\n#   image automatically if it is not used. For now we still go through all layers below but they are empty\nRUN if [[ \"${APT_DEPS_IMAGE}\" == \"airflow-ci-apt-deps\" ]] ; then mkdir -pv /usr/share/man/man1 \\\n && mkdir -pv /usr/share/man/man7 \\\n && apt-get update \\\n && apt-get install --no-install-recommends gnupg krb5-user ldap-utils less lsb-release net-tools openjdk-8-jdk openssh-client openssh-server postgresql-client python-selinux sqlite3 tmux unzip vim -y \\\n && apt-get autoremove -yqq --purge \\\n && apt-get clean \\\n && rm -rf /var/lib/apt/lists/* ; fi\nENV HADOOP_DISTRO=\"cdh\" \\\n    HADOOP_MAJOR=\"5\" \\\n    HADOOP_DISTRO_VERSION=\"5.11.0\" \\\n    HADOOP_VERSION=\"2.6.0\" \\\n    HIVE_VERSION=\"1.1.0\"\nENV HADOOP_URL=\"https://archive.cloudera.com/${HADOOP_DISTRO}${HADOOP_MAJOR}/${HADOOP_DISTRO}/${HADOOP_MAJOR}/\"\nENV HADOOP_HOME=\"/tmp/hadoop-cdh\" \\\n    HIVE_HOME=\"/tmp/hive\"\nRUN if [[ \"${APT_DEPS_IMAGE}\" == \"airflow-ci-apt-deps\" ]] ; then mkdir -pv ${HADOOP_HOME} \\\n && mkdir -pv ${HIVE_HOME} \\\n && mkdir /tmp/minicluster \\\n && mkdir -pv /user/hive/warehouse \\\n && chmod -R 777 ${HIVE_HOME} \\\n && chmod -R 777 /user/ ; fi\n#   Install Hadoop\n#   --absolute-names is a work around to avoid this issue https://github.com/docker/hub-feedback/issues/727\nRUN if [[ \"${APT_DEPS_IMAGE}\" == \"airflow-ci-apt-deps\" ]] ; then HADOOP_URL=${HADOOP_URL}hadoop-${HADOOP_VERSION}-${HADOOP_DISTRO}${HADOOP_DISTRO_VERSION}.tar.gz \\\n && HADOOP_TMP_FILE=/tmp/hadoop.tar.gz \\\n && curl -sL ${HADOOP_URL} > ${HADOOP_TMP_FILE} \\\n && tar xzf ${HADOOP_TMP_FILE} --absolute-names --strip-components 1 -C ${HADOOP_HOME} \\\n && rm ${HADOOP_TMP_FILE} ; fi\n#   Install Hive\nRUN if [[ \"${APT_DEPS_IMAGE}\" == \"airflow-ci-apt-deps\" ]] ; then HIVE_URL=${HADOOP_URL}hive-${HIVE_VERSION}-${HADOOP_DISTRO}${HADOOP_DISTRO_VERSION}.tar.gz \\\n && HIVE_TMP_FILE=/tmp/hive.tar.gz \\\n && curl -sL ${HIVE_URL} > ${HIVE_TMP_FILE} \\\n && tar xzf ${HIVE_TMP_FILE} --strip-components 1 -C ${HIVE_HOME} \\\n && rm ${HIVE_TMP_FILE} ; fi\nENV MINICLUSTER_URL=\"https://github.com/bolkedebruin/minicluster/releases/download/\"\nENV MINICLUSTER_VER=\"1.1\"\n#   Install MiniCluster TODO: install it differently. Installing to /tmp is probably a bad idea\nRUN if [[ \"${APT_DEPS_IMAGE}\" == \"airflow-ci-apt-deps\" ]] ; then MINICLUSTER_URL=${MINICLUSTER_URL}${MINICLUSTER_VER}/minicluster-${MINICLUSTER_VER}-SNAPSHOT-bin.zip \\\n && MINICLUSTER_TMP_FILE=/tmp/minicluster.zip \\\n && curl -sL ${MINICLUSTER_URL} > ${MINICLUSTER_TMP_FILE} \\\n && unzip ${MINICLUSTER_TMP_FILE} -d /tmp \\\n && rm ${MINICLUSTER_TMP_FILE} ; fi\nENV PATH=\"\\\"${PATH}:/tmp/hive/bin\\\"\"\n#  ###########################################################################################################\n#   This is the target image - it installs PIP and NPM dependencies including efficient caching\n#   mechanisms - it might be used to build the bare airflow build or CI build\n#   Parameters:\n#      APT_DEPS_IMAGE - image with APT dependencies. It might either be base deps image with airflow\n#                       dependencies or CI deps image that contains also CI-required dependencies\n#  ###########################################################################################################\nFROM ${APT_DEPS_IMAGE} AS main\nSHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-e\", \"-u\", \"-x\", \"-c\"]\nWORKDIR /opt/airflow\nRUN echo \"Airflow version: ${AIRFLOW_VERSION}\"\nARG APT_DEPS_IMAGE\nENV APT_DEPS_IMAGE=\"${APT_DEPS_IMAGE}\"\nARG AIRFLOW_USER=airflow\nENV AIRFLOW_USER=\"${AIRFLOW_USER}\"\nARG HOME=/home/airflow\nENV HOME=\"${HOME}\"\nARG AIRFLOW_HOME=${HOME}/airflow\nENV AIRFLOW_HOME=\"${AIRFLOW_HOME}\"\nARG AIRFLOW_SOURCES=/opt/airflow\nENV AIRFLOW_SOURCES=\"${AIRFLOW_SOURCES}\"\nRUN mkdir -pv ${AIRFLOW_HOME} mkdir -pv ${AIRFLOW_HOME}/dags mkdir -pv ${AIRFLOW_HOME}/logs \\\n && chown -R ${AIRFLOW_USER}.${AIRFLOW_USER} ${AIRFLOW_HOME}\n#   Increase the value here to force reinstalling Apache Airflow pip dependencies\nARG PIP_DEPENDENCIES_EPOCH_NUMBER=\"1\"\nENV PIP_DEPENDENCIES_EPOCH_NUMBER=\"${PIP_DEPENDENCIES_EPOCH_NUMBER}\"\n#   Optimizing installation of Cassandra driver\n#   Speeds up building the image - cassandra driver without CYTHON saves around 10 minutes\nARG CASS_DRIVER_NO_CYTHON=\"1\"\n#   Build cassandra driver on multiple CPUs\nARG CASS_DRIVER_BUILD_CONCURRENCY=\"8\"\nENV CASS_DRIVER_BUILD_CONCURRENCY=\"${CASS_DRIVER_BUILD_CONCURRENCY}\"\nENV CASS_DRIVER_NO_CYTHON=\"${CASS_DRIVER_NO_CYTHON}\"\n#   By default PIP install run without cache to make image smaller\nARG PIP_NO_CACHE_DIR=\"true\"\nENV PIP_NO_CACHE_DIR=\"${PIP_NO_CACHE_DIR}\"\nRUN echo \"Pip no cache dir: ${PIP_NO_CACHE_DIR}\"\n#   PIP version used to install dependencies\nARG PIP_VERSION=\"19.0.1\"\nENV PIP_VERSION=\"${PIP_VERSION}\"\nRUN echo \"Pip version: ${PIP_VERSION}\"\nRUN pip install pip==${PIP_VERSION} --upgrade\n#   We are copying everything with airflow:airflow user:group even if we use root to run the scripts\n#   This is fine as root user will be able to use those dirs anyway.\n#   Airflow sources change frequently but dependency configuration won't change that often\n#   We copy setup.py and other files needed to perform setup of dependencies\n#   This way cache here will only be invalidated if any of the\n#   version/setup configuration change but not when airflow sources change\nCOPY --chown=airflow:airflow setup.py ${AIRFLOW_SOURCES}/setup.py\nCOPY --chown=airflow:airflow setup.cfg ${AIRFLOW_SOURCES}/setup.cfg\nCOPY --chown=airflow:airflow airflow/version.py ${AIRFLOW_SOURCES}/airflow/version.py\nCOPY --chown=airflow:airflow airflow/__init__.py ${AIRFLOW_SOURCES}/airflow/__init__.py\nCOPY --chown=airflow:airflow airflow/bin/airflow ${AIRFLOW_SOURCES}/airflow/bin/airflow\n#   Airflow Extras installed\nARG AIRFLOW_EXTRAS=\"all\"\nENV AIRFLOW_EXTRAS=\"${AIRFLOW_EXTRAS}\"\nRUN echo \"Installing with extras: ${AIRFLOW_EXTRAS}.\"\n#   First install only dependencies but no Apache Airflow itself\n#   This way regular changes in sources of Airflow will not trigger reinstallation of all dependencies\n#   And this Docker layer will be reused between builds.\nRUN pip install --no-use-pep517 -e \".[${AIRFLOW_EXTRAS}]\"\nCOPY --chown=airflow:airflow airflow/www/package.json ${AIRFLOW_SOURCES}/airflow/www/package.json\nCOPY --chown=airflow:airflow airflow/www/package-lock.json ${AIRFLOW_SOURCES}/airflow/www/package-lock.json\nWORKDIR ${AIRFLOW_SOURCES}/airflow/www\n#   Install necessary NPM dependencies (triggered by changes in package-lock.json)\nRUN gosu ${AIRFLOW_USER} npm ci\nCOPY --chown=airflow:airflow airflow/www/ ${AIRFLOW_SOURCES}/airflow/www/\n#   Package NPM for production\nRUN gosu ${AIRFLOW_USER} npm run prod\n#   Always apt-get update/upgrade here to get latest dependencies before\n#   we redo pip install\nRUN apt-get update \\\n && apt-get upgrade -y --no-install-recommends \\\n && apt-get clean \\\n && rm -rf /var/lib/apt/lists/*\n#   Cache for this line will be automatically invalidated if any\n#   of airflow sources change\nCOPY --chown=airflow:airflow . ${AIRFLOW_SOURCES}/\nWORKDIR ${AIRFLOW_SOURCES}\n#   Always add-get update/upgrade here to get latest dependencies before\n#   we redo pip install\nRUN apt-get update \\\n && apt-get upgrade -y --no-install-recommends \\\n && apt-get clean \\\n && rm -rf /var/lib/apt/lists/*\n#   Additional python deps to install\nARG ADDITIONAL_PYTHON_DEPS=\"\"\nRUN if [[ -n \"${ADDITIONAL_PYTHON_DEPS}\" ]] ; then pip install ${ADDITIONAL_PYTHON_DEPS} ; fi\nCOPY --chown=airflow:airflow ./scripts/docker/entrypoint.sh /entrypoint.sh\nUSER ${AIRFLOW_USER}\nWORKDIR ${AIRFLOW_SOURCES}\nENV PATH=\"${HOME}:${PATH}\"\nEXPOSE 8080/tcp\nENTRYPOINT [\"/usr/local/bin/dumb-init\", \"--\", \"/entrypoint.sh\"]\nCMD [\"--help\"]\n","originalDockerfileUglifiedHash":"7ef010f9505eea0c90847159ae6a2b6e","fileName":"/ICSME-replicationpackage/dataset/smelly_dockerfiles_bianncle/1002689fafe7d5f302e672f18631619944b23abc.dockerfile"}