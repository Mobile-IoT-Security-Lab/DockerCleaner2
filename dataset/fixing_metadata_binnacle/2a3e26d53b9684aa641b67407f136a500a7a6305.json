{"seed":2585403224,"processedDockerfileHash":"cfa626ba27c4797ad3ec6aceaefdbb47","fixedSmells":["use-no-install-recommends","do-not-use-apt-get-update-alone","pin-package-manager-versions-apt-get","pin-package-manager-versions-pip","pin-package-manager-versions-npm","pin-package-manager-versions-gem","pin-package-manager-versions-apk","use-copy-instead-of-add","use-wget-instead-of-add","do-not-have-secrets","have-a-healthcheck","have-a-user"],"successfullyFixedSmells":["pin-package-manager-versions-apt-get","pin-package-manager-versions-pip","use-copy-instead-of-add","have-a-healthcheck"],"processedDockerfile":"FROM ubuntu:bionic\nMAINTAINER akchinSTC\nARG NB_USER=\"jovyan\"\nARG NB_UID=\"1000\"\nARG NB_GID=\"100\"\nUSER root\nENV HADOOP_PREFIX=\"/usr/hdp/current/hadoop\" \\\n    ANACONDA_HOME=\"/opt/conda\"\nENV SHELL=\"/bin/bash\" \\\n    NB_USER=\"$NB_USER\" \\\n    NB_UID=\"$NB_UID\" \\\n    NB_GID=\"$NB_GID\" \\\n    LC_ALL=\"en_US.UTF-8\" \\\n    LANG=\"en_US.UTF-8\" \\\n    LANGUAGE=\"en_US.UTF-8\" \\\n    JAVA_HOME=\"/usr/lib/jvm/java-8-openjdk-amd64\" \\\n    SPARK_HOME=\"/usr/hdp/current/spark2-client\" \\\n    PYSPARK_PYTHON=\"$ANACONDA_HOME/bin/python\" \\\n    HADOOP_CONF_DIR=\"$HADOOP_PREFIX/etc/hadoop\"\nENV HOME=\"/home/$NB_USER\" \\\n    PATH=\"$ANACONDA_HOME/bin:$HADOOP_PREFIX/bin:$JAVA_HOME/bin:$SPARK_HOME/bin:$PATH\"\nENV SPARK_VER=\"2.4.1\"\nENV HADOOP_VER=\"2.7.7\"\n#   INSTALL / DOWNLOAD ALL NEEDED PACKAGES\nRUN apt-get update \\\n && apt-get -yq dist-upgrade \\\n && apt-get install --no-install-recommends wget=1.19.4-1ubuntu2.2 bzip2=1.0.6-8.1ubuntu0.2 tar=1.29b-2ubuntu0.4 curl=7.58.0-2ubuntu3.24 less=487-0.1 nano=2.9.3-2 ca-certificates=20211016ubuntu0.18.04.1 libkrb5-dev=1.16-2ubuntu0.4 sudo=1.8.21p2-3ubuntu1.5 locales=2.27-3ubuntu1.6 gcc=4:7.4.0-1ubuntu2.3 fonts-liberation=1:1.07.4-7~18.04.1 unzip=6.0-21ubuntu1.2 libsm6=2:1.2.2-1 libxext-dev=2:1.3.3-1 libxrender1=1:0.9.10-1 openjdk-8-jdk=8u362-ga-0ubuntu1~18.04.1 openssh-server=1:7.6p1-4ubuntu0.7 openssh-client=1:7.6p1-4ubuntu0.7 -yq \\\n && rm -rf /var/lib/apt/lists/*\nRUN echo \"en_US.UTF-8 UTF-8\" > /etc/locale.gen \\\n && locale-gen\nCOPY fix-permissions /usr/local/bin/fix-permissions\n#   Create jovyan user with UID=1000 and in the 'users' group\n#   and make sure these dirs are writable by the `users` group.\nRUN groupadd wheel -g 11 \\\n && echo \"auth required pam_wheel.so use_uid\" >> /etc/pam.d/su \\\n && useradd -m -s /bin/bash -N -u $NB_UID $NB_USER \\\n && mkdir -p $ANACONDA_HOME \\\n && mkdir -p /usr/hdp/current \\\n && mkdir -p /usr/local/share/jupyter \\\n && chown $NB_USER:$NB_GID $ANACONDA_HOME \\\n && chmod g+w /etc/passwd \\\n && chmod +x /usr/local/bin/fix-permissions \\\n && fix-permissions $HOME \\\n && fix-permissions $ANACONDA_HOME \\\n && fix-permissions /usr/hdp/current \\\n && fix-permissions /usr/local/share/jupyter\n#   Create service user 'jovyan'. Pin uid/gid to 1000.\nRUN useradd -m -s /bin/bash -N -u 1111 elyra \\\n && useradd -m -s /bin/bash -N -u 1112 bob \\\n && useradd -m -s /bin/bash -N -u 1113 alice\nUSER $NB_UID\n#   Setup work directory for backward-compatibility\nRUN mkdir /home/$NB_USER/work \\\n && fix-permissions /home/$NB_USER\n#   DOWNLOAD HADOOP AND SPARK\nRUN curl -s http://www.eu.apache.org/dist/hadoop/common/hadoop-$HADOOP_VER/hadoop-$HADOOP_VER.tar.gz | tar -xz -C /usr/hdp/current\nRUN curl -s https://archive.apache.org/dist/spark/spark-${SPARK_VER}/spark-${SPARK_VER}-bin-hadoop2.7.tgz | tar -xz -C /usr/hdp/current\n#   SETUP SPARK AND HADOOP SYMLINKS\nRUN cd /usr/hdp/current \\\n && ln -s ./hadoop-$HADOOP_VER hadoop \\\n && ln -s ./spark-${SPARK_VER}-bin-hadoop2.7 spark2-client\n#   INSTALL MINI-CONDA AND PYTHON PACKAGES\nRUN wget --quiet https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda.sh \\\n && /bin/bash ~/miniconda.sh -f -b -p $ANACONDA_HOME \\\n && rm ~/miniconda.sh \\\n && conda clean -tipsy \\\n && rm -rf /home/$NB_USER/.cache/yarn \\\n && fix-permissions $ANACONDA_HOME \\\n && fix-permissions /home/$NB_USER\nRUN conda install --yes --quiet 'pip' 'jupyter' 'r-devtools' 'r-stringr' 'r-argparse' \\\n && conda clean -tipsy \\\n && fix-permissions $ANACONDA_HOME \\\n && fix-permissions /home/$NB_USER\nRUN Rscript -e 'install.packages(\"IRkernel\", repos=\"http://cran.cnr.berkeley.edu\", lib=\"/opt/conda/lib/R/library\")' -e 'IRkernel::installspec(prefix = \"/usr/local\")' -e 'devtools::install_github(\"apache/spark@v2.4.1\", subdir=\"R/pkg\")' \\\n && fix-permissions $ANACONDA_HOME \\\n && fix-permissions /home/$NB_USER\n#   SETUP HADOOP CONFIGS\nRUN sed -i '/^export JAVA_HOME/ s:.*:export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\\nexport HADOOP_PREFIX=/usr/hdp/current/hadoop\\nexport HADOOP_HOME=/usr/hdp/current/hadoop\\n:' $HADOOP_PREFIX/etc/hadoop/hadoop-env.sh\nRUN sed -i '/^export HADOOP_CONF_DIR/ s:.*:export HADOOP_CONF_DIR=/usr/hdp/current/hadoop/etc/hadoop/:' $HADOOP_PREFIX/etc/hadoop/hadoop-env.sh\n#   SETUP PSEUDO - DISTRIBUTED CONFIGS FOR HADOOP\nCOPY core-site.xml.template hdfs-site.xml mapred-site.xml yarn-site.xml.template $HADOOP_PREFIX/etc/hadoop/\n#   working around docker.io build error\nRUN ls -la /usr/hdp/current/hadoop/etc/hadoop/*-env.sh \\\n && chmod +x /usr/hdp/current/hadoop/etc/hadoop/*-env.sh \\\n && ls -la /usr/hdp/current/hadoop/etc/hadoop/*-env.sh\n#   Install Toree\nRUN cd /tmp \\\n && curl -O https://dist.apache.org/repos/dist/release/incubator/toree/0.3.0-incubating/toree-pip/toree-0.3.0.tar.gz \\\n && pip install setuptools==67.6.1 --upgrade --user \\\n && pip install /tmp/toree-0.3.0.tar.gz \\\n && jupyter toree install --spark_home=$SPARK_HOME --kernel_name=\"Spark 2.4.1\" --interpreters=Scala \\\n && rm -f /tmp/toree-0.3.0.tar.gz \\\n && fix-permissions $ANACONDA_HOME \\\n && fix-permissions /home/$NB_USER\n#   SETUP PASSWORDLESS SSH FOR $NB_USER\nRUN ssh-keygen -q -N \"\" -t rsa -f /home/$NB_USER/.ssh/id_rsa \\\n && cp /home/$NB_USER/.ssh/id_rsa.pub /home/$NB_USER/.ssh/authorized_keys \\\n && chmod 0700 /home/$NB_USER\nUSER root\n#   SETUP PASSWORDLESS SSH\nRUN yes y | ssh-keygen -q -N \"\" -t dsa -f /etc/ssh/ssh_host_dsa_key \\\n && yes y | ssh-keygen -q -N \"\" -t rsa -f /etc/ssh/ssh_host_rsa_key \\\n && yes y | ssh-keygen -q -N \"\" -t rsa -f /root/.ssh/id_rsa \\\n && cp /root/.ssh/id_rsa.pub /root/.ssh/authorized_keys\nRUN ssh-keygen -A\nCOPY ssh_config /root/.ssh/config\nRUN chmod 600 /root/.ssh/config \\\n && chown root:root /root/.ssh/config \\\n && echo \"Port 2122\" >> /etc/ssh/sshd_config \\\n && echo \"${NB_USER} ALL=(ALL) NOPASSWD: ALL\" >> /etc/sudoers\nRUN service ssh restart\nCOPY ssh_config /home/$NB_USER/.ssh/config\nRUN chmod 600 /home/$NB_USER/.ssh/config \\\n && chown $NB_USER: /home/$NB_USER/.ssh/config\nCOPY bootstrap-yarn-spark.sh /usr/local/bin/\nRUN chown $NB_USER: /usr/local/bin/bootstrap-yarn-spark.sh \\\n && chmod 0700 /usr/local/bin/bootstrap-yarn-spark.sh\nCMD [\"/usr/local/bin/bootstrap-yarn-spark.sh\"]\nLABEL Hadoop.version=\"$HADOOP_VER\"\nLABEL Spark.version=\"$SPARK_VER\"\n#   Hdfs ports\nEXPOSE 50010/tcp 50020/tcp 50070/tcp 50075/tcp 50090/tcp 8020/tcp 9000/tcp 19888/tcp 8030/tcp 8031/tcp 8032/tcp 8033/tcp 8040/tcp 8042/tcp 8088/tcp 49707/tcp 2122/tcp\nUSER $NB_USER\n# Please add your HEALTHCHECK here!!!\n","originalDockerfile":"FROM ubuntu:bionic\nMAINTAINER akchinSTC\nARG NB_USER=\"jovyan\"\nARG NB_UID=\"1000\"\nARG NB_GID=\"100\"\nUSER root\nENV HADOOP_PREFIX=\"/usr/hdp/current/hadoop\" \\\n    ANACONDA_HOME=\"/opt/conda\"\nENV SHELL=\"/bin/bash\" \\\n    NB_USER=\"$NB_USER\" \\\n    NB_UID=\"$NB_UID\" \\\n    NB_GID=\"$NB_GID\" \\\n    LC_ALL=\"en_US.UTF-8\" \\\n    LANG=\"en_US.UTF-8\" \\\n    LANGUAGE=\"en_US.UTF-8\" \\\n    JAVA_HOME=\"/usr/lib/jvm/java-8-openjdk-amd64\" \\\n    SPARK_HOME=\"/usr/hdp/current/spark2-client\" \\\n    PYSPARK_PYTHON=\"$ANACONDA_HOME/bin/python\" \\\n    HADOOP_CONF_DIR=\"$HADOOP_PREFIX/etc/hadoop\"\nENV HOME=\"/home/$NB_USER\" \\\n    PATH=\"$ANACONDA_HOME/bin:$HADOOP_PREFIX/bin:$JAVA_HOME/bin:$SPARK_HOME/bin:$PATH\"\nENV SPARK_VER=\"2.4.1\"\nENV HADOOP_VER=\"2.7.7\"\n#  INSTALL / DOWNLOAD ALL NEEDED PACKAGES\nRUN apt-get update \\\n && apt-get -yq dist-upgrade \\\n && apt-get install --no-install-recommends wget bzip2 tar curl less nano ca-certificates libkrb5-dev sudo locales gcc fonts-liberation unzip libsm6 libxext-dev libxrender1 openjdk-8-jdk openssh-server openssh-client -yq \\\n && rm -rf /var/lib/apt/lists/*\nRUN echo \"en_US.UTF-8 UTF-8\" > /etc/locale.gen \\\n && locale-gen\nADD fix-permissions /usr/local/bin/fix-permissions\n#  Create jovyan user with UID=1000 and in the 'users' group\n#  and make sure these dirs are writable by the `users` group.\nRUN groupadd wheel -g 11 \\\n && echo \"auth required pam_wheel.so use_uid\" >> /etc/pam.d/su \\\n && useradd -m -s /bin/bash -N -u $NB_UID $NB_USER \\\n && mkdir -p $ANACONDA_HOME \\\n && mkdir -p /usr/hdp/current \\\n && mkdir -p /usr/local/share/jupyter \\\n && chown $NB_USER:$NB_GID $ANACONDA_HOME \\\n && chmod g+w /etc/passwd \\\n && chmod +x /usr/local/bin/fix-permissions \\\n && fix-permissions $HOME \\\n && fix-permissions $ANACONDA_HOME \\\n && fix-permissions /usr/hdp/current \\\n && fix-permissions /usr/local/share/jupyter\n#  Create service user 'jovyan'. Pin uid/gid to 1000.\nRUN useradd -m -s /bin/bash -N -u 1111 elyra \\\n && useradd -m -s /bin/bash -N -u 1112 bob \\\n && useradd -m -s /bin/bash -N -u 1113 alice\nUSER $NB_UID\n#  Setup work directory for backward-compatibility\nRUN mkdir /home/$NB_USER/work \\\n && fix-permissions /home/$NB_USER\n#  DOWNLOAD HADOOP AND SPARK\nRUN curl -s http://www.eu.apache.org/dist/hadoop/common/hadoop-$HADOOP_VER/hadoop-$HADOOP_VER.tar.gz | tar -xz -C /usr/hdp/current\nRUN curl -s https://archive.apache.org/dist/spark/spark-${SPARK_VER}/spark-${SPARK_VER}-bin-hadoop2.7.tgz | tar -xz -C /usr/hdp/current\n#  SETUP SPARK AND HADOOP SYMLINKS\nRUN cd /usr/hdp/current \\\n && ln -s ./hadoop-$HADOOP_VER hadoop \\\n && ln -s ./spark-${SPARK_VER}-bin-hadoop2.7 spark2-client\n#  INSTALL MINI-CONDA AND PYTHON PACKAGES\nRUN wget --quiet https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda.sh \\\n && /bin/bash ~/miniconda.sh -f -b -p $ANACONDA_HOME \\\n && rm ~/miniconda.sh \\\n && conda clean -tipsy \\\n && rm -rf /home/$NB_USER/.cache/yarn \\\n && fix-permissions $ANACONDA_HOME \\\n && fix-permissions /home/$NB_USER\nRUN conda install --yes --quiet 'pip' 'jupyter' 'r-devtools' 'r-stringr' 'r-argparse' \\\n && conda clean -tipsy \\\n && fix-permissions $ANACONDA_HOME \\\n && fix-permissions /home/$NB_USER\nRUN Rscript -e 'install.packages(\"IRkernel\", repos=\"http://cran.cnr.berkeley.edu\", lib=\"/opt/conda/lib/R/library\")' -e 'IRkernel::installspec(prefix = \"/usr/local\")' -e 'devtools::install_github(\"apache/spark@v2.4.1\", subdir=\"R/pkg\")' \\\n && fix-permissions $ANACONDA_HOME \\\n && fix-permissions /home/$NB_USER\n#  SETUP HADOOP CONFIGS\nRUN sed -i '/^export JAVA_HOME/ s:.*:export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\\nexport HADOOP_PREFIX=/usr/hdp/current/hadoop\\nexport HADOOP_HOME=/usr/hdp/current/hadoop\\n:' $HADOOP_PREFIX/etc/hadoop/hadoop-env.sh\nRUN sed -i '/^export HADOOP_CONF_DIR/ s:.*:export HADOOP_CONF_DIR=/usr/hdp/current/hadoop/etc/hadoop/:' $HADOOP_PREFIX/etc/hadoop/hadoop-env.sh\n#  SETUP PSEUDO - DISTRIBUTED CONFIGS FOR HADOOP\nCOPY core-site.xml.template hdfs-site.xml mapred-site.xml yarn-site.xml.template $HADOOP_PREFIX/etc/hadoop/\n#  working around docker.io build error\nRUN ls -la /usr/hdp/current/hadoop/etc/hadoop/*-env.sh \\\n && chmod +x /usr/hdp/current/hadoop/etc/hadoop/*-env.sh \\\n && ls -la /usr/hdp/current/hadoop/etc/hadoop/*-env.sh\n#  Install Toree\nRUN cd /tmp \\\n && curl -O https://dist.apache.org/repos/dist/release/incubator/toree/0.3.0-incubating/toree-pip/toree-0.3.0.tar.gz \\\n && pip install setuptools --upgrade --user \\\n && pip install /tmp/toree-0.3.0.tar.gz \\\n && jupyter toree install --spark_home=$SPARK_HOME --kernel_name=\"Spark 2.4.1\" --interpreters=Scala \\\n && rm -f /tmp/toree-0.3.0.tar.gz \\\n && fix-permissions $ANACONDA_HOME \\\n && fix-permissions /home/$NB_USER\n#  SETUP PASSWORDLESS SSH FOR $NB_USER\nRUN ssh-keygen -q -N \"\" -t rsa -f /home/$NB_USER/.ssh/id_rsa \\\n && cp /home/$NB_USER/.ssh/id_rsa.pub /home/$NB_USER/.ssh/authorized_keys \\\n && chmod 0700 /home/$NB_USER\nUSER root\n#  SETUP PASSWORDLESS SSH\nRUN yes y | ssh-keygen -q -N \"\" -t dsa -f /etc/ssh/ssh_host_dsa_key \\\n && yes y | ssh-keygen -q -N \"\" -t rsa -f /etc/ssh/ssh_host_rsa_key \\\n && yes y | ssh-keygen -q -N \"\" -t rsa -f /root/.ssh/id_rsa \\\n && cp /root/.ssh/id_rsa.pub /root/.ssh/authorized_keys\nRUN ssh-keygen -A\nCOPY ssh_config /root/.ssh/config\nRUN chmod 600 /root/.ssh/config \\\n && chown root:root /root/.ssh/config \\\n && echo \"Port 2122\" >> /etc/ssh/sshd_config \\\n && echo \"${NB_USER} ALL=(ALL) NOPASSWD: ALL\" >> /etc/sudoers\nRUN service ssh restart\nCOPY ssh_config /home/$NB_USER/.ssh/config\nRUN chmod 600 /home/$NB_USER/.ssh/config \\\n && chown $NB_USER: /home/$NB_USER/.ssh/config\nCOPY bootstrap-yarn-spark.sh /usr/local/bin/\nRUN chown $NB_USER: /usr/local/bin/bootstrap-yarn-spark.sh \\\n && chmod 0700 /usr/local/bin/bootstrap-yarn-spark.sh\nCMD [\"/usr/local/bin/bootstrap-yarn-spark.sh\"]\nLABEL Hadoop.version=\"$HADOOP_VER\"\nLABEL Spark.version=\"$SPARK_VER\"\n#  Hdfs ports\nEXPOSE 50010/tcp 50020/tcp 50070/tcp 50075/tcp 50090/tcp 8020/tcp 9000/tcp 19888/tcp 8030/tcp 8031/tcp 8032/tcp 8033/tcp 8040/tcp 8042/tcp 8088/tcp 49707/tcp 2122/tcp\nUSER $NB_USER\n","injectedSmells":[],"originalDockerfileHash":"c9dbf766e66a35aa43c615219a1a1f58","successfullyInjectedSmells":[],"originalDockerfileUglified":"FROM ubuntu:bionic\nMAINTAINER akchinSTC\nARG NB_USER=\"jovyan\"\nARG NB_UID=\"1000\"\nARG NB_GID=\"100\"\nUSER root\nENV HADOOP_PREFIX=\"/usr/hdp/current/hadoop\" \\\n    ANACONDA_HOME=\"/opt/conda\"\nENV SHELL=\"/bin/bash\" \\\n    NB_USER=\"$NB_USER\" \\\n    NB_UID=\"$NB_UID\" \\\n    NB_GID=\"$NB_GID\" \\\n    LC_ALL=\"en_US.UTF-8\" \\\n    LANG=\"en_US.UTF-8\" \\\n    LANGUAGE=\"en_US.UTF-8\" \\\n    JAVA_HOME=\"/usr/lib/jvm/java-8-openjdk-amd64\" \\\n    SPARK_HOME=\"/usr/hdp/current/spark2-client\" \\\n    PYSPARK_PYTHON=\"$ANACONDA_HOME/bin/python\" \\\n    HADOOP_CONF_DIR=\"$HADOOP_PREFIX/etc/hadoop\"\nENV HOME=\"/home/$NB_USER\" \\\n    PATH=\"$ANACONDA_HOME/bin:$HADOOP_PREFIX/bin:$JAVA_HOME/bin:$SPARK_HOME/bin:$PATH\"\nENV SPARK_VER=\"2.4.1\"\nENV HADOOP_VER=\"2.7.7\"\n#   INSTALL / DOWNLOAD ALL NEEDED PACKAGES\nRUN apt-get update \\\n && apt-get -yq dist-upgrade \\\n && apt-get install --no-install-recommends wget bzip2 tar curl less nano ca-certificates libkrb5-dev sudo locales gcc fonts-liberation unzip libsm6 libxext-dev libxrender1 openjdk-8-jdk openssh-server openssh-client -yq \\\n && rm -rf /var/lib/apt/lists/*\nRUN echo \"en_US.UTF-8 UTF-8\" > /etc/locale.gen \\\n && locale-gen\nADD fix-permissions /usr/local/bin/fix-permissions\n#   Create jovyan user with UID=1000 and in the 'users' group\n#   and make sure these dirs are writable by the `users` group.\nRUN groupadd wheel -g 11 \\\n && echo \"auth required pam_wheel.so use_uid\" >> /etc/pam.d/su \\\n && useradd -m -s /bin/bash -N -u $NB_UID $NB_USER \\\n && mkdir -p $ANACONDA_HOME \\\n && mkdir -p /usr/hdp/current \\\n && mkdir -p /usr/local/share/jupyter \\\n && chown $NB_USER:$NB_GID $ANACONDA_HOME \\\n && chmod g+w /etc/passwd \\\n && chmod +x /usr/local/bin/fix-permissions \\\n && fix-permissions $HOME \\\n && fix-permissions $ANACONDA_HOME \\\n && fix-permissions /usr/hdp/current \\\n && fix-permissions /usr/local/share/jupyter\n#   Create service user 'jovyan'. Pin uid/gid to 1000.\nRUN useradd -m -s /bin/bash -N -u 1111 elyra \\\n && useradd -m -s /bin/bash -N -u 1112 bob \\\n && useradd -m -s /bin/bash -N -u 1113 alice\nUSER $NB_UID\n#   Setup work directory for backward-compatibility\nRUN mkdir /home/$NB_USER/work \\\n && fix-permissions /home/$NB_USER\n#   DOWNLOAD HADOOP AND SPARK\nRUN curl -s http://www.eu.apache.org/dist/hadoop/common/hadoop-$HADOOP_VER/hadoop-$HADOOP_VER.tar.gz | tar -xz -C /usr/hdp/current\nRUN curl -s https://archive.apache.org/dist/spark/spark-${SPARK_VER}/spark-${SPARK_VER}-bin-hadoop2.7.tgz | tar -xz -C /usr/hdp/current\n#   SETUP SPARK AND HADOOP SYMLINKS\nRUN cd /usr/hdp/current \\\n && ln -s ./hadoop-$HADOOP_VER hadoop \\\n && ln -s ./spark-${SPARK_VER}-bin-hadoop2.7 spark2-client\n#   INSTALL MINI-CONDA AND PYTHON PACKAGES\nRUN wget --quiet https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda.sh \\\n && /bin/bash ~/miniconda.sh -f -b -p $ANACONDA_HOME \\\n && rm ~/miniconda.sh \\\n && conda clean -tipsy \\\n && rm -rf /home/$NB_USER/.cache/yarn \\\n && fix-permissions $ANACONDA_HOME \\\n && fix-permissions /home/$NB_USER\nRUN conda install --yes --quiet 'pip' 'jupyter' 'r-devtools' 'r-stringr' 'r-argparse' \\\n && conda clean -tipsy \\\n && fix-permissions $ANACONDA_HOME \\\n && fix-permissions /home/$NB_USER\nRUN Rscript -e 'install.packages(\"IRkernel\", repos=\"http://cran.cnr.berkeley.edu\", lib=\"/opt/conda/lib/R/library\")' -e 'IRkernel::installspec(prefix = \"/usr/local\")' -e 'devtools::install_github(\"apache/spark@v2.4.1\", subdir=\"R/pkg\")' \\\n && fix-permissions $ANACONDA_HOME \\\n && fix-permissions /home/$NB_USER\n#   SETUP HADOOP CONFIGS\nRUN sed -i '/^export JAVA_HOME/ s:.*:export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\\nexport HADOOP_PREFIX=/usr/hdp/current/hadoop\\nexport HADOOP_HOME=/usr/hdp/current/hadoop\\n:' $HADOOP_PREFIX/etc/hadoop/hadoop-env.sh\nRUN sed -i '/^export HADOOP_CONF_DIR/ s:.*:export HADOOP_CONF_DIR=/usr/hdp/current/hadoop/etc/hadoop/:' $HADOOP_PREFIX/etc/hadoop/hadoop-env.sh\n#   SETUP PSEUDO - DISTRIBUTED CONFIGS FOR HADOOP\nCOPY core-site.xml.template hdfs-site.xml mapred-site.xml yarn-site.xml.template $HADOOP_PREFIX/etc/hadoop/\n#   working around docker.io build error\nRUN ls -la /usr/hdp/current/hadoop/etc/hadoop/*-env.sh \\\n && chmod +x /usr/hdp/current/hadoop/etc/hadoop/*-env.sh \\\n && ls -la /usr/hdp/current/hadoop/etc/hadoop/*-env.sh\n#   Install Toree\nRUN cd /tmp \\\n && curl -O https://dist.apache.org/repos/dist/release/incubator/toree/0.3.0-incubating/toree-pip/toree-0.3.0.tar.gz \\\n && pip install setuptools --upgrade --user \\\n && pip install /tmp/toree-0.3.0.tar.gz \\\n && jupyter toree install --spark_home=$SPARK_HOME --kernel_name=\"Spark 2.4.1\" --interpreters=Scala \\\n && rm -f /tmp/toree-0.3.0.tar.gz \\\n && fix-permissions $ANACONDA_HOME \\\n && fix-permissions /home/$NB_USER\n#   SETUP PASSWORDLESS SSH FOR $NB_USER\nRUN ssh-keygen -q -N \"\" -t rsa -f /home/$NB_USER/.ssh/id_rsa \\\n && cp /home/$NB_USER/.ssh/id_rsa.pub /home/$NB_USER/.ssh/authorized_keys \\\n && chmod 0700 /home/$NB_USER\nUSER root\n#   SETUP PASSWORDLESS SSH\nRUN yes y | ssh-keygen -q -N \"\" -t dsa -f /etc/ssh/ssh_host_dsa_key \\\n && yes y | ssh-keygen -q -N \"\" -t rsa -f /etc/ssh/ssh_host_rsa_key \\\n && yes y | ssh-keygen -q -N \"\" -t rsa -f /root/.ssh/id_rsa \\\n && cp /root/.ssh/id_rsa.pub /root/.ssh/authorized_keys\nRUN ssh-keygen -A\nCOPY ssh_config /root/.ssh/config\nRUN chmod 600 /root/.ssh/config \\\n && chown root:root /root/.ssh/config \\\n && echo \"Port 2122\" >> /etc/ssh/sshd_config \\\n && echo \"${NB_USER} ALL=(ALL) NOPASSWD: ALL\" >> /etc/sudoers\nRUN service ssh restart\nCOPY ssh_config /home/$NB_USER/.ssh/config\nRUN chmod 600 /home/$NB_USER/.ssh/config \\\n && chown $NB_USER: /home/$NB_USER/.ssh/config\nCOPY bootstrap-yarn-spark.sh /usr/local/bin/\nRUN chown $NB_USER: /usr/local/bin/bootstrap-yarn-spark.sh \\\n && chmod 0700 /usr/local/bin/bootstrap-yarn-spark.sh\nCMD [\"/usr/local/bin/bootstrap-yarn-spark.sh\"]\nLABEL Hadoop.version=\"$HADOOP_VER\"\nLABEL Spark.version=\"$SPARK_VER\"\n#   Hdfs ports\nEXPOSE 50010/tcp 50020/tcp 50070/tcp 50075/tcp 50090/tcp 8020/tcp 9000/tcp 19888/tcp 8030/tcp 8031/tcp 8032/tcp 8033/tcp 8040/tcp 8042/tcp 8088/tcp 49707/tcp 2122/tcp\nUSER $NB_USER\n","originalDockerfileUglifiedHash":"c0f0eba0a10061ee98a1ea19bd413567","fileName":"/ICSME-replicationpackage/dataset/smelly_dockerfiles_bianncle/2a3e26d53b9684aa641b67407f136a500a7a6305.dockerfile"}