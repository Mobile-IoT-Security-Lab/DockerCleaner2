{"seed":2669766489,"processedDockerfileHash":"5eb3af4c0c992e67b1b9668f0983fab6","fixedSmells":["use-no-install-recommends","do-not-use-apt-get-update-alone","pin-package-manager-versions-apt-get","pin-package-manager-versions-pip","pin-package-manager-versions-npm","pin-package-manager-versions-gem","pin-package-manager-versions-apk","use-copy-instead-of-add","use-wget-instead-of-add","do-not-have-secrets","have-a-healthcheck","have-a-user"],"successfullyFixedSmells":["pin-package-manager-versions-apt-get","pin-package-manager-versions-pip","have-a-healthcheck","have-a-user"],"processedDockerfile":"#   VERSION 1.8.1-1\n#   AUTHOR: Matthieu \"Puckel_\" Roisil\n#   DESCRIPTION: Basic Airflow container\n#   BUILD: docker build --rm -t puckel/docker-airflow .\n#   SOURCE: https://github.com/puckel/docker-airflow\nFROM python:3.6\n#   Never prompts the user for choices on installation/configuration of packages\nENV DEBIAN_FRONTEND=\"noninteractive\"\nENV TERM=\"linux\"\n#   Java\nARG JAVA_MAJOR_VERSION=8\nARG JAVA_MINOR_VERSION=181\n#   Spark\nARG SPARK_VERSION=2.3.1\n#   Airflow\nARG AIRFLOW_VERSION=1.9.0\nARG AIRFLOW_HOME=/usr/local/airflow\nENV AIRFLOW_HOME=\"/usr/local/airflow\"\n#   Define en_US.\nENV LANGUAGE=\"en_US.UTF-8\"\nENV LANG=\"en_US.UTF-8\"\nENV LC_ALL=\"en_US.UTF-8\"\nENV LC_CTYPE=\"en_US.UTF-8\"\nENV LC_MESSAGES=\"en_US.UTF-8\"\nENV LC_ALL=\"en_US.UTF-8\"\nRUN set -ex \\\n && buildDeps=' python3-dev libkrb5-dev libsasl2-dev libssl-dev libffi-dev build-essential libblas-dev liblapack-dev libpq-dev git ' \\\n && apt-get update -yqq \\\n && apt-get install --no-install-recommends python3-pip=20.3.4-4+deb11u1 python3-requests=2.25.1+dfsg-2 apt-utils=2.2.4 curl=7.74.0-1.3+deb11u7 netcat=1.10-46 locales=2.31-13+deb11u5 $buildDeps -yqq \\\n && sed -i 's/^# en_US.UTF-8 UTF-8$/en_US.UTF-8 UTF-8/g' /etc/locale.gen \\\n && locale-gen \\\n && update-locale LANG=en_US.UTF-8 LC_ALL=en_US.UTF-8 \\\n && useradd -ms /bin/bash -d ${AIRFLOW_HOME} airflow \\\n && python -m pip install -U pip setuptools wheel \\\n && pip install Cython==0.29.34 \\\n && pip install pytz==2023.3 \\\n && pip install pyOpenSSL==23.1.1 \\\n && pip install ndg-httpsclient==0.5.1 \\\n && pip install pytest==7.3.1 \\\n && pip install pyasn1==0.4.8 \\\n && pip install apache-airflow[crypto,celery,postgres,hive,jdbc]==$AIRFLOW_VERSION \\\n && pip install celery[redis]==3.1.17 \\\n && apt-get clean \\\n && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/* /usr/share/man /usr/share/doc /usr/share/doc-base\n#   Java\nRUN cd /opt/ \\\n && wget --no-cookies --no-check-certificate --header \"Cookie: gpw_e24=http%3A%2F%2Fwww.oracle.com%2F; oraclelicense=accept-securebackup-cookie\" \"http://download.oracle.com/otn-pub/java/jdk/8u181-b13/96a7b8442fe848ef90c96a2fad6ed6d1/jdk-${JAVA_MAJOR_VERSION}u${JAVA_MINOR_VERSION}-linux-x64.tar.gz\" -O jdk-${JAVA_MAJOR_VERSION}.tar.gz \\\n && tar xzf jdk-${JAVA_MAJOR_VERSION}.tar.gz \\\n && rm jdk-${JAVA_MAJOR_VERSION}.tar.gz \\\n && update-alternatives --install /usr/bin/java java /opt/jdk1.${JAVA_MAJOR_VERSION}.0_${JAVA_MINOR_VERSION}/bin/java 100 \\\n && update-alternatives --install /usr/bin/jar jar /opt/jdk1.${JAVA_MAJOR_VERSION}.0_${JAVA_MINOR_VERSION}/bin/jar 100 \\\n && update-alternatives --install /usr/bin/javac javac /opt/jdk1.${JAVA_MAJOR_VERSION}.0_${JAVA_MINOR_VERSION}/bin/javac 100\n#   SPARK\nRUN cd /usr/ \\\n && wget \"http://www-eu.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop2.7.tgz\" \\\n && tar xzf spark-${SPARK_VERSION}-bin-hadoop2.7.tgz \\\n && rm spark-${SPARK_VERSION}-bin-hadoop2.7.tgz \\\n && mv spark-${SPARK_VERSION}-bin-hadoop2.7 spark\nENV SPARK_HOME=\"/usr/spark\"\nENV SPARK_MAJOR_VERSION=\"2\"\nENV PYTHONPATH=\"$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$SPARK_HOME/python/:$PYTHONPATH\"\nRUN mkdir -p /usr/spark/work/ \\\n && chmod -R 777 /usr/spark/work/\nENV SPARK_MASTER_PORT=\"7077\"\nCOPY docker_files/entrypoint.sh /entrypoint.sh\nCOPY docker_files/airflow.cfg ${AIRFLOW_HOME}/airflow.cfg\nRUN chown -R airflow: ${AIRFLOW_HOME}\nRUN chown airflow: /entrypoint.sh\nRUN chmod +x /entrypoint.sh\nEXPOSE 8080/tcp 5555/tcp 8793/tcp\nWORKDIR ${AIRFLOW_HOME}\n#   dev\nRUN mkdir -p ${AIRFLOW_HOME}/dags\nCOPY docker_files/populate_tables.py /usr/local/airflow/populate_tables.py\nRUN cd ${AIRFLOW_HOME}/dags \\\n && git clone https://github.com/danielvdende/data-testing-with-airflow.git development\nRUN cd ${AIRFLOW_HOME}/dags/development \\\n && git checkout development\nCOPY docker_files/dev.conf ${AIRFLOW_HOME}/dags/development/dags/environment.conf\n#   tst\nRUN cd ${AIRFLOW_HOME}/dags \\\n && git clone https://github.com/danielvdende/data-testing-with-airflow.git test\nRUN cd ${AIRFLOW_HOME}/dags/test \\\n && git checkout test\nCOPY docker_files/tst.conf ${AIRFLOW_HOME}/dags/test/dags/environment.conf\n#\n#  # acc\nRUN cd ${AIRFLOW_HOME}/dags \\\n && git clone https://github.com/danielvdende/data-testing-with-airflow.git acceptance\nRUN cd ${AIRFLOW_HOME}/dags/acceptance \\\n && git checkout acceptance\nCOPY docker_files/acc.conf ${AIRFLOW_HOME}/dags/acceptance/dags/environment.conf\n#\n#  # prd\nRUN cd ${AIRFLOW_HOME}/dags \\\n && git clone https://github.com/danielvdende/data-testing-with-airflow.git production\nRUN cd ${AIRFLOW_HOME}/dags/production \\\n && git checkout master\nCOPY docker_files/prd.conf ${AIRFLOW_HOME}/dags/production/dags/environment.conf\nENTRYPOINT /entrypoint.sh\nRUN cd /usr/local/airflow \\\n && /usr/spark/bin/spark-submit --master local populate_tables.py\nRUN groupadd --system docker-user ; useradd --system --gid docker-user docker-user\nUSER docker-user\n# Please add your HEALTHCHECK here!!!\n","originalDockerfile":"#  VERSION 1.8.1-1\n#  AUTHOR: Matthieu \"Puckel_\" Roisil\n#  DESCRIPTION: Basic Airflow container\n#  BUILD: docker build --rm -t puckel/docker-airflow .\n#  SOURCE: https://github.com/puckel/docker-airflow\nFROM python:3.6\n#  Never prompts the user for choices on installation/configuration of packages\nENV DEBIAN_FRONTEND=\"noninteractive\"\nENV TERM=\"linux\"\n#  Java\nARG JAVA_MAJOR_VERSION=8\nARG JAVA_MINOR_VERSION=181\n#  Spark\nARG SPARK_VERSION=2.3.1\n#  Airflow\nARG AIRFLOW_VERSION=1.9.0\nARG AIRFLOW_HOME=/usr/local/airflow\nENV AIRFLOW_HOME=\"/usr/local/airflow\"\n#  Define en_US.\nENV LANGUAGE=\"en_US.UTF-8\"\nENV LANG=\"en_US.UTF-8\"\nENV LC_ALL=\"en_US.UTF-8\"\nENV LC_CTYPE=\"en_US.UTF-8\"\nENV LC_MESSAGES=\"en_US.UTF-8\"\nENV LC_ALL=\"en_US.UTF-8\"\nRUN set -ex \\\n && buildDeps=' python3-dev libkrb5-dev libsasl2-dev libssl-dev libffi-dev build-essential libblas-dev liblapack-dev libpq-dev git ' \\\n && apt-get update -yqq \\\n && apt-get install --no-install-recommends python3-pip python3-requests apt-utils curl netcat locales $buildDeps -yqq \\\n && sed -i 's/^# en_US.UTF-8 UTF-8$/en_US.UTF-8 UTF-8/g' /etc/locale.gen \\\n && locale-gen \\\n && update-locale LANG=en_US.UTF-8 LC_ALL=en_US.UTF-8 \\\n && useradd -ms /bin/bash -d ${AIRFLOW_HOME} airflow \\\n && python -m pip install -U pip setuptools wheel \\\n && pip install Cython \\\n && pip install pytz \\\n && pip install pyOpenSSL \\\n && pip install ndg-httpsclient \\\n && pip install pytest \\\n && pip install pyasn1 \\\n && pip install apache-airflow[crypto,celery,postgres,hive,jdbc]==$AIRFLOW_VERSION \\\n && pip install celery[redis]==3.1.17 \\\n && apt-get clean \\\n && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/* /usr/share/man /usr/share/doc /usr/share/doc-base\n#  Java\nRUN cd /opt/ \\\n && wget --no-cookies --no-check-certificate --header \"Cookie: gpw_e24=http%3A%2F%2Fwww.oracle.com%2F; oraclelicense=accept-securebackup-cookie\" \"http://download.oracle.com/otn-pub/java/jdk/8u181-b13/96a7b8442fe848ef90c96a2fad6ed6d1/jdk-${JAVA_MAJOR_VERSION}u${JAVA_MINOR_VERSION}-linux-x64.tar.gz\" -O jdk-${JAVA_MAJOR_VERSION}.tar.gz \\\n && tar xzf jdk-${JAVA_MAJOR_VERSION}.tar.gz \\\n && rm jdk-${JAVA_MAJOR_VERSION}.tar.gz \\\n && update-alternatives --install /usr/bin/java java /opt/jdk1.${JAVA_MAJOR_VERSION}.0_${JAVA_MINOR_VERSION}/bin/java 100 \\\n && update-alternatives --install /usr/bin/jar jar /opt/jdk1.${JAVA_MAJOR_VERSION}.0_${JAVA_MINOR_VERSION}/bin/jar 100 \\\n && update-alternatives --install /usr/bin/javac javac /opt/jdk1.${JAVA_MAJOR_VERSION}.0_${JAVA_MINOR_VERSION}/bin/javac 100\n#  SPARK\nRUN cd /usr/ \\\n && wget \"http://www-eu.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop2.7.tgz\" \\\n && tar xzf spark-${SPARK_VERSION}-bin-hadoop2.7.tgz \\\n && rm spark-${SPARK_VERSION}-bin-hadoop2.7.tgz \\\n && mv spark-${SPARK_VERSION}-bin-hadoop2.7 spark\nENV SPARK_HOME=\"/usr/spark\"\nENV SPARK_MAJOR_VERSION=\"2\"\nENV PYTHONPATH=\"$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$SPARK_HOME/python/:$PYTHONPATH\"\nRUN mkdir -p /usr/spark/work/ \\\n && chmod -R 777 /usr/spark/work/\nENV SPARK_MASTER_PORT=\"7077\"\nCOPY docker_files/entrypoint.sh /entrypoint.sh\nCOPY docker_files/airflow.cfg ${AIRFLOW_HOME}/airflow.cfg\nRUN chown -R airflow: ${AIRFLOW_HOME}\nRUN chown airflow: /entrypoint.sh\nRUN chmod +x /entrypoint.sh\nEXPOSE 8080/tcp 5555/tcp 8793/tcp\nWORKDIR ${AIRFLOW_HOME}\n#  dev\nRUN mkdir -p ${AIRFLOW_HOME}/dags\nCOPY docker_files/populate_tables.py /usr/local/airflow/populate_tables.py\nRUN cd ${AIRFLOW_HOME}/dags \\\n && git clone https://github.com/danielvdende/data-testing-with-airflow.git development\nRUN cd ${AIRFLOW_HOME}/dags/development \\\n && git checkout development\nCOPY docker_files/dev.conf ${AIRFLOW_HOME}/dags/development/dags/environment.conf\n#  tst\nRUN cd ${AIRFLOW_HOME}/dags \\\n && git clone https://github.com/danielvdende/data-testing-with-airflow.git test\nRUN cd ${AIRFLOW_HOME}/dags/test \\\n && git checkout test\nCOPY docker_files/tst.conf ${AIRFLOW_HOME}/dags/test/dags/environment.conf\n#\n# # acc\nRUN cd ${AIRFLOW_HOME}/dags \\\n && git clone https://github.com/danielvdende/data-testing-with-airflow.git acceptance\nRUN cd ${AIRFLOW_HOME}/dags/acceptance \\\n && git checkout acceptance\nCOPY docker_files/acc.conf ${AIRFLOW_HOME}/dags/acceptance/dags/environment.conf\n#\n# # prd\nRUN cd ${AIRFLOW_HOME}/dags \\\n && git clone https://github.com/danielvdende/data-testing-with-airflow.git production\nRUN cd ${AIRFLOW_HOME}/dags/production \\\n && git checkout master\nCOPY docker_files/prd.conf ${AIRFLOW_HOME}/dags/production/dags/environment.conf\nENTRYPOINT /entrypoint.sh\nRUN cd /usr/local/airflow \\\n && /usr/spark/bin/spark-submit --master local populate_tables.py\n","injectedSmells":[],"originalDockerfileHash":"734da322db3f4b2d22a699789f216759","successfullyInjectedSmells":[],"originalDockerfileUglified":"#   VERSION 1.8.1-1\n#   AUTHOR: Matthieu \"Puckel_\" Roisil\n#   DESCRIPTION: Basic Airflow container\n#   BUILD: docker build --rm -t puckel/docker-airflow .\n#   SOURCE: https://github.com/puckel/docker-airflow\nFROM python:3.6\n#   Never prompts the user for choices on installation/configuration of packages\nENV DEBIAN_FRONTEND=\"noninteractive\"\nENV TERM=\"linux\"\n#   Java\nARG JAVA_MAJOR_VERSION=8\nARG JAVA_MINOR_VERSION=181\n#   Spark\nARG SPARK_VERSION=2.3.1\n#   Airflow\nARG AIRFLOW_VERSION=1.9.0\nARG AIRFLOW_HOME=/usr/local/airflow\nENV AIRFLOW_HOME=\"/usr/local/airflow\"\n#   Define en_US.\nENV LANGUAGE=\"en_US.UTF-8\"\nENV LANG=\"en_US.UTF-8\"\nENV LC_ALL=\"en_US.UTF-8\"\nENV LC_CTYPE=\"en_US.UTF-8\"\nENV LC_MESSAGES=\"en_US.UTF-8\"\nENV LC_ALL=\"en_US.UTF-8\"\nRUN set -ex \\\n && buildDeps=' python3-dev libkrb5-dev libsasl2-dev libssl-dev libffi-dev build-essential libblas-dev liblapack-dev libpq-dev git ' \\\n && apt-get update -yqq \\\n && apt-get install --no-install-recommends python3-pip python3-requests apt-utils curl netcat locales $buildDeps -yqq \\\n && sed -i 's/^# en_US.UTF-8 UTF-8$/en_US.UTF-8 UTF-8/g' /etc/locale.gen \\\n && locale-gen \\\n && update-locale LANG=en_US.UTF-8 LC_ALL=en_US.UTF-8 \\\n && useradd -ms /bin/bash -d ${AIRFLOW_HOME} airflow \\\n && python -m pip install -U pip setuptools wheel \\\n && pip install Cython \\\n && pip install pytz \\\n && pip install pyOpenSSL \\\n && pip install ndg-httpsclient \\\n && pip install pytest \\\n && pip install pyasn1 \\\n && pip install apache-airflow[crypto,celery,postgres,hive,jdbc]==$AIRFLOW_VERSION \\\n && pip install celery[redis]==3.1.17 \\\n && apt-get clean \\\n && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/* /usr/share/man /usr/share/doc /usr/share/doc-base\n#   Java\nRUN cd /opt/ \\\n && wget --no-cookies --no-check-certificate --header \"Cookie: gpw_e24=http%3A%2F%2Fwww.oracle.com%2F; oraclelicense=accept-securebackup-cookie\" \"http://download.oracle.com/otn-pub/java/jdk/8u181-b13/96a7b8442fe848ef90c96a2fad6ed6d1/jdk-${JAVA_MAJOR_VERSION}u${JAVA_MINOR_VERSION}-linux-x64.tar.gz\" -O jdk-${JAVA_MAJOR_VERSION}.tar.gz \\\n && tar xzf jdk-${JAVA_MAJOR_VERSION}.tar.gz \\\n && rm jdk-${JAVA_MAJOR_VERSION}.tar.gz \\\n && update-alternatives --install /usr/bin/java java /opt/jdk1.${JAVA_MAJOR_VERSION}.0_${JAVA_MINOR_VERSION}/bin/java 100 \\\n && update-alternatives --install /usr/bin/jar jar /opt/jdk1.${JAVA_MAJOR_VERSION}.0_${JAVA_MINOR_VERSION}/bin/jar 100 \\\n && update-alternatives --install /usr/bin/javac javac /opt/jdk1.${JAVA_MAJOR_VERSION}.0_${JAVA_MINOR_VERSION}/bin/javac 100\n#   SPARK\nRUN cd /usr/ \\\n && wget \"http://www-eu.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop2.7.tgz\" \\\n && tar xzf spark-${SPARK_VERSION}-bin-hadoop2.7.tgz \\\n && rm spark-${SPARK_VERSION}-bin-hadoop2.7.tgz \\\n && mv spark-${SPARK_VERSION}-bin-hadoop2.7 spark\nENV SPARK_HOME=\"/usr/spark\"\nENV SPARK_MAJOR_VERSION=\"2\"\nENV PYTHONPATH=\"$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$SPARK_HOME/python/:$PYTHONPATH\"\nRUN mkdir -p /usr/spark/work/ \\\n && chmod -R 777 /usr/spark/work/\nENV SPARK_MASTER_PORT=\"7077\"\nCOPY docker_files/entrypoint.sh /entrypoint.sh\nCOPY docker_files/airflow.cfg ${AIRFLOW_HOME}/airflow.cfg\nRUN chown -R airflow: ${AIRFLOW_HOME}\nRUN chown airflow: /entrypoint.sh\nRUN chmod +x /entrypoint.sh\nEXPOSE 8080/tcp 5555/tcp 8793/tcp\nWORKDIR ${AIRFLOW_HOME}\n#   dev\nRUN mkdir -p ${AIRFLOW_HOME}/dags\nCOPY docker_files/populate_tables.py /usr/local/airflow/populate_tables.py\nRUN cd ${AIRFLOW_HOME}/dags \\\n && git clone https://github.com/danielvdende/data-testing-with-airflow.git development\nRUN cd ${AIRFLOW_HOME}/dags/development \\\n && git checkout development\nCOPY docker_files/dev.conf ${AIRFLOW_HOME}/dags/development/dags/environment.conf\n#   tst\nRUN cd ${AIRFLOW_HOME}/dags \\\n && git clone https://github.com/danielvdende/data-testing-with-airflow.git test\nRUN cd ${AIRFLOW_HOME}/dags/test \\\n && git checkout test\nCOPY docker_files/tst.conf ${AIRFLOW_HOME}/dags/test/dags/environment.conf\n#\n#  # acc\nRUN cd ${AIRFLOW_HOME}/dags \\\n && git clone https://github.com/danielvdende/data-testing-with-airflow.git acceptance\nRUN cd ${AIRFLOW_HOME}/dags/acceptance \\\n && git checkout acceptance\nCOPY docker_files/acc.conf ${AIRFLOW_HOME}/dags/acceptance/dags/environment.conf\n#\n#  # prd\nRUN cd ${AIRFLOW_HOME}/dags \\\n && git clone https://github.com/danielvdende/data-testing-with-airflow.git production\nRUN cd ${AIRFLOW_HOME}/dags/production \\\n && git checkout master\nCOPY docker_files/prd.conf ${AIRFLOW_HOME}/dags/production/dags/environment.conf\nENTRYPOINT /entrypoint.sh\nRUN cd /usr/local/airflow \\\n && /usr/spark/bin/spark-submit --master local populate_tables.py\n","originalDockerfileUglifiedHash":"16096ea21c57d20082fb8d480a43c8df","fileName":"/ICSME-replicationpackage/dataset/smelly_dockerfiles_bianncle/1e25d93db8aebd3142c5332ff6b85321f8e52793.dockerfile"}