{"seed":156932190,"processedDockerfileHash":"630f2759bb1bd896bd529c3dd6158212","fixedSmells":["use-no-install-recommends","do-not-use-apt-get-update-alone","pin-package-manager-versions-apt-get","pin-package-manager-versions-pip","pin-package-manager-versions-npm","pin-package-manager-versions-gem","pin-package-manager-versions-apk","use-copy-instead-of-add","use-wget-instead-of-add","do-not-have-secrets","have-a-healthcheck","have-a-user"],"successfullyFixedSmells":["do-not-use-apt-get-update-alone","pin-package-manager-versions-apt-get","pin-package-manager-versions-npm","have-a-healthcheck","have-a-user"],"processedDockerfile":"FROM fluxproject/ros_base\nRUN apt-get update \\\n && (apt-get update ;apt-get install --no-install-recommends locales=2.37-0ubuntu2 bzip2=1.0.8-5build1 tree=2.1.0-1 unzip=6.0-27ubuntu1 xz-utils=5.4.1-0.2 curl=7.88.1-7ubuntu1 wget=1.21.3-1ubuntu1 iproute2=6.1.0-1ubuntu2 sudo=1.9.13p1-1ubuntu2 python-pip python3-pip=23.0.1+dfsg-1 python-setuptools python3-setuptools=66.1.1-1 openjdk-8-jdk-headless=8u362-ga-0ubuntu2 nodejs=18.13.0+dfsg1-1ubuntu2 npm=9.2.0~ds1-1 nodejs-legacy iputils-ping=3:20221126-1 net-tools=2.10-0.1ubuntu3 iproute knot-dnsutils=3.2.5-1 vim=2:9.0.1000-4ubuntu2 ffmpeg=7:5.1.2-3ubuntu1 -y ) \\\n && rm -rf /var/lib/apt/lists/*\n#   Jupyterhub setting\nRUN mkdir -p /etc/jupyterhub\nCOPY jupyterhub_config.py /etc/jupyterhub/\n#   Introduce flux user   # TODO: link\nRUN npm install configurable-http-proxy@4.5.5 -g\nRUN useradd -u 11111 -m -s /bin/bash flux\nRUN usermod -aG sudo flux\nRUN bash -c \" echo flux:flux | chpasswd \"\n#   Flux user setting    # TODO: link\nCOPY spark-ex-kubernetes.sh /home/flux/\nRUN python2 -m pip install --upgrade --user pip \\\n && python3 -m pip install --upgrade --user pip \\\n && python3 -m pip install --no-cache-dir --upgrade jupyter jupyterhub jupyterlab \\\n && python2 -m pip install --no-cache-dir --upgrade pyspark matplotlib pandas tensorflow keras Pillow \\\n && python2 -m pip install --no-cache-dir --upgrade --force-reinstall requests imageio moviepy seaborn gmaps \\\n && python2 -m pip install ipykernel \\\n && python2 -m ipykernel install \\\n && python3 -m pip install ipykernel \\\n && python3 -m ipykernel install\nRUN apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 66F84AE1EB71A8AC108087DCAF677210FF6D3CDA \\\n && bash -c 'echo \"deb [ arch=amd64 ] http://packages.dataspeedinc.com/ros/ubuntu $(lsb_release -sc) main\" > /etc/apt/sources.list.d/ros-dataspeed-public.list' \\\n && :\nRUN bash -c 'echo \"yaml http://packages.dataspeedinc.com/ros/ros-public-'$ROS_DISTRO'.yaml '$ROS_DISTRO'\" > /etc/ros/rosdep/sources.list.d/30-dataspeed-public-'$ROS_DISTRO'.list' \\\n && rosdep update 2> /dev/null \\\n && (apt-get update ;apt-get install --no-install-recommends ros-$ROS_DISTRO-dbw-mkz ros-$ROS_DISTRO-mobility-base ros-$ROS_DISTRO-baxter-sdk ros-$ROS_DISTRO-velodyne -y ) \\\n && rm -rf /var/lib/apt/lists/*\n#   Default to UTF-8\nRUN locale-gen en_US.UTF-8\nENV LANG=\"en_US.UTF-8\"\nENV JAVA_HOME=\"/usr/lib/jvm/java-8-openjdk-amd64\"\nENV PATH=\"$PATH:/opt/apache/hadoop/bin\"\nENV ROSIF_JAR=\"/opt/ros_hadoop/master/lib/rosbaginputformat.jar\"\nRUN mkdir -p /opt/ros_hadoop/master/dist/\nRUN mkdir -p /opt/apache/\nRUN mkdir -p /opt/ros_spark/dist/\nCOPY . /opt/ros_hadoop/master/\n#   TODO: ENV ROS_HADOOP='0.9.11'\n#   RUN \\\n#     curl -s \"https://codeload.github.com/valtech/ros_hadoop/tar.gz/v${ROS_HADOOP}\" | \\\n#     tar -C /opt/ros_hadoop -xvzf - && \\\n#     mv /opt/ros_hadoop/ros_hadoop-${ROS_HADOOP} /opt/ros_hadoop/latest\nRUN curl -s \"https://codeload.github.com/valtech/ros_hadoop/tar.gz/master\" | tar -C /opt/ros_hadoop -xvzf - \\\n && mv /opt/ros_hadoop/ros_hadoop-master /opt/ros_hadoop/latest\nRUN bash -c \"if [ ! -f /opt/ros_hadoop/master/dist/hadoop-3.0.0.tar.gz ] ; then wget --no-check-certificate -O /opt/ros_hadoop/master/dist/hadoop-3.1.1.tar.gz -q https://www.eu.apache.org/dist/hadoop/common/hadoop-3.1.1/hadoop-3.1.1.tar.gz ; fi\"\nRUN tar -xzf /opt/ros_hadoop/master/dist/hadoop-3.1.1.tar.gz -C /opt/apache \\\n && rm /opt/ros_hadoop/master/dist/hadoop-3.1.1.tar.gz\nRUN ln -s /opt/apache/hadoop-3.1.1 /opt/apache/hadoop\nRUN bash -c \"if [ ! -f /opt/ros_hadoop/latest/lib/rosbaginputformat.jar ] ; then ln -s /opt/ros_hadoop/master/lib/rosbaginputformat.jar /opt/ros_hadoop/latest/lib/rosbaginputformat.jar ; fi\"\n#  # for spark example tests\nRUN bash -c \"if [ ! -f /opt/ros_spark/dist/spark-2.3.1-bin-hadoop2.7.tgz ] ; then wget --quiet -O /opt/ros_spark/dist/spark-2.3.1-bin-hadoop2.7.tgz http://apache.lauf-forum.at/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz ; fi\"\nRUN tar -xzf /opt/ros_spark/dist/spark-2.3.1-bin-hadoop2.7.tgz -C /opt/apache \\\n && rm /opt/ros_spark/dist/spark-2.3.1-bin-hadoop2.7.tgz\nRUN printf \"<configuration>\\n\\n<property>\\n<name>fs.defaultFS</name>\\n<value>hdfs://localhost:9000</value>\\n</property>\\n</configuration>\" > /opt/apache/hadoop/etc/hadoop/core-site.xml \\\n && printf \"<configuration>\\n<property>\\n<name>dfs.replication</name>\\n<value>1</value>\\n</property>\\n</configuration>\" > /opt/apache/hadoop/etc/hadoop/hdfs-site.xml \\\n && bash -c \"/opt/apache/hadoop/bin/hdfs namenode -format 2>/dev/null\" \\\n && printf \"#! /bin/bash\\n/opt/apache/hadoop/bin/hdfs --daemon stop datanode\\n/opt/apache/hadoop/bin/hdfs --daemon stop namenode\\n/opt/apache/hadoop/bin/hdfs --daemon start namenode\\n/opt/apache/hadoop/bin/hdfs --daemon start datanode\\nexec \\\"$@\\\"\\n\" > /start_hadoop.sh \\\n && chmod a+x /start_hadoop.sh\nRUN printf \"#! /bin/bash\\nset -e\\nsource \\\"/opt/ros/$ROS_DISTRO/setup.bash\\\"\\n/start_hadoop.sh\\nexec \\\"$@\\\"\\n\" > /ros_hadoop.sh \\\n && chmod a+x /ros_hadoop.sh\nRUN bash -c \"if [ ! -f /opt/ros_hadoop/master/dist/HMB_4.bag ] ; then wget --quiet -O /opt/ros_hadoop/master/dist/HMB_4.bag https://xfiles.valtech.io/f/c494d168522045e3bcc0/?dl=1 ; fi\" \\\n && java -jar \"$ROSIF_JAR\" -f /opt/ros_hadoop/master/dist/HMB_4.bag\nRUN bash -c \"/start_hadoop.sh\" \\\n && until /opt/apache/hadoop/bin/hdfs dfsadmin -safemode wait ; do sleep 1s ; done \\\n && until /opt/apache/hadoop/bin/hdfs dfsadmin -report ; do sleep 1s ; done \\\n && until /opt/apache/hadoop/bin/hdfs dfs -mkdir /user ; do sleep 1s ; done \\\n && /opt/apache/hadoop/bin/hdfs dfs -mkdir /user/root \\\n && /opt/apache/hadoop/bin/hdfs dfs -mkdir /user/flux \\\n && /opt/apache/hadoop/bin/hdfs dfs -put /opt/ros_hadoop/master/dist/HMB_4.bag \\\n && /opt/apache/hadoop/bin/hdfs --daemon stop datanode \\\n && /opt/apache/hadoop/bin/hdfs --daemon stop namenode\nRUN mkdir -p /ope/ros_hadoop/latest/doc \\\n && chmod -R 777 /opt/ros_hadoop\nWORKDIR /opt/ros_hadoop/latest/doc/\nENTRYPOINT [\"/ros_hadoop.sh\"]\nCMD [\"jupyterhub\", \"-f\", \"/etc/jupyterhub/jupyterhub_config.py\"]\nRUN groupadd --system docker-user ; useradd --system --gid docker-user docker-user\nUSER docker-user\n# Please add your HEALTHCHECK here!!!\n","originalDockerfile":"FROM fluxproject/ros_base\nRUN apt-get update \\\n && apt-get install --no-install-recommends locales bzip2 tree unzip xz-utils curl wget iproute2 sudo python-pip python3-pip python-setuptools python3-setuptools openjdk-8-jdk-headless nodejs npm nodejs-legacy iputils-ping net-tools iproute knot-dnsutils vim ffmpeg -y \\\n && rm -rf /var/lib/apt/lists/*\n#  Jupyterhub setting\nRUN mkdir -p /etc/jupyterhub\nCOPY jupyterhub_config.py /etc/jupyterhub/\n#  Introduce flux user   # TODO: link\nRUN npm install configurable-http-proxy -g\nRUN useradd -u 11111 -m -s /bin/bash flux\nRUN usermod -aG sudo flux\nRUN bash -c \" echo flux:flux | chpasswd \"\n#  Flux user setting    # TODO: link\nCOPY spark-ex-kubernetes.sh /home/flux/\nRUN python2 -m pip install --upgrade --user pip \\\n && python3 -m pip install --upgrade --user pip \\\n && python3 -m pip install --no-cache-dir --upgrade jupyter jupyterhub jupyterlab \\\n && python2 -m pip install --no-cache-dir --upgrade pyspark matplotlib pandas tensorflow keras Pillow \\\n && python2 -m pip install --no-cache-dir --upgrade --force-reinstall requests imageio moviepy seaborn gmaps \\\n && python2 -m pip install ipykernel \\\n && python2 -m ipykernel install \\\n && python3 -m pip install ipykernel \\\n && python3 -m ipykernel install\nRUN apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 66F84AE1EB71A8AC108087DCAF677210FF6D3CDA \\\n && bash -c 'echo \"deb [ arch=amd64 ] http://packages.dataspeedinc.com/ros/ubuntu $(lsb_release -sc) main\" > /etc/apt/sources.list.d/ros-dataspeed-public.list' \\\n && apt-get update\nRUN bash -c 'echo \"yaml http://packages.dataspeedinc.com/ros/ros-public-'$ROS_DISTRO'.yaml '$ROS_DISTRO'\" > /etc/ros/rosdep/sources.list.d/30-dataspeed-public-'$ROS_DISTRO'.list' \\\n && rosdep update 2> /dev/null \\\n && apt-get install --no-install-recommends ros-$ROS_DISTRO-dbw-mkz ros-$ROS_DISTRO-mobility-base ros-$ROS_DISTRO-baxter-sdk ros-$ROS_DISTRO-velodyne -y \\\n && rm -rf /var/lib/apt/lists/*\n#  Default to UTF-8\nRUN locale-gen en_US.UTF-8\nENV LANG=\"en_US.UTF-8\"\nENV JAVA_HOME=\"/usr/lib/jvm/java-8-openjdk-amd64\"\nENV PATH=\"$PATH:/opt/apache/hadoop/bin\"\nENV ROSIF_JAR=\"/opt/ros_hadoop/master/lib/rosbaginputformat.jar\"\nRUN mkdir -p /opt/ros_hadoop/master/dist/\nRUN mkdir -p /opt/apache/\nRUN mkdir -p /opt/ros_spark/dist/\nCOPY . /opt/ros_hadoop/master/\n#  TODO: ENV ROS_HADOOP='0.9.11'\n#  RUN \\\n#    curl -s \"https://codeload.github.com/valtech/ros_hadoop/tar.gz/v${ROS_HADOOP}\" | \\\n#    tar -C /opt/ros_hadoop -xvzf - && \\\n#    mv /opt/ros_hadoop/ros_hadoop-${ROS_HADOOP} /opt/ros_hadoop/latest\nRUN curl -s \"https://codeload.github.com/valtech/ros_hadoop/tar.gz/master\" | tar -C /opt/ros_hadoop -xvzf - \\\n && mv /opt/ros_hadoop/ros_hadoop-master /opt/ros_hadoop/latest\nRUN bash -c \"if [ ! -f /opt/ros_hadoop/master/dist/hadoop-3.0.0.tar.gz ] ; then wget --no-check-certificate -O /opt/ros_hadoop/master/dist/hadoop-3.1.1.tar.gz -q https://www.eu.apache.org/dist/hadoop/common/hadoop-3.1.1/hadoop-3.1.1.tar.gz ; fi\"\nRUN tar -xzf /opt/ros_hadoop/master/dist/hadoop-3.1.1.tar.gz -C /opt/apache \\\n && rm /opt/ros_hadoop/master/dist/hadoop-3.1.1.tar.gz\nRUN ln -s /opt/apache/hadoop-3.1.1 /opt/apache/hadoop\nRUN bash -c \"if [ ! -f /opt/ros_hadoop/latest/lib/rosbaginputformat.jar ] ; then ln -s /opt/ros_hadoop/master/lib/rosbaginputformat.jar /opt/ros_hadoop/latest/lib/rosbaginputformat.jar ; fi\"\n# # for spark example tests\nRUN bash -c \"if [ ! -f /opt/ros_spark/dist/spark-2.3.1-bin-hadoop2.7.tgz ] ; then wget --quiet -O /opt/ros_spark/dist/spark-2.3.1-bin-hadoop2.7.tgz http://apache.lauf-forum.at/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz ; fi\"\nRUN tar -xzf /opt/ros_spark/dist/spark-2.3.1-bin-hadoop2.7.tgz -C /opt/apache \\\n && rm /opt/ros_spark/dist/spark-2.3.1-bin-hadoop2.7.tgz\nRUN printf \"<configuration>\\n\\n<property>\\n<name>fs.defaultFS</name>\\n<value>hdfs://localhost:9000</value>\\n</property>\\n</configuration>\" > /opt/apache/hadoop/etc/hadoop/core-site.xml \\\n && printf \"<configuration>\\n<property>\\n<name>dfs.replication</name>\\n<value>1</value>\\n</property>\\n</configuration>\" > /opt/apache/hadoop/etc/hadoop/hdfs-site.xml \\\n && bash -c \"/opt/apache/hadoop/bin/hdfs namenode -format 2>/dev/null\" \\\n && printf \"#! /bin/bash\\n/opt/apache/hadoop/bin/hdfs --daemon stop datanode\\n/opt/apache/hadoop/bin/hdfs --daemon stop namenode\\n/opt/apache/hadoop/bin/hdfs --daemon start namenode\\n/opt/apache/hadoop/bin/hdfs --daemon start datanode\\nexec \\\"$@\\\"\\n\" > /start_hadoop.sh \\\n && chmod a+x /start_hadoop.sh\nRUN printf \"#! /bin/bash\\nset -e\\nsource \\\"/opt/ros/$ROS_DISTRO/setup.bash\\\"\\n/start_hadoop.sh\\nexec \\\"$@\\\"\\n\" > /ros_hadoop.sh \\\n && chmod a+x /ros_hadoop.sh\nRUN bash -c \"if [ ! -f /opt/ros_hadoop/master/dist/HMB_4.bag ] ; then wget --quiet -O /opt/ros_hadoop/master/dist/HMB_4.bag https://xfiles.valtech.io/f/c494d168522045e3bcc0/?dl=1 ; fi\" \\\n && java -jar \"$ROSIF_JAR\" -f /opt/ros_hadoop/master/dist/HMB_4.bag\nRUN bash -c \"/start_hadoop.sh\" \\\n && until /opt/apache/hadoop/bin/hdfs dfsadmin -safemode wait ; do sleep 1s ; done \\\n && until /opt/apache/hadoop/bin/hdfs dfsadmin -report ; do sleep 1s ; done \\\n && until /opt/apache/hadoop/bin/hdfs dfs -mkdir /user ; do sleep 1s ; done \\\n && /opt/apache/hadoop/bin/hdfs dfs -mkdir /user/root \\\n && /opt/apache/hadoop/bin/hdfs dfs -mkdir /user/flux \\\n && /opt/apache/hadoop/bin/hdfs dfs -put /opt/ros_hadoop/master/dist/HMB_4.bag \\\n && /opt/apache/hadoop/bin/hdfs --daemon stop datanode \\\n && /opt/apache/hadoop/bin/hdfs --daemon stop namenode\nRUN mkdir -p /ope/ros_hadoop/latest/doc \\\n && chmod -R 777 /opt/ros_hadoop\nWORKDIR /opt/ros_hadoop/latest/doc/\nENTRYPOINT [\"/ros_hadoop.sh\"]\nCMD [\"jupyterhub\", \"-f\", \"/etc/jupyterhub/jupyterhub_config.py\"]\n","injectedSmells":[],"originalDockerfileHash":"c5cbca608f8fa84d98c467e1095a985e","successfullyInjectedSmells":[],"originalDockerfileUglified":"FROM fluxproject/ros_base\nRUN apt-get update \\\n && apt-get install --no-install-recommends locales bzip2 tree unzip xz-utils curl wget iproute2 sudo python-pip python3-pip python-setuptools python3-setuptools openjdk-8-jdk-headless nodejs npm nodejs-legacy iputils-ping net-tools iproute knot-dnsutils vim ffmpeg -y \\\n && rm -rf /var/lib/apt/lists/*\n#   Jupyterhub setting\nRUN mkdir -p /etc/jupyterhub\nCOPY jupyterhub_config.py /etc/jupyterhub/\n#   Introduce flux user   # TODO: link\nRUN npm install configurable-http-proxy -g\nRUN useradd -u 11111 -m -s /bin/bash flux\nRUN usermod -aG sudo flux\nRUN bash -c \" echo flux:flux | chpasswd \"\n#   Flux user setting    # TODO: link\nCOPY spark-ex-kubernetes.sh /home/flux/\nRUN python2 -m pip install --upgrade --user pip \\\n && python3 -m pip install --upgrade --user pip \\\n && python3 -m pip install --no-cache-dir --upgrade jupyter jupyterhub jupyterlab \\\n && python2 -m pip install --no-cache-dir --upgrade pyspark matplotlib pandas tensorflow keras Pillow \\\n && python2 -m pip install --no-cache-dir --upgrade --force-reinstall requests imageio moviepy seaborn gmaps \\\n && python2 -m pip install ipykernel \\\n && python2 -m ipykernel install \\\n && python3 -m pip install ipykernel \\\n && python3 -m ipykernel install\nRUN apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 66F84AE1EB71A8AC108087DCAF677210FF6D3CDA \\\n && bash -c 'echo \"deb [ arch=amd64 ] http://packages.dataspeedinc.com/ros/ubuntu $(lsb_release -sc) main\" > /etc/apt/sources.list.d/ros-dataspeed-public.list' \\\n && apt-get update\nRUN bash -c 'echo \"yaml http://packages.dataspeedinc.com/ros/ros-public-'$ROS_DISTRO'.yaml '$ROS_DISTRO'\" > /etc/ros/rosdep/sources.list.d/30-dataspeed-public-'$ROS_DISTRO'.list' \\\n && rosdep update 2> /dev/null \\\n && apt-get install --no-install-recommends ros-$ROS_DISTRO-dbw-mkz ros-$ROS_DISTRO-mobility-base ros-$ROS_DISTRO-baxter-sdk ros-$ROS_DISTRO-velodyne -y \\\n && rm -rf /var/lib/apt/lists/*\n#   Default to UTF-8\nRUN locale-gen en_US.UTF-8\nENV LANG=\"en_US.UTF-8\"\nENV JAVA_HOME=\"/usr/lib/jvm/java-8-openjdk-amd64\"\nENV PATH=\"$PATH:/opt/apache/hadoop/bin\"\nENV ROSIF_JAR=\"/opt/ros_hadoop/master/lib/rosbaginputformat.jar\"\nRUN mkdir -p /opt/ros_hadoop/master/dist/\nRUN mkdir -p /opt/apache/\nRUN mkdir -p /opt/ros_spark/dist/\nCOPY . /opt/ros_hadoop/master/\n#   TODO: ENV ROS_HADOOP='0.9.11'\n#   RUN \\\n#     curl -s \"https://codeload.github.com/valtech/ros_hadoop/tar.gz/v${ROS_HADOOP}\" | \\\n#     tar -C /opt/ros_hadoop -xvzf - && \\\n#     mv /opt/ros_hadoop/ros_hadoop-${ROS_HADOOP} /opt/ros_hadoop/latest\nRUN curl -s \"https://codeload.github.com/valtech/ros_hadoop/tar.gz/master\" | tar -C /opt/ros_hadoop -xvzf - \\\n && mv /opt/ros_hadoop/ros_hadoop-master /opt/ros_hadoop/latest\nRUN bash -c \"if [ ! -f /opt/ros_hadoop/master/dist/hadoop-3.0.0.tar.gz ] ; then wget --no-check-certificate -O /opt/ros_hadoop/master/dist/hadoop-3.1.1.tar.gz -q https://www.eu.apache.org/dist/hadoop/common/hadoop-3.1.1/hadoop-3.1.1.tar.gz ; fi\"\nRUN tar -xzf /opt/ros_hadoop/master/dist/hadoop-3.1.1.tar.gz -C /opt/apache \\\n && rm /opt/ros_hadoop/master/dist/hadoop-3.1.1.tar.gz\nRUN ln -s /opt/apache/hadoop-3.1.1 /opt/apache/hadoop\nRUN bash -c \"if [ ! -f /opt/ros_hadoop/latest/lib/rosbaginputformat.jar ] ; then ln -s /opt/ros_hadoop/master/lib/rosbaginputformat.jar /opt/ros_hadoop/latest/lib/rosbaginputformat.jar ; fi\"\n#  # for spark example tests\nRUN bash -c \"if [ ! -f /opt/ros_spark/dist/spark-2.3.1-bin-hadoop2.7.tgz ] ; then wget --quiet -O /opt/ros_spark/dist/spark-2.3.1-bin-hadoop2.7.tgz http://apache.lauf-forum.at/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz ; fi\"\nRUN tar -xzf /opt/ros_spark/dist/spark-2.3.1-bin-hadoop2.7.tgz -C /opt/apache \\\n && rm /opt/ros_spark/dist/spark-2.3.1-bin-hadoop2.7.tgz\nRUN printf \"<configuration>\\n\\n<property>\\n<name>fs.defaultFS</name>\\n<value>hdfs://localhost:9000</value>\\n</property>\\n</configuration>\" > /opt/apache/hadoop/etc/hadoop/core-site.xml \\\n && printf \"<configuration>\\n<property>\\n<name>dfs.replication</name>\\n<value>1</value>\\n</property>\\n</configuration>\" > /opt/apache/hadoop/etc/hadoop/hdfs-site.xml \\\n && bash -c \"/opt/apache/hadoop/bin/hdfs namenode -format 2>/dev/null\" \\\n && printf \"#! /bin/bash\\n/opt/apache/hadoop/bin/hdfs --daemon stop datanode\\n/opt/apache/hadoop/bin/hdfs --daemon stop namenode\\n/opt/apache/hadoop/bin/hdfs --daemon start namenode\\n/opt/apache/hadoop/bin/hdfs --daemon start datanode\\nexec \\\"$@\\\"\\n\" > /start_hadoop.sh \\\n && chmod a+x /start_hadoop.sh\nRUN printf \"#! /bin/bash\\nset -e\\nsource \\\"/opt/ros/$ROS_DISTRO/setup.bash\\\"\\n/start_hadoop.sh\\nexec \\\"$@\\\"\\n\" > /ros_hadoop.sh \\\n && chmod a+x /ros_hadoop.sh\nRUN bash -c \"if [ ! -f /opt/ros_hadoop/master/dist/HMB_4.bag ] ; then wget --quiet -O /opt/ros_hadoop/master/dist/HMB_4.bag https://xfiles.valtech.io/f/c494d168522045e3bcc0/?dl=1 ; fi\" \\\n && java -jar \"$ROSIF_JAR\" -f /opt/ros_hadoop/master/dist/HMB_4.bag\nRUN bash -c \"/start_hadoop.sh\" \\\n && until /opt/apache/hadoop/bin/hdfs dfsadmin -safemode wait ; do sleep 1s ; done \\\n && until /opt/apache/hadoop/bin/hdfs dfsadmin -report ; do sleep 1s ; done \\\n && until /opt/apache/hadoop/bin/hdfs dfs -mkdir /user ; do sleep 1s ; done \\\n && /opt/apache/hadoop/bin/hdfs dfs -mkdir /user/root \\\n && /opt/apache/hadoop/bin/hdfs dfs -mkdir /user/flux \\\n && /opt/apache/hadoop/bin/hdfs dfs -put /opt/ros_hadoop/master/dist/HMB_4.bag \\\n && /opt/apache/hadoop/bin/hdfs --daemon stop datanode \\\n && /opt/apache/hadoop/bin/hdfs --daemon stop namenode\nRUN mkdir -p /ope/ros_hadoop/latest/doc \\\n && chmod -R 777 /opt/ros_hadoop\nWORKDIR /opt/ros_hadoop/latest/doc/\nENTRYPOINT [\"/ros_hadoop.sh\"]\nCMD [\"jupyterhub\", \"-f\", \"/etc/jupyterhub/jupyterhub_config.py\"]\n","originalDockerfileUglifiedHash":"43fbfe627a5b037d927599ad20129499","fileName":"/ICSME-replicationpackage/dataset/smelly_dockerfiles_bianncle/236ddec428632536276152ab357f8f6d4c128be7.dockerfile"}