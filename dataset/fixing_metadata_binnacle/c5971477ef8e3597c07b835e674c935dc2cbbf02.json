{"seed":3816831389,"processedDockerfileHash":"0ad9af8b86e7c638122cb91f4a5ffd43","fixedSmells":["use-no-install-recommends","do-not-use-apt-get-update-alone","pin-package-manager-versions-apt-get","pin-package-manager-versions-pip","pin-package-manager-versions-npm","pin-package-manager-versions-gem","pin-package-manager-versions-apk","use-copy-instead-of-add","use-wget-instead-of-add","do-not-have-secrets","have-a-healthcheck","have-a-user"],"successfullyFixedSmells":["pin-package-manager-versions-pip","pin-package-manager-versions-npm","have-a-healthcheck","have-a-user"],"processedDockerfile":"#   First things first, we build an image which is where we're going to compile\n#   our static assets with. It is important that the steps in this remain the\n#   same as the steps in Dockerfile.static, EXCEPT this may include additional\n#   steps appended onto the end.\nFROM node:8.15.1 AS static\nWORKDIR /opt/warehouse/src/\n#   The list of C packages we need are almost never going to change, so installing\n#   them first, right off the bat lets us cache that and having node.js level\n#   dependency changes not trigger a reinstall.\nRUN set -x \\\n && apt-get update \\\n && apt-get install --no-install-recommends libjpeg-dev nasm -y\n#   However, we do want to trigger a reinstall of our node.js dependencies anytime\n#   our package.json changes, so we'll ensure that we're copying that into our\n#   static container prior to actually installing the npm dependencies.\nCOPY package.json package-lock.json .babelrc /opt/warehouse/src/\n#   Installing npm dependencies is done as a distinct step and *prior* to copying\n#   over our static files so that, you guessed it, we don't invalidate the cache\n#   of installed dependencies just because files have been modified.\nRUN set -x \\\n && npm install npm@latest -g \\\n && npm install gulp-cli@2.3.0 -g \\\n && npm install\n#   Actually copy over our static files, we only copy over the static files to\n#   save a small amount of space in our image and because we don't need them. We\n#   copy Gulpfile.babel.js last even though it's least likely to change, because\n#   it's very small so copying it needlessly isn't a big deal but it will save a\n#   small amount of copying when only Gulpfile.babel.js is modified.\nCOPY warehouse/static/ /opt/warehouse/src/warehouse/static/\nCOPY warehouse/admin/static/ /opt/warehouse/src/warehouse/admin/static/\nCOPY Gulpfile.babel.js /opt/warehouse/src/\nRUN gulp dist\n#   Now we're going to build our actual application, but not the actual production\n#   image that it gets deployed into.\nFROM python:3.7.3-slim-stretch AS build\n#   Define whether we're building a production or a development image. This will\n#   generally be used to control whether or not we install our development and\n#   test dependencies.\nARG DEVEL=no\n#   To enable Ipython in the development environment set to yes (for using ipython\n#   as the warehouse shell interpreter,\n#   i.e. 'docker-compose run --rm web python -m warehouse shell --type=ipython')\nARG IPYTHON=no\n#   Install System level Warehouse build requirements, this is done before\n#   everything else because these are rarely ever going to change.\nRUN set -x \\\n && apt-get update \\\n && apt-get install --no-install-recommends build-essential libffi-dev libxml2-dev libxslt-dev libpq-dev libcurl4-openssl-dev libssl-dev $( if [ \"$DEVEL\" = \"yes\" ] ; then echo 'libjpeg-dev' ; fi ;) -y\n#   We need a way for the build system to pass in a repository that will be used\n#   to install our theme from. For this we'll add the THEME_REPO build argument\n#   which takes a PEP 503 compatible repository URL that must be available to\n#   install the requirements/theme.txt requirement file.\nARG THEME_REPO\n#   We create an /opt directory with a virtual environment in it to store our\n#   application in.\nRUN set -x \\\n && python3 -m venv /opt/warehouse\n#   Now that we've created our virtual environment, we'll go ahead and update\n#   our $PATH to refer to it first.\nENV PATH=\"/opt/warehouse/bin:${PATH}\"\n#   Next, we want to update pip, setuptools, and wheel inside of this virtual\n#   environment to ensure that we have the latest versions of them.\n#   TODO: We use --require-hashes in our requirements files, but not here, making\n#         the ones in the requirements files kind of a moot point. We should\n#         probably pin these too, and update them as we do anything else.\nRUN pip install pip==23.1 setuptools==67.6.1 wheel==0.40.0 --no-cache-dir --disable-pip-version-check --upgrade\n#   We copy this into the docker container prior to copying in the rest of our\n#   application so that we can skip installing requirements if the only thing\n#   that has changed is the Warehouse code itself.\nCOPY requirements /tmp/requirements\n#   Install our development dependencies if we're building a development install\n#   otherwise this will do nothing.\nRUN set -x \\\n && if [ \"$DEVEL\" = \"yes\" ] ; then pip install --no-cache-dir --disable-pip-version-check -r /tmp/requirements/dev.txt ; fi\nRUN set -x \\\n && if [ \"$DEVEL\" = \"yes\" ] \\\n && [ \"$IPYTHON\" = \"yes\" ] ; then pip install --no-cache-dir --disable-pip-version-check -r /tmp/requirements/ipython.txt ; fi\n#   Install the Python level Warehouse requirements, this is done after copying\n#   the requirements but prior to copying Warehouse itself into the container so\n#   that code changes don't require triggering an entire install of all of\n#   Warehouse's dependencies.\nRUN set -x \\\n && PIP_EXTRA_INDEX_URL=$THEME_REPO pip --no-cache-dir --disable-pip-version-check install --no-binary hiredis -r /tmp/requirements/deploy.txt -r /tmp/requirements/main.txt $( if [ \"$DEVEL\" = \"yes\" ] ; then echo '-r /tmp/requirements/tests.txt' ; fi ;) $( if [ \"$THEME_REPO\" != \"\" ] ; then echo '-r /tmp/requirements/theme.txt' ; fi ;) \\\n && find /opt/warehouse -name '*.pyc' -delete\n#   Now we're going to build our actual application image, which will eventually\n#   pull in the static files that were built above.\nFROM python:3.7.3-slim-stretch\n#   Setup some basic environment variables that are ~never going to change.\nENV PYTHONUNBUFFERED=\"1\"\nENV PYTHONPATH=\"/opt/warehouse/src/\"\nENV PATH=\"/opt/warehouse/bin:${PATH}\"\nWORKDIR /opt/warehouse/src/\n#   Define whether we're building a production or a development image. This will\n#   generally be used to control whether or not we install our development and\n#   test dependencies.\nARG DEVEL=no\n#   This is a work around because otherwise postgresql-client bombs out trying\n#   to create symlinks to these directories.\nRUN set -x \\\n && mkdir -p /usr/share/man/man1 \\\n && mkdir -p /usr/share/man/man7\n#   Install System level Warehouse requirements, this is done before everything\n#   else because these are rarely ever going to change.\nRUN set -x \\\n && apt-get update \\\n && apt-get install --no-install-recommends libpq5 libxml2 libxslt1.1 libcurl3 $( if [ \"$DEVEL\" = \"yes\" ] ; then echo 'bash libjpeg62 postgresql-client' ; fi ;) -y \\\n && apt-get clean \\\n && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*\n#   Copy the directory into the container, this is done last so that changes to\n#   Warehouse itself require the least amount of layers being invalidated from\n#   the cache. This is most important in development, but it also useful for\n#   deploying new code changes.\nCOPY --from=static /opt/warehouse/src/warehouse/static/dist/ /opt/warehouse/src/warehouse/static/dist/\nCOPY --from=static /opt/warehouse/src/warehouse/admin/static/dist/ /opt/warehouse/src/warehouse/admin/static/dist/\nCOPY --from=build /opt/warehouse/ /opt/warehouse/\nCOPY . /opt/warehouse/src/\nRUN groupadd --system docker-user ; useradd --system --gid docker-user docker-user\nUSER docker-user\nHEALTHCHECK CMD curl --fail http://127.0.0.1:3000 || exit 1\n","originalDockerfile":"#  First things first, we build an image which is where we're going to compile\n#  our static assets with. It is important that the steps in this remain the\n#  same as the steps in Dockerfile.static, EXCEPT this may include additional\n#  steps appended onto the end.\nFROM node:8.15.1 AS static\nWORKDIR /opt/warehouse/src/\n#  The list of C packages we need are almost never going to change, so installing\n#  them first, right off the bat lets us cache that and having node.js level\n#  dependency changes not trigger a reinstall.\nRUN set -x \\\n && apt-get update \\\n && apt-get install --no-install-recommends libjpeg-dev nasm -y\n#  However, we do want to trigger a reinstall of our node.js dependencies anytime\n#  our package.json changes, so we'll ensure that we're copying that into our\n#  static container prior to actually installing the npm dependencies.\nCOPY package.json package-lock.json .babelrc /opt/warehouse/src/\n#  Installing npm dependencies is done as a distinct step and *prior* to copying\n#  over our static files so that, you guessed it, we don't invalidate the cache\n#  of installed dependencies just because files have been modified.\nRUN set -x \\\n && npm install npm@latest -g \\\n && npm install gulp-cli -g \\\n && npm install\n#  Actually copy over our static files, we only copy over the static files to\n#  save a small amount of space in our image and because we don't need them. We\n#  copy Gulpfile.babel.js last even though it's least likely to change, because\n#  it's very small so copying it needlessly isn't a big deal but it will save a\n#  small amount of copying when only Gulpfile.babel.js is modified.\nCOPY warehouse/static/ /opt/warehouse/src/warehouse/static/\nCOPY warehouse/admin/static/ /opt/warehouse/src/warehouse/admin/static/\nCOPY Gulpfile.babel.js /opt/warehouse/src/\nRUN gulp dist\n#  Now we're going to build our actual application, but not the actual production\n#  image that it gets deployed into.\nFROM python:3.7.3-slim-stretch AS build\n#  Define whether we're building a production or a development image. This will\n#  generally be used to control whether or not we install our development and\n#  test dependencies.\nARG DEVEL=no\n#  To enable Ipython in the development environment set to yes (for using ipython\n#  as the warehouse shell interpreter,\n#  i.e. 'docker-compose run --rm web python -m warehouse shell --type=ipython')\nARG IPYTHON=no\n#  Install System level Warehouse build requirements, this is done before\n#  everything else because these are rarely ever going to change.\nRUN set -x \\\n && apt-get update \\\n && apt-get install --no-install-recommends build-essential libffi-dev libxml2-dev libxslt-dev libpq-dev libcurl4-openssl-dev libssl-dev $( if [ \"$DEVEL\" = \"yes\" ] ; then echo 'libjpeg-dev' ; fi ;) -y\n#  We need a way for the build system to pass in a repository that will be used\n#  to install our theme from. For this we'll add the THEME_REPO build argument\n#  which takes a PEP 503 compatible repository URL that must be available to\n#  install the requirements/theme.txt requirement file.\nARG THEME_REPO\n#  We create an /opt directory with a virtual environment in it to store our\n#  application in.\nRUN set -x \\\n && python3 -m venv /opt/warehouse\n#  Now that we've created our virtual environment, we'll go ahead and update\n#  our $PATH to refer to it first.\nENV PATH=\"/opt/warehouse/bin:${PATH}\"\n#  Next, we want to update pip, setuptools, and wheel inside of this virtual\n#  environment to ensure that we have the latest versions of them.\n#  TODO: We use --require-hashes in our requirements files, but not here, making\n#        the ones in the requirements files kind of a moot point. We should\n#        probably pin these too, and update them as we do anything else.\nRUN pip install pip setuptools wheel --no-cache-dir --disable-pip-version-check --upgrade\n#  We copy this into the docker container prior to copying in the rest of our\n#  application so that we can skip installing requirements if the only thing\n#  that has changed is the Warehouse code itself.\nCOPY requirements /tmp/requirements\n#  Install our development dependencies if we're building a development install\n#  otherwise this will do nothing.\nRUN set -x \\\n && if [ \"$DEVEL\" = \"yes\" ] ; then pip install --no-cache-dir --disable-pip-version-check -r /tmp/requirements/dev.txt ; fi\nRUN set -x \\\n && if [ \"$DEVEL\" = \"yes\" ] \\\n && [ \"$IPYTHON\" = \"yes\" ] ; then pip install --no-cache-dir --disable-pip-version-check -r /tmp/requirements/ipython.txt ; fi\n#  Install the Python level Warehouse requirements, this is done after copying\n#  the requirements but prior to copying Warehouse itself into the container so\n#  that code changes don't require triggering an entire install of all of\n#  Warehouse's dependencies.\nRUN set -x \\\n && PIP_EXTRA_INDEX_URL=$THEME_REPO pip --no-cache-dir --disable-pip-version-check install --no-binary hiredis -r /tmp/requirements/deploy.txt -r /tmp/requirements/main.txt $( if [ \"$DEVEL\" = \"yes\" ] ; then echo '-r /tmp/requirements/tests.txt' ; fi ;) $( if [ \"$THEME_REPO\" != \"\" ] ; then echo '-r /tmp/requirements/theme.txt' ; fi ;) \\\n && find /opt/warehouse -name '*.pyc' -delete\n#  Now we're going to build our actual application image, which will eventually\n#  pull in the static files that were built above.\nFROM python:3.7.3-slim-stretch\n#  Setup some basic environment variables that are ~never going to change.\nENV PYTHONUNBUFFERED=\"1\"\nENV PYTHONPATH=\"/opt/warehouse/src/\"\nENV PATH=\"/opt/warehouse/bin:${PATH}\"\nWORKDIR /opt/warehouse/src/\n#  Define whether we're building a production or a development image. This will\n#  generally be used to control whether or not we install our development and\n#  test dependencies.\nARG DEVEL=no\n#  This is a work around because otherwise postgresql-client bombs out trying\n#  to create symlinks to these directories.\nRUN set -x \\\n && mkdir -p /usr/share/man/man1 \\\n && mkdir -p /usr/share/man/man7\n#  Install System level Warehouse requirements, this is done before everything\n#  else because these are rarely ever going to change.\nRUN set -x \\\n && apt-get update \\\n && apt-get install --no-install-recommends libpq5 libxml2 libxslt1.1 libcurl3 $( if [ \"$DEVEL\" = \"yes\" ] ; then echo 'bash libjpeg62 postgresql-client' ; fi ;) -y \\\n && apt-get clean \\\n && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*\n#  Copy the directory into the container, this is done last so that changes to\n#  Warehouse itself require the least amount of layers being invalidated from\n#  the cache. This is most important in development, but it also useful for\n#  deploying new code changes.\nCOPY --from=static /opt/warehouse/src/warehouse/static/dist/ /opt/warehouse/src/warehouse/static/dist/\nCOPY --from=static /opt/warehouse/src/warehouse/admin/static/dist/ /opt/warehouse/src/warehouse/admin/static/dist/\nCOPY --from=build /opt/warehouse/ /opt/warehouse/\nCOPY . /opt/warehouse/src/\n","injectedSmells":[],"originalDockerfileHash":"b2a47ec963451913a9eb0d6de380399d","successfullyInjectedSmells":[],"originalDockerfileUglified":"#   First things first, we build an image which is where we're going to compile\n#   our static assets with. It is important that the steps in this remain the\n#   same as the steps in Dockerfile.static, EXCEPT this may include additional\n#   steps appended onto the end.\nFROM node:8.15.1 AS static\nWORKDIR /opt/warehouse/src/\n#   The list of C packages we need are almost never going to change, so installing\n#   them first, right off the bat lets us cache that and having node.js level\n#   dependency changes not trigger a reinstall.\nRUN set -x \\\n && apt-get update \\\n && apt-get install --no-install-recommends libjpeg-dev nasm -y\n#   However, we do want to trigger a reinstall of our node.js dependencies anytime\n#   our package.json changes, so we'll ensure that we're copying that into our\n#   static container prior to actually installing the npm dependencies.\nCOPY package.json package-lock.json .babelrc /opt/warehouse/src/\n#   Installing npm dependencies is done as a distinct step and *prior* to copying\n#   over our static files so that, you guessed it, we don't invalidate the cache\n#   of installed dependencies just because files have been modified.\nRUN set -x \\\n && npm install npm@latest -g \\\n && npm install gulp-cli -g \\\n && npm install\n#   Actually copy over our static files, we only copy over the static files to\n#   save a small amount of space in our image and because we don't need them. We\n#   copy Gulpfile.babel.js last even though it's least likely to change, because\n#   it's very small so copying it needlessly isn't a big deal but it will save a\n#   small amount of copying when only Gulpfile.babel.js is modified.\nCOPY warehouse/static/ /opt/warehouse/src/warehouse/static/\nCOPY warehouse/admin/static/ /opt/warehouse/src/warehouse/admin/static/\nCOPY Gulpfile.babel.js /opt/warehouse/src/\nRUN gulp dist\n#   Now we're going to build our actual application, but not the actual production\n#   image that it gets deployed into.\nFROM python:3.7.3-slim-stretch AS build\n#   Define whether we're building a production or a development image. This will\n#   generally be used to control whether or not we install our development and\n#   test dependencies.\nARG DEVEL=no\n#   To enable Ipython in the development environment set to yes (for using ipython\n#   as the warehouse shell interpreter,\n#   i.e. 'docker-compose run --rm web python -m warehouse shell --type=ipython')\nARG IPYTHON=no\n#   Install System level Warehouse build requirements, this is done before\n#   everything else because these are rarely ever going to change.\nRUN set -x \\\n && apt-get update \\\n && apt-get install --no-install-recommends build-essential libffi-dev libxml2-dev libxslt-dev libpq-dev libcurl4-openssl-dev libssl-dev $( if [ \"$DEVEL\" = \"yes\" ] ; then echo 'libjpeg-dev' ; fi ;) -y\n#   We need a way for the build system to pass in a repository that will be used\n#   to install our theme from. For this we'll add the THEME_REPO build argument\n#   which takes a PEP 503 compatible repository URL that must be available to\n#   install the requirements/theme.txt requirement file.\nARG THEME_REPO\n#   We create an /opt directory with a virtual environment in it to store our\n#   application in.\nRUN set -x \\\n && python3 -m venv /opt/warehouse\n#   Now that we've created our virtual environment, we'll go ahead and update\n#   our $PATH to refer to it first.\nENV PATH=\"/opt/warehouse/bin:${PATH}\"\n#   Next, we want to update pip, setuptools, and wheel inside of this virtual\n#   environment to ensure that we have the latest versions of them.\n#   TODO: We use --require-hashes in our requirements files, but not here, making\n#         the ones in the requirements files kind of a moot point. We should\n#         probably pin these too, and update them as we do anything else.\nRUN pip install pip setuptools wheel --no-cache-dir --disable-pip-version-check --upgrade\n#   We copy this into the docker container prior to copying in the rest of our\n#   application so that we can skip installing requirements if the only thing\n#   that has changed is the Warehouse code itself.\nCOPY requirements /tmp/requirements\n#   Install our development dependencies if we're building a development install\n#   otherwise this will do nothing.\nRUN set -x \\\n && if [ \"$DEVEL\" = \"yes\" ] ; then pip install --no-cache-dir --disable-pip-version-check -r /tmp/requirements/dev.txt ; fi\nRUN set -x \\\n && if [ \"$DEVEL\" = \"yes\" ] \\\n && [ \"$IPYTHON\" = \"yes\" ] ; then pip install --no-cache-dir --disable-pip-version-check -r /tmp/requirements/ipython.txt ; fi\n#   Install the Python level Warehouse requirements, this is done after copying\n#   the requirements but prior to copying Warehouse itself into the container so\n#   that code changes don't require triggering an entire install of all of\n#   Warehouse's dependencies.\nRUN set -x \\\n && PIP_EXTRA_INDEX_URL=$THEME_REPO pip --no-cache-dir --disable-pip-version-check install --no-binary hiredis -r /tmp/requirements/deploy.txt -r /tmp/requirements/main.txt $( if [ \"$DEVEL\" = \"yes\" ] ; then echo '-r /tmp/requirements/tests.txt' ; fi ;) $( if [ \"$THEME_REPO\" != \"\" ] ; then echo '-r /tmp/requirements/theme.txt' ; fi ;) \\\n && find /opt/warehouse -name '*.pyc' -delete\n#   Now we're going to build our actual application image, which will eventually\n#   pull in the static files that were built above.\nFROM python:3.7.3-slim-stretch\n#   Setup some basic environment variables that are ~never going to change.\nENV PYTHONUNBUFFERED=\"1\"\nENV PYTHONPATH=\"/opt/warehouse/src/\"\nENV PATH=\"/opt/warehouse/bin:${PATH}\"\nWORKDIR /opt/warehouse/src/\n#   Define whether we're building a production or a development image. This will\n#   generally be used to control whether or not we install our development and\n#   test dependencies.\nARG DEVEL=no\n#   This is a work around because otherwise postgresql-client bombs out trying\n#   to create symlinks to these directories.\nRUN set -x \\\n && mkdir -p /usr/share/man/man1 \\\n && mkdir -p /usr/share/man/man7\n#   Install System level Warehouse requirements, this is done before everything\n#   else because these are rarely ever going to change.\nRUN set -x \\\n && apt-get update \\\n && apt-get install --no-install-recommends libpq5 libxml2 libxslt1.1 libcurl3 $( if [ \"$DEVEL\" = \"yes\" ] ; then echo 'bash libjpeg62 postgresql-client' ; fi ;) -y \\\n && apt-get clean \\\n && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*\n#   Copy the directory into the container, this is done last so that changes to\n#   Warehouse itself require the least amount of layers being invalidated from\n#   the cache. This is most important in development, but it also useful for\n#   deploying new code changes.\nCOPY --from=static /opt/warehouse/src/warehouse/static/dist/ /opt/warehouse/src/warehouse/static/dist/\nCOPY --from=static /opt/warehouse/src/warehouse/admin/static/dist/ /opt/warehouse/src/warehouse/admin/static/dist/\nCOPY --from=build /opt/warehouse/ /opt/warehouse/\nCOPY . /opt/warehouse/src/\n","originalDockerfileUglifiedHash":"6c1bc33a2ab125249dac23544ffbae59","fileName":"/ICSME-replicationpackage/dataset/smelly_dockerfiles_bianncle/c5971477ef8e3597c07b835e674c935dc2cbbf02.dockerfile"}