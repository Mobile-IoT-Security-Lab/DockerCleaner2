{"seed":2032502012,"processedDockerfileHash":"85ff7129f298e05ecd5d62cbf9b6591f","fixedSmells":["use-no-install-recommends","do-not-use-apt-get-update-alone","pin-package-manager-versions-apt-get","pin-package-manager-versions-pip","pin-package-manager-versions-npm","pin-package-manager-versions-gem","pin-package-manager-versions-apk","use-copy-instead-of-add","use-wget-instead-of-add","do-not-have-secrets","have-a-healthcheck","have-a-user"],"successfullyFixedSmells":["pin-package-manager-versions-apt-get","have-a-healthcheck","have-a-user"],"processedDockerfile":"#   Copyright 2018 Google LLC\n#\n#   Licensed under the Apache License, Version 2.0 (the \"License\");\n#   you may not use this file except in compliance with the License.\n#   You may obtain a copy of the License at\n#\n#       https://www.apache.org/licenses/LICENSE-2.0\n#\n#   Unless required by applicable law or agreed to in writing, software\n#   distributed under the License is distributed on an \"AS IS\" BASIS,\n#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#   See the License for the specific language governing permissions and\n#   limitations under the License.\nARG TF_SERVING_VERSION=latest\nARG TF_SERVING_BUILD_IMAGE=tensorflow/serving:${TF_SERVING_VERSION}-devel-mkl\nFROM ${TF_SERVING_BUILD_IMAGE} AS build_image\nFROM ubuntu:18.04\nARG TF_SERVING_VERSION_GIT_BRANCH=master\nARG TF_SERVING_VERSION_GIT_COMMIT=head\nLABEL maintainer=\"Karthik Vadla <karthik.vadla@intel.com>\"\nLABEL tensorflow_serving_github_branchtag=\"${TF_SERVING_VERSION_GIT_BRANCH}\"\nLABEL tensorflow_serving_github_commit=\"${TF_SERVING_VERSION_GIT_COMMIT}\"\nRUN apt-get update \\\n && apt-get install --no-install-recommends ca-certificates=20230311 -y \\\n && apt-get clean \\\n && rm -rf /var/lib/apt/lists/*\n#   Install TF Serving pkg\nCOPY --from=build_image /usr/local/bin/tensorflow_model_server /usr/bin/tensorflow_model_server\n#   Install MKL libraries\nCOPY --from=build_image /usr/local/lib/libiomp5.so /usr/local/lib\nCOPY --from=build_image /usr/local/lib/libmklml_gnu.so /usr/local/lib\nCOPY --from=build_image /usr/local/lib/libmklml_intel.so /usr/local/lib\nENV LIBRARY_PATH=\"'/usr/local/lib:$LIBRARY_PATH'\"\nENV LD_LIBRARY_PATH=\"'/usr/local/lib:$LD_LIBRARY_PATH'\"\n#   Expose ports\n#   gRPC\nEXPOSE 8500/tcp\n#   REST\nEXPOSE 8501/tcp\n#   Set where models should be stored in the container\nENV MODEL_BASE_PATH=\"/models\"\nRUN mkdir -p ${MODEL_BASE_PATH}\n#   The only required piece is the model name in order to differentiate endpoints\nENV MODEL_NAME=\"model\"\n#   Create a script that runs the model server so we can use environment variables\n#   while also passing in arguments from the docker command line\n#   Setting MKL environment variables can improve performance.\n#   https://www.tensorflow.org/guide/performance/overview\n#   Read about Tuning MKL for the best performance\n#   Add export MKLDNN_VERBOSE=1 to the below script,\n#   to see MKL messages in the docker logs when you send predict request.\n#   Based on our observations during experiments,\n#   setting tensorflow_session_parallelism=<1/4th of physical cores> and\n#   setting OMP_NUM_THREADS=<Total physical cores>\n#   gave optimal performance results with MKL\n#   KMP_BLOCKTIME=<Varies based on your model>\n#   NOTE: We don't guarantee same settings to give optimal peformance across all hardware\n#   please tune variables as required.\nENV OMP_NUM_THREADS=\"2\"\nENV KMP_BLOCKTIME=\"1\"\nENV KMP_SETTINGS=\"1\"\nENV KMP_AFFINITY=\"granularity=fine,verbose,compact,1,0\"\nENV MKLDNN_VERBOSE=\"0\"\n#   Recommended settings, may vary depending on the model.\n#   Set TENSORFLOW_INTRA_OP_PARALLELISM=#No.of Physical cores\n#   Set TENSORFLOW_INTER_OP_PARALLELISM=#No.of Sockets\n#   For more information vist below reference\n#   https://www.tensorflow.org/guide/performance/overview#tuning_mkl_for_the_best_performance\n#   NOTE: As TENSORFLOW_INTRA_OP_PARALLELISM and TENSORFLOW_INTER_OP_PARALLELISM are\n#   configured via SessionOptions in tensorflow, these values\n#   will override the values configured via TF_NUM_INTEROP_THREADS and TF_NUM_INTRAOP_THREADS\n#   environment variables in tensorflow.\n#   https://github.com/tensorflow/tensorflow/commit/d1823e2e966e96ee4ea7baa202ad9f292ac7427b\n#   Defaults\nENV TENSORFLOW_INTRA_OP_PARALLELISM=\"2\"\nENV TENSORFLOW_INTER_OP_PARALLELISM=\"2\"\nRUN echo '#!/bin/bash \\n\\ntensorflow_model_server --port=8500 --rest_api_port=8501 --tensorflow_intra_op_parallelism=${TENSORFLOW_INTRA_OP_PARALLELISM} --tensorflow_inter_op_parallelism=${TENSORFLOW_INTER_OP_PARALLELISM} --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \"$@\"' > /usr/bin/tf_serving_entrypoint.sh \\\n && chmod +x /usr/bin/tf_serving_entrypoint.sh\nENTRYPOINT [\"/usr/bin/tf_serving_entrypoint.sh\"]\nRUN groupadd --system docker-user ; useradd --system --gid docker-user docker-user\nUSER docker-user\n# Please add your HEALTHCHECK here!!!\n","originalDockerfile":"#  Copyright 2018 Google LLC\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      https://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\nARG TF_SERVING_VERSION=latest\nARG TF_SERVING_BUILD_IMAGE=tensorflow/serving:${TF_SERVING_VERSION}-devel-mkl\nFROM ${TF_SERVING_BUILD_IMAGE} AS build_image\nFROM ubuntu:18.04\nARG TF_SERVING_VERSION_GIT_BRANCH=master\nARG TF_SERVING_VERSION_GIT_COMMIT=head\nLABEL maintainer=\"Karthik Vadla <karthik.vadla@intel.com>\"\nLABEL tensorflow_serving_github_branchtag=\"${TF_SERVING_VERSION_GIT_BRANCH}\"\nLABEL tensorflow_serving_github_commit=\"${TF_SERVING_VERSION_GIT_COMMIT}\"\nRUN apt-get update \\\n && apt-get install --no-install-recommends ca-certificates -y \\\n && apt-get clean \\\n && rm -rf /var/lib/apt/lists/*\n#  Install TF Serving pkg\nCOPY --from=build_image /usr/local/bin/tensorflow_model_server /usr/bin/tensorflow_model_server\n#  Install MKL libraries\nCOPY --from=build_image /usr/local/lib/libiomp5.so /usr/local/lib\nCOPY --from=build_image /usr/local/lib/libmklml_gnu.so /usr/local/lib\nCOPY --from=build_image /usr/local/lib/libmklml_intel.so /usr/local/lib\nENV LIBRARY_PATH=\"'/usr/local/lib:$LIBRARY_PATH'\"\nENV LD_LIBRARY_PATH=\"'/usr/local/lib:$LD_LIBRARY_PATH'\"\n#  Expose ports\n#  gRPC\nEXPOSE 8500/tcp\n#  REST\nEXPOSE 8501/tcp\n#  Set where models should be stored in the container\nENV MODEL_BASE_PATH=\"/models\"\nRUN mkdir -p ${MODEL_BASE_PATH}\n#  The only required piece is the model name in order to differentiate endpoints\nENV MODEL_NAME=\"model\"\n#  Create a script that runs the model server so we can use environment variables\n#  while also passing in arguments from the docker command line\n#  Setting MKL environment variables can improve performance.\n#  https://www.tensorflow.org/guide/performance/overview\n#  Read about Tuning MKL for the best performance\n#  Add export MKLDNN_VERBOSE=1 to the below script,\n#  to see MKL messages in the docker logs when you send predict request.\n#  Based on our observations during experiments,\n#  setting tensorflow_session_parallelism=<1/4th of physical cores> and\n#  setting OMP_NUM_THREADS=<Total physical cores>\n#  gave optimal performance results with MKL\n#  KMP_BLOCKTIME=<Varies based on your model>\n#  NOTE: We don't guarantee same settings to give optimal peformance across all hardware\n#  please tune variables as required.\nENV OMP_NUM_THREADS=\"2\"\nENV KMP_BLOCKTIME=\"1\"\nENV KMP_SETTINGS=\"1\"\nENV KMP_AFFINITY=\"granularity=fine,verbose,compact,1,0\"\nENV MKLDNN_VERBOSE=\"0\"\n#  Recommended settings, may vary depending on the model.\n#  Set TENSORFLOW_INTRA_OP_PARALLELISM=#No.of Physical cores\n#  Set TENSORFLOW_INTER_OP_PARALLELISM=#No.of Sockets\n#  For more information vist below reference\n#  https://www.tensorflow.org/guide/performance/overview#tuning_mkl_for_the_best_performance\n#  NOTE: As TENSORFLOW_INTRA_OP_PARALLELISM and TENSORFLOW_INTER_OP_PARALLELISM are\n#  configured via SessionOptions in tensorflow, these values\n#  will override the values configured via TF_NUM_INTEROP_THREADS and TF_NUM_INTRAOP_THREADS\n#  environment variables in tensorflow.\n#  https://github.com/tensorflow/tensorflow/commit/d1823e2e966e96ee4ea7baa202ad9f292ac7427b\n#  Defaults\nENV TENSORFLOW_INTRA_OP_PARALLELISM=\"2\"\nENV TENSORFLOW_INTER_OP_PARALLELISM=\"2\"\nRUN echo '#!/bin/bash \\n\\ntensorflow_model_server --port=8500 --rest_api_port=8501 --tensorflow_intra_op_parallelism=${TENSORFLOW_INTRA_OP_PARALLELISM} --tensorflow_inter_op_parallelism=${TENSORFLOW_INTER_OP_PARALLELISM} --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \"$@\"' > /usr/bin/tf_serving_entrypoint.sh \\\n && chmod +x /usr/bin/tf_serving_entrypoint.sh\nENTRYPOINT [\"/usr/bin/tf_serving_entrypoint.sh\"]\n","injectedSmells":[],"originalDockerfileHash":"d3747db5aabc8508e601af1e792584fc","successfullyInjectedSmells":[],"originalDockerfileUglified":"#   Copyright 2018 Google LLC\n#\n#   Licensed under the Apache License, Version 2.0 (the \"License\");\n#   you may not use this file except in compliance with the License.\n#   You may obtain a copy of the License at\n#\n#       https://www.apache.org/licenses/LICENSE-2.0\n#\n#   Unless required by applicable law or agreed to in writing, software\n#   distributed under the License is distributed on an \"AS IS\" BASIS,\n#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#   See the License for the specific language governing permissions and\n#   limitations under the License.\nARG TF_SERVING_VERSION=latest\nARG TF_SERVING_BUILD_IMAGE=tensorflow/serving:${TF_SERVING_VERSION}-devel-mkl\nFROM ${TF_SERVING_BUILD_IMAGE} AS build_image\nFROM ubuntu:18.04\nARG TF_SERVING_VERSION_GIT_BRANCH=master\nARG TF_SERVING_VERSION_GIT_COMMIT=head\nLABEL maintainer=\"Karthik Vadla <karthik.vadla@intel.com>\"\nLABEL tensorflow_serving_github_branchtag=\"${TF_SERVING_VERSION_GIT_BRANCH}\"\nLABEL tensorflow_serving_github_commit=\"${TF_SERVING_VERSION_GIT_COMMIT}\"\nRUN apt-get update \\\n && apt-get install --no-install-recommends ca-certificates -y \\\n && apt-get clean \\\n && rm -rf /var/lib/apt/lists/*\n#   Install TF Serving pkg\nCOPY --from=build_image /usr/local/bin/tensorflow_model_server /usr/bin/tensorflow_model_server\n#   Install MKL libraries\nCOPY --from=build_image /usr/local/lib/libiomp5.so /usr/local/lib\nCOPY --from=build_image /usr/local/lib/libmklml_gnu.so /usr/local/lib\nCOPY --from=build_image /usr/local/lib/libmklml_intel.so /usr/local/lib\nENV LIBRARY_PATH=\"'/usr/local/lib:$LIBRARY_PATH'\"\nENV LD_LIBRARY_PATH=\"'/usr/local/lib:$LD_LIBRARY_PATH'\"\n#   Expose ports\n#   gRPC\nEXPOSE 8500/tcp\n#   REST\nEXPOSE 8501/tcp\n#   Set where models should be stored in the container\nENV MODEL_BASE_PATH=\"/models\"\nRUN mkdir -p ${MODEL_BASE_PATH}\n#   The only required piece is the model name in order to differentiate endpoints\nENV MODEL_NAME=\"model\"\n#   Create a script that runs the model server so we can use environment variables\n#   while also passing in arguments from the docker command line\n#   Setting MKL environment variables can improve performance.\n#   https://www.tensorflow.org/guide/performance/overview\n#   Read about Tuning MKL for the best performance\n#   Add export MKLDNN_VERBOSE=1 to the below script,\n#   to see MKL messages in the docker logs when you send predict request.\n#   Based on our observations during experiments,\n#   setting tensorflow_session_parallelism=<1/4th of physical cores> and\n#   setting OMP_NUM_THREADS=<Total physical cores>\n#   gave optimal performance results with MKL\n#   KMP_BLOCKTIME=<Varies based on your model>\n#   NOTE: We don't guarantee same settings to give optimal peformance across all hardware\n#   please tune variables as required.\nENV OMP_NUM_THREADS=\"2\"\nENV KMP_BLOCKTIME=\"1\"\nENV KMP_SETTINGS=\"1\"\nENV KMP_AFFINITY=\"granularity=fine,verbose,compact,1,0\"\nENV MKLDNN_VERBOSE=\"0\"\n#   Recommended settings, may vary depending on the model.\n#   Set TENSORFLOW_INTRA_OP_PARALLELISM=#No.of Physical cores\n#   Set TENSORFLOW_INTER_OP_PARALLELISM=#No.of Sockets\n#   For more information vist below reference\n#   https://www.tensorflow.org/guide/performance/overview#tuning_mkl_for_the_best_performance\n#   NOTE: As TENSORFLOW_INTRA_OP_PARALLELISM and TENSORFLOW_INTER_OP_PARALLELISM are\n#   configured via SessionOptions in tensorflow, these values\n#   will override the values configured via TF_NUM_INTEROP_THREADS and TF_NUM_INTRAOP_THREADS\n#   environment variables in tensorflow.\n#   https://github.com/tensorflow/tensorflow/commit/d1823e2e966e96ee4ea7baa202ad9f292ac7427b\n#   Defaults\nENV TENSORFLOW_INTRA_OP_PARALLELISM=\"2\"\nENV TENSORFLOW_INTER_OP_PARALLELISM=\"2\"\nRUN echo '#!/bin/bash \\n\\ntensorflow_model_server --port=8500 --rest_api_port=8501 --tensorflow_intra_op_parallelism=${TENSORFLOW_INTRA_OP_PARALLELISM} --tensorflow_inter_op_parallelism=${TENSORFLOW_INTER_OP_PARALLELISM} --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \"$@\"' > /usr/bin/tf_serving_entrypoint.sh \\\n && chmod +x /usr/bin/tf_serving_entrypoint.sh\nENTRYPOINT [\"/usr/bin/tf_serving_entrypoint.sh\"]\n","originalDockerfileUglifiedHash":"17afc510a3664f84a1aab796ce46d4eb","fileName":"/ICSME-replicationpackage/dataset/smelly_dockerfiles_bianncle/3faeecbc2d829dd66f5cef939df60ab9df920efa.dockerfile"}