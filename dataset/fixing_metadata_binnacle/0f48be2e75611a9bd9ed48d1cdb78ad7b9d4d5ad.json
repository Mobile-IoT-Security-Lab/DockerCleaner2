{"seed":4165528639,"processedDockerfileHash":"2a025c7928fa160be96edad5d3d2c043","fixedSmells":["use-no-install-recommends","do-not-use-apt-get-update-alone","pin-package-manager-versions-apt-get","pin-package-manager-versions-pip","pin-package-manager-versions-npm","pin-package-manager-versions-gem","pin-package-manager-versions-apk","use-copy-instead-of-add","use-wget-instead-of-add","do-not-have-secrets","have-a-healthcheck","have-a-user"],"successfullyFixedSmells":["use-no-install-recommends","pin-package-manager-versions-apt-get","pin-package-manager-versions-pip","use-copy-instead-of-add"],"processedDockerfile":"#   VERSION 1.0 (apache-docker)\n#   AUTHOR: Abhishek Sharma<abhioncbr@yahoo.com>\n#   DESCRIPTION: docker apache airflow container\nFROM python:3.6\nMAINTAINER Abhishek Sharma <abhioncbr@yahoo.com>\nARG PYTHON_DEPS=\"boto3 \"\nARG AIRFLOW_VERSION\nARG AIRFLOW_PATCH_VERSION\nARG AIRFLOW_DEPS=\"all,password\"\nARG BUILD_DEPS=\"freetds-dev libkrb5-dev libsasl2-dev libssl-dev libffi-dev libpq-dev git\"\nARG OTHER_DEPS=\"sshpass openssh-server openssh-client less gcc make wget vim curl rsync netcat logrotate\"\nARG APT_DEPS=\"$BUILD_DEPS $OTHER_DEPS libsasl2-dev freetds-bin build-essential default-libmysqlclient-dev apt-utils locales \"\nENV AIRFLOW_HOME=\"/usr/local/airflow\"\nENV AIRFLOW_GPL_UNIDECODE=\"yes\"\n#  install dependencies packages.\nRUN set -x \\\n && apt-get update \\\n && if [ -n \"${APT_DEPS}\" ] ; then apt-get install --no-install-recommends $APT_DEPS -y ; fi\n#  Install Redis\nRUN apt-get update \\\n && apt-get policy redis-server \\\n && apt-get install --no-install-recommends redis-server=5:6.0.16-1+deb11u2 -y\n#  Install java for java based application.\nRUN apt-get update \\\n && apt-get policy openjdk-8-jdk \\\n && apt-get install --no-install-recommends openjdk-8-jdk -y\n#  for older versions[1.8.1, 1.8.2] of airflow, pip downgrading is required.\nRUN if [ ${AIRFLOW_VERSION} \\< \"1.8.3\" ] ; then pip install pip==9.0 ; else python -m pip install --upgrade pip setuptools wheel ; fi\n#  Install python dependencies.\nRUN if [ -n \"${PYTHON_DEPS}\" ] ; then pip install ${PYTHON_DEPS} --no-cache-dir ; fi\n#  Install Airflow all packages\nRUN pip install apache-airflow[$AIRFLOW_DEPS]==$AIRFLOW_VERSION \\\n && apt-get clean\n#  Install GCloud[GCP] packages.\nRUN curl https://dl.google.com/dl/cloudsdk/release/google-cloud-sdk.tar.gz > /tmp/google-cloud-sdk.tar.gz\nRUN mkdir -p /usr/local/gcloud\nRUN tar -C /usr/local/gcloud -xvf /tmp/google-cloud-sdk.tar.gz\nRUN /usr/local/gcloud/google-cloud-sdk/install.sh\nRUN pip install google-api-python-client==2.85.0 --upgrade \\\n && pip install google-cloud-storage==2.8.0\n#  Install aws cli.\nRUN pip3 install awscli --upgrade --user\n#  Adding 'airflow' as group & user.\nRUN groupadd -g 5555 airflow \\\n && useradd -ms /bin/bash -d ${AIRFLOW_HOME} -u 5555 -g 5555 -p airflow airflow \\\n && echo \"airflow:airflow\" | chpasswd \\\n && adduser airflow sudo\n#  Creating folder.\nRUN mkdir /code-artifacts /data /home/airflow ${AIRFLOW_HOME}/startup_log ${AIRFLOW_HOME}/.ssh ${AIRFLOW_HOME}/.aws ${AIRFLOW_HOME}/.gcp ${AIRFLOW_HOME}/dags ${AIRFLOW_HOME}/logs ${AIRFLOW_HOME}/plugins \\\n && mkdir -p /user/airflow \\\n && chown -R airflow:airflow ${AIRFLOW_HOME}/* /data /code-artifacts /user/airflow /home/airflow\nCOPY airflow-config/airflow-${AIRFLOW_VERSION}.cfg ${AIRFLOW_HOME}/airflow.cfg\nCOPY config/user_add.py ${AIRFLOW_HOME}/user_add.py\nCOPY config/rbac_user_add.py ${AIRFLOW_HOME}/rbac_user_add.py\nCOPY config/execute_continous_scheduler.sh ${AIRFLOW_HOME}/execute_continous_scheduler.sh\n#   airflow patch files present in airflowPatch folder\nRUN mkdir /tmp/airflow_patch\nCOPY airflowPatch1.8/* /tmp/airflow_patch/airflowPatch1.8/\nCOPY airflowPatch1.9/* /tmp/airflow_patch/airflowPatch1.9/\nCOPY airflowPatch1.10/* /tmp/airflow_patch/airflowPatch1.10/\nRUN if [ ! -z \"$AIRFLOW_PATCH_VERSION\" ] ; then cp /tmp/airflow_patch/airflowPatch${AIRFLOW_PATCH_VERSION}/models.py /usr/local/lib/python3.6/site-packages/airflow/models.py ;cp /tmp/airflow_patch/airflowPatch${AIRFLOW_PATCH_VERSION}/views.py /usr/local/lib/python3.6/site-packages/airflow/www/views.py ;cp /tmp/airflow_patch/airflowPatch${AIRFLOW_PATCH_VERSION}/password_auth.py /usr/local/lib/python3.6/site-packages/airflow/contrib/auth/backends/password_auth.py ;cp /tmp/airflow_patch/airflowPatch${AIRFLOW_PATCH_VERSION}/e3a246e0dc1_current_schema.py /usr/local/lib/python3.6/site-packages/airflow/migrations/versions/e3a246e0dc1_current_schema.py ;chown root:staff /usr/local/lib/python3.6/site-packages/airflow/models.py ;chown root:staff /usr/local/lib/python3.6/site-packages/airflow/www/views.py ;chown root:staff /usr/local/lib/python3.6/site-packages/airflow/contrib/auth/backends/password_auth.py ;chown root:staff /usr/local/lib/python3.6/site-packages/airflow/migrations/versions/e3a246e0dc1_current_schema.py ; fi\nRUN rm -rf /tmp/airflow_patch\n#  Adding S3 logger.\nCOPY airflowExtraFeatures/s3_logger.py /usr/local/lib/python3.6/site-packages/airflow/config_templates/s3_logger.py\nRUN chown root:staff /usr/local/lib/python3.6/site-packages/airflow/config_templates/s3_logger.py\nRUN chown -R airflow:airflow ${AIRFLOW_HOME}/*\nVOLUME /usr/hdp\nVOLUME /code-artifacts\nVOLUME ${AIRFLOW_HOME}/.gcp\nVOLUME ${AIRFLOW_HOME}/.aws\nVOLUME ${AIRFLOW_HOME}/dags\nVOLUME ${AIRFLOW_HOME}/logs\nENV PATH=\"$PATH::/usr/local/gcloud/google-cloud-sdk/bin/\"\nENV PYTHONPATH=\"${PYTHONPATH}:/usr/local/lib/python3.6/\"\n#  for airflow processes log rotation.\nCOPY logrotate/airflow /etc/logrotate.d/airflow\nCOPY logrotate/run_log_rotate.sh ${AIRFLOW_HOME}/run_log_rotate.sh\n#  setting up logrotation cron.\nRUN echo \"30 * * * * /bin/sh ${AIRFLOW_HOME}/run_log_rotate.sh >> ${AIRFLOW_HOME}/logRotate_logs.txt\" >> ${AIRFLOW_HOME}/airflow_cron\nRUN crontab ${AIRFLOW_HOME}/airflow_cron\nRUN rm ${AIRFLOW_HOME}/airflow_cron\n#  ENV HDP_VERSION=2.6.1.0-129\n#  export HDP_VERSION=${HDP_VERSION}\n#  export HADOOP_CONF_DIR=/etc/hadoop/${HDP_VERSION}/0\n#  export SPARK_CONF_DIR=/etc/spark/${HDP_VERSION}/0\n#  export HIVE_CONF_DIR=/etc/hive/${HDP_VERSION}/0\n#  export TEZ_CONF_DIR=/etc/tez/${HDP_VERSION}/0\nEXPOSE 5555/tcp 8793/tcp 2222/tcp 6379/tcp\nCOPY script/docker-entrypoint.sh /usr/local/bin/\nRUN chmod +x /usr/local/bin/docker-entrypoint.sh\nRUN ln -s /usr/local/bin/docker-entrypoint.sh /entrypoint.sh\nWORKDIR ${AIRFLOW_HOME}\nUSER airflow\n#  setting default mode of docker-airflow as 'standalone'. Will be helpful when from Kitematic.\nENV MODE=\"standalone\"\nHEALTHCHECK CMD [\"curl\", \"-f\", \"http://localhost:2222/health\"]\nENTRYPOINT [\"docker-entrypoint.sh\"]\n","originalDockerfile":"#  VERSION 1.0 (apache-docker)\n#  AUTHOR: Abhishek Sharma<abhioncbr@yahoo.com>\n#  DESCRIPTION: docker apache airflow container\nFROM python:3.6\nMAINTAINER Abhishek Sharma <abhioncbr@yahoo.com>\nARG PYTHON_DEPS=\"boto3 \"\nARG AIRFLOW_VERSION\nARG AIRFLOW_PATCH_VERSION\nARG AIRFLOW_DEPS=\"all,password\"\nARG BUILD_DEPS=\"freetds-dev libkrb5-dev libsasl2-dev libssl-dev libffi-dev libpq-dev git\"\nARG OTHER_DEPS=\"sshpass openssh-server openssh-client less gcc make wget vim curl rsync netcat logrotate\"\nARG APT_DEPS=\"$BUILD_DEPS $OTHER_DEPS libsasl2-dev freetds-bin build-essential default-libmysqlclient-dev apt-utils locales \"\nENV AIRFLOW_HOME=\"/usr/local/airflow\"\nENV AIRFLOW_GPL_UNIDECODE=\"yes\"\n# install dependencies packages.\nRUN set -x \\\n && apt-get update \\\n && if [ -n \"${APT_DEPS}\" ] ; then apt-get install $APT_DEPS -y ; fi\n# Install Redis\nRUN apt-get update \\\n && apt-get policy redis-server \\\n && apt-get install redis-server -y\n# Install java for java based application.\nRUN apt-get update \\\n && apt-get policy openjdk-8-jdk \\\n && apt-get install openjdk-8-jdk -y\n# for older versions[1.8.1, 1.8.2] of airflow, pip downgrading is required.\nRUN if [ ${AIRFLOW_VERSION} \\< \"1.8.3\" ] ; then pip install pip==9.0 ; else python -m pip install --upgrade pip setuptools wheel ; fi\n# Install python dependencies.\nRUN if [ -n \"${PYTHON_DEPS}\" ] ; then pip install ${PYTHON_DEPS} --no-cache-dir ; fi\n# Install Airflow all packages\nRUN pip install apache-airflow[$AIRFLOW_DEPS]==$AIRFLOW_VERSION \\\n && apt-get clean\n# Install GCloud[GCP] packages.\nRUN curl https://dl.google.com/dl/cloudsdk/release/google-cloud-sdk.tar.gz > /tmp/google-cloud-sdk.tar.gz\nRUN mkdir -p /usr/local/gcloud\nRUN tar -C /usr/local/gcloud -xvf /tmp/google-cloud-sdk.tar.gz\nRUN /usr/local/gcloud/google-cloud-sdk/install.sh\nRUN pip install google-api-python-client --upgrade \\\n && pip install google-cloud-storage\n# Install aws cli.\nRUN pip3 install awscli --upgrade --user\n# Adding 'airflow' as group & user.\nRUN groupadd -g 5555 airflow \\\n && useradd -ms /bin/bash -d ${AIRFLOW_HOME} -u 5555 -g 5555 -p airflow airflow \\\n && echo \"airflow:airflow\" | chpasswd \\\n && adduser airflow sudo\n# Creating folder.\nRUN mkdir /code-artifacts /data /home/airflow ${AIRFLOW_HOME}/startup_log ${AIRFLOW_HOME}/.ssh ${AIRFLOW_HOME}/.aws ${AIRFLOW_HOME}/.gcp ${AIRFLOW_HOME}/dags ${AIRFLOW_HOME}/logs ${AIRFLOW_HOME}/plugins \\\n && mkdir -p /user/airflow \\\n && chown -R airflow:airflow ${AIRFLOW_HOME}/* /data /code-artifacts /user/airflow /home/airflow\nADD airflow-config/airflow-${AIRFLOW_VERSION}.cfg ${AIRFLOW_HOME}/airflow.cfg\nADD config/user_add.py ${AIRFLOW_HOME}/user_add.py\nADD config/rbac_user_add.py ${AIRFLOW_HOME}/rbac_user_add.py\nADD config/execute_continous_scheduler.sh ${AIRFLOW_HOME}/execute_continous_scheduler.sh\n#  airflow patch files present in airflowPatch folder\nRUN mkdir /tmp/airflow_patch\nADD airflowPatch1.8/* /tmp/airflow_patch/airflowPatch1.8/\nADD airflowPatch1.9/* /tmp/airflow_patch/airflowPatch1.9/\nADD airflowPatch1.10/* /tmp/airflow_patch/airflowPatch1.10/\nRUN if [ ! -z \"$AIRFLOW_PATCH_VERSION\" ] ; then cp /tmp/airflow_patch/airflowPatch${AIRFLOW_PATCH_VERSION}/models.py /usr/local/lib/python3.6/site-packages/airflow/models.py ;cp /tmp/airflow_patch/airflowPatch${AIRFLOW_PATCH_VERSION}/views.py /usr/local/lib/python3.6/site-packages/airflow/www/views.py ;cp /tmp/airflow_patch/airflowPatch${AIRFLOW_PATCH_VERSION}/password_auth.py /usr/local/lib/python3.6/site-packages/airflow/contrib/auth/backends/password_auth.py ;cp /tmp/airflow_patch/airflowPatch${AIRFLOW_PATCH_VERSION}/e3a246e0dc1_current_schema.py /usr/local/lib/python3.6/site-packages/airflow/migrations/versions/e3a246e0dc1_current_schema.py ;chown root:staff /usr/local/lib/python3.6/site-packages/airflow/models.py ;chown root:staff /usr/local/lib/python3.6/site-packages/airflow/www/views.py ;chown root:staff /usr/local/lib/python3.6/site-packages/airflow/contrib/auth/backends/password_auth.py ;chown root:staff /usr/local/lib/python3.6/site-packages/airflow/migrations/versions/e3a246e0dc1_current_schema.py ; fi\nRUN rm -rf /tmp/airflow_patch\n# Adding S3 logger.\nADD airflowExtraFeatures/s3_logger.py /usr/local/lib/python3.6/site-packages/airflow/config_templates/s3_logger.py\nRUN chown root:staff /usr/local/lib/python3.6/site-packages/airflow/config_templates/s3_logger.py\nRUN chown -R airflow:airflow ${AIRFLOW_HOME}/*\nVOLUME /usr/hdp\nVOLUME /code-artifacts\nVOLUME ${AIRFLOW_HOME}/.gcp\nVOLUME ${AIRFLOW_HOME}/.aws\nVOLUME ${AIRFLOW_HOME}/dags\nVOLUME ${AIRFLOW_HOME}/logs\nENV PATH=\"$PATH::/usr/local/gcloud/google-cloud-sdk/bin/\"\nENV PYTHONPATH=\"${PYTHONPATH}:/usr/local/lib/python3.6/\"\n# for airflow processes log rotation.\nADD logrotate/airflow /etc/logrotate.d/airflow\nADD logrotate/run_log_rotate.sh ${AIRFLOW_HOME}/run_log_rotate.sh\n# setting up logrotation cron.\nRUN echo \"30 * * * * /bin/sh ${AIRFLOW_HOME}/run_log_rotate.sh >> ${AIRFLOW_HOME}/logRotate_logs.txt\" >> ${AIRFLOW_HOME}/airflow_cron\nRUN crontab ${AIRFLOW_HOME}/airflow_cron\nRUN rm ${AIRFLOW_HOME}/airflow_cron\n# ENV HDP_VERSION=2.6.1.0-129\n# export HDP_VERSION=${HDP_VERSION}\n# export HADOOP_CONF_DIR=/etc/hadoop/${HDP_VERSION}/0\n# export SPARK_CONF_DIR=/etc/spark/${HDP_VERSION}/0\n# export HIVE_CONF_DIR=/etc/hive/${HDP_VERSION}/0\n# export TEZ_CONF_DIR=/etc/tez/${HDP_VERSION}/0\nEXPOSE 5555/tcp 8793/tcp 2222/tcp 6379/tcp\nCOPY script/docker-entrypoint.sh /usr/local/bin/\nRUN chmod +x /usr/local/bin/docker-entrypoint.sh\nRUN ln -s /usr/local/bin/docker-entrypoint.sh /entrypoint.sh\nWORKDIR ${AIRFLOW_HOME}\nUSER airflow\n# setting default mode of docker-airflow as 'standalone'. Will be helpful when from Kitematic.\nENV MODE=\"standalone\"\nHEALTHCHECK CMD [\"curl\", \"-f\", \"http://localhost:2222/health\"]\nENTRYPOINT [\"docker-entrypoint.sh\"]\n","injectedSmells":[],"originalDockerfileHash":"93d75a5dca6d0e1f7e0996d9f848ec3f","successfullyInjectedSmells":[],"originalDockerfileUglified":"#   VERSION 1.0 (apache-docker)\n#   AUTHOR: Abhishek Sharma<abhioncbr@yahoo.com>\n#   DESCRIPTION: docker apache airflow container\nFROM python:3.6\nMAINTAINER Abhishek Sharma <abhioncbr@yahoo.com>\nARG PYTHON_DEPS=\"boto3 \"\nARG AIRFLOW_VERSION\nARG AIRFLOW_PATCH_VERSION\nARG AIRFLOW_DEPS=\"all,password\"\nARG BUILD_DEPS=\"freetds-dev libkrb5-dev libsasl2-dev libssl-dev libffi-dev libpq-dev git\"\nARG OTHER_DEPS=\"sshpass openssh-server openssh-client less gcc make wget vim curl rsync netcat logrotate\"\nARG APT_DEPS=\"$BUILD_DEPS $OTHER_DEPS libsasl2-dev freetds-bin build-essential default-libmysqlclient-dev apt-utils locales \"\nENV AIRFLOW_HOME=\"/usr/local/airflow\"\nENV AIRFLOW_GPL_UNIDECODE=\"yes\"\n#  install dependencies packages.\nRUN set -x \\\n && apt-get update \\\n && if [ -n \"${APT_DEPS}\" ] ; then apt-get install $APT_DEPS -y ; fi\n#  Install Redis\nRUN apt-get update \\\n && apt-get policy redis-server \\\n && apt-get install redis-server -y\n#  Install java for java based application.\nRUN apt-get update \\\n && apt-get policy openjdk-8-jdk \\\n && apt-get install openjdk-8-jdk -y\n#  for older versions[1.8.1, 1.8.2] of airflow, pip downgrading is required.\nRUN if [ ${AIRFLOW_VERSION} \\< \"1.8.3\" ] ; then pip install pip==9.0 ; else python -m pip install --upgrade pip setuptools wheel ; fi\n#  Install python dependencies.\nRUN if [ -n \"${PYTHON_DEPS}\" ] ; then pip install ${PYTHON_DEPS} --no-cache-dir ; fi\n#  Install Airflow all packages\nRUN pip install apache-airflow[$AIRFLOW_DEPS]==$AIRFLOW_VERSION \\\n && apt-get clean\n#  Install GCloud[GCP] packages.\nRUN curl https://dl.google.com/dl/cloudsdk/release/google-cloud-sdk.tar.gz > /tmp/google-cloud-sdk.tar.gz\nRUN mkdir -p /usr/local/gcloud\nRUN tar -C /usr/local/gcloud -xvf /tmp/google-cloud-sdk.tar.gz\nRUN /usr/local/gcloud/google-cloud-sdk/install.sh\nRUN pip install google-api-python-client --upgrade \\\n && pip install google-cloud-storage\n#  Install aws cli.\nRUN pip3 install awscli --upgrade --user\n#  Adding 'airflow' as group & user.\nRUN groupadd -g 5555 airflow \\\n && useradd -ms /bin/bash -d ${AIRFLOW_HOME} -u 5555 -g 5555 -p airflow airflow \\\n && echo \"airflow:airflow\" | chpasswd \\\n && adduser airflow sudo\n#  Creating folder.\nRUN mkdir /code-artifacts /data /home/airflow ${AIRFLOW_HOME}/startup_log ${AIRFLOW_HOME}/.ssh ${AIRFLOW_HOME}/.aws ${AIRFLOW_HOME}/.gcp ${AIRFLOW_HOME}/dags ${AIRFLOW_HOME}/logs ${AIRFLOW_HOME}/plugins \\\n && mkdir -p /user/airflow \\\n && chown -R airflow:airflow ${AIRFLOW_HOME}/* /data /code-artifacts /user/airflow /home/airflow\nADD airflow-config/airflow-${AIRFLOW_VERSION}.cfg ${AIRFLOW_HOME}/airflow.cfg\nADD config/user_add.py ${AIRFLOW_HOME}/user_add.py\nADD config/rbac_user_add.py ${AIRFLOW_HOME}/rbac_user_add.py\nADD config/execute_continous_scheduler.sh ${AIRFLOW_HOME}/execute_continous_scheduler.sh\n#   airflow patch files present in airflowPatch folder\nRUN mkdir /tmp/airflow_patch\nADD airflowPatch1.8/* /tmp/airflow_patch/airflowPatch1.8/\nADD airflowPatch1.9/* /tmp/airflow_patch/airflowPatch1.9/\nADD airflowPatch1.10/* /tmp/airflow_patch/airflowPatch1.10/\nRUN if [ ! -z \"$AIRFLOW_PATCH_VERSION\" ] ; then cp /tmp/airflow_patch/airflowPatch${AIRFLOW_PATCH_VERSION}/models.py /usr/local/lib/python3.6/site-packages/airflow/models.py ;cp /tmp/airflow_patch/airflowPatch${AIRFLOW_PATCH_VERSION}/views.py /usr/local/lib/python3.6/site-packages/airflow/www/views.py ;cp /tmp/airflow_patch/airflowPatch${AIRFLOW_PATCH_VERSION}/password_auth.py /usr/local/lib/python3.6/site-packages/airflow/contrib/auth/backends/password_auth.py ;cp /tmp/airflow_patch/airflowPatch${AIRFLOW_PATCH_VERSION}/e3a246e0dc1_current_schema.py /usr/local/lib/python3.6/site-packages/airflow/migrations/versions/e3a246e0dc1_current_schema.py ;chown root:staff /usr/local/lib/python3.6/site-packages/airflow/models.py ;chown root:staff /usr/local/lib/python3.6/site-packages/airflow/www/views.py ;chown root:staff /usr/local/lib/python3.6/site-packages/airflow/contrib/auth/backends/password_auth.py ;chown root:staff /usr/local/lib/python3.6/site-packages/airflow/migrations/versions/e3a246e0dc1_current_schema.py ; fi\nRUN rm -rf /tmp/airflow_patch\n#  Adding S3 logger.\nADD airflowExtraFeatures/s3_logger.py /usr/local/lib/python3.6/site-packages/airflow/config_templates/s3_logger.py\nRUN chown root:staff /usr/local/lib/python3.6/site-packages/airflow/config_templates/s3_logger.py\nRUN chown -R airflow:airflow ${AIRFLOW_HOME}/*\nVOLUME /usr/hdp\nVOLUME /code-artifacts\nVOLUME ${AIRFLOW_HOME}/.gcp\nVOLUME ${AIRFLOW_HOME}/.aws\nVOLUME ${AIRFLOW_HOME}/dags\nVOLUME ${AIRFLOW_HOME}/logs\nENV PATH=\"$PATH::/usr/local/gcloud/google-cloud-sdk/bin/\"\nENV PYTHONPATH=\"${PYTHONPATH}:/usr/local/lib/python3.6/\"\n#  for airflow processes log rotation.\nADD logrotate/airflow /etc/logrotate.d/airflow\nADD logrotate/run_log_rotate.sh ${AIRFLOW_HOME}/run_log_rotate.sh\n#  setting up logrotation cron.\nRUN echo \"30 * * * * /bin/sh ${AIRFLOW_HOME}/run_log_rotate.sh >> ${AIRFLOW_HOME}/logRotate_logs.txt\" >> ${AIRFLOW_HOME}/airflow_cron\nRUN crontab ${AIRFLOW_HOME}/airflow_cron\nRUN rm ${AIRFLOW_HOME}/airflow_cron\n#  ENV HDP_VERSION=2.6.1.0-129\n#  export HDP_VERSION=${HDP_VERSION}\n#  export HADOOP_CONF_DIR=/etc/hadoop/${HDP_VERSION}/0\n#  export SPARK_CONF_DIR=/etc/spark/${HDP_VERSION}/0\n#  export HIVE_CONF_DIR=/etc/hive/${HDP_VERSION}/0\n#  export TEZ_CONF_DIR=/etc/tez/${HDP_VERSION}/0\nEXPOSE 5555/tcp 8793/tcp 2222/tcp 6379/tcp\nCOPY script/docker-entrypoint.sh /usr/local/bin/\nRUN chmod +x /usr/local/bin/docker-entrypoint.sh\nRUN ln -s /usr/local/bin/docker-entrypoint.sh /entrypoint.sh\nWORKDIR ${AIRFLOW_HOME}\nUSER airflow\n#  setting default mode of docker-airflow as 'standalone'. Will be helpful when from Kitematic.\nENV MODE=\"standalone\"\nHEALTHCHECK CMD [\"curl\", \"-f\", \"http://localhost:2222/health\"]\nENTRYPOINT [\"docker-entrypoint.sh\"]\n","originalDockerfileUglifiedHash":"c82fb17674694c5dd4088e94a0fbdf57","fileName":"/ICSME-replicationpackage/dataset/smelly_dockerfiles_bianncle/0f48be2e75611a9bd9ed48d1cdb78ad7b9d4d5ad.dockerfile"}