{"seed":3114800265,"processedDockerfileHash":"dc25f041c2e6021d13192ef735aef399","fixedSmells":["use-no-install-recommends","do-not-use-apt-get-update-alone","pin-package-manager-versions-apt-get","pin-package-manager-versions-pip","pin-package-manager-versions-npm","pin-package-manager-versions-gem","pin-package-manager-versions-apk","use-copy-instead-of-add","use-wget-instead-of-add","do-not-have-secrets","have-a-healthcheck","have-a-user"],"successfullyFixedSmells":["pin-package-manager-versions-apt-get","have-a-healthcheck","have-a-user"],"processedDockerfile":"#   Copyright (c) 2018-2019, NVIDIA CORPORATION. All rights reserved.\n#\n#   Redistribution and use in source and binary forms, with or without\n#   modification, are permitted provided that the following conditions\n#   are met:\n#    * Redistributions of source code must retain the above copyright\n#      notice, this list of conditions and the following disclaimer.\n#    * Redistributions in binary form must reproduce the above copyright\n#      notice, this list of conditions and the following disclaimer in the\n#      documentation and/or other materials provided with the distribution.\n#    * Neither the name of NVIDIA CORPORATION nor the names of its\n#      contributors may be used to endorse or promote products derived\n#      from this software without specific prior written permission.\n#\n#   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\n#   EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n#   IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n#   PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n#   CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n#   EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n#   PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n#   PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n#   OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n#   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n#   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n#\n#   Multistage build.\n#\nARG BASE_IMAGE=nvcr.io/nvidia/tensorrtserver:19.05-py3\nARG PYTORCH_IMAGE=nvcr.io/nvidia/pytorch:19.05-py3\nARG TENSORFLOW_IMAGE=nvcr.io/nvidia/tensorflow:19.05-py3\n#  ###########################################################################\n#  # TensorFlow stage: Use TensorFlow container to build\n#  ###########################################################################\nFROM ${TENSORFLOW_IMAGE} AS trtserver_tf\n#   Modify the TF model loader to allow us to set the default GPU for\n#   multi-GPU support\nCOPY tools/patch/tensorflow /tmp/trtis/tools/patch/tensorflow\nRUN sha1sum -c /tmp/trtis/tools/patch/tensorflow/checksums \\\n && patch -i /tmp/trtis/tools/patch/tensorflow/cc/saved_model/loader.cc /opt/tensorflow/tensorflow/cc/saved_model/loader.cc \\\n && patch -i /tmp/trtis/tools/patch/tensorflow/BUILD /opt/tensorflow/tensorflow/BUILD \\\n && patch -i /tmp/trtis/tools/patch/tensorflow/tf_version_script.lds /opt/tensorflow/tensorflow/tf_version_script.lds \\\n && patch -i /tmp/trtis/tools/patch/tensorflow/nvbuild.sh /opt/tensorflow/nvbuild.sh \\\n && patch -i /tmp/trtis/tools/patch/tensorflow/nvbuildopts /opt/tensorflow/nvbuildopts \\\n && patch -i /tmp/trtis/tools/patch/tensorflow/bazel_build.sh /opt/tensorflow/bazel_build.sh\n#   Copy tensorflow_backend_tf into TensorFlow so it builds into the\n#   monolithic libtensorflow_cc library. We want tensorflow_backend_tf\n#   to build against the TensorFlow protobuf since it interfaces with\n#   that code.\nCOPY src/backends/tensorflow/tensorflow_backend_tf.* /opt/tensorflow/tensorflow/\n#   Build TensorFlow library for TRTIS\nWORKDIR /opt/tensorflow\nRUN ./nvbuild.sh --python3.5\n#  ###########################################################################\n#  # PyTorch stage: Use PyTorch container for Caffe2 and libtorch\n#  ###########################################################################\nFROM ${PYTORCH_IMAGE} AS trtserver_caffe2\n#   Copy netdef_backend_c2 into Caffe2 core so it builds into the\n#   libcaffe2 library. We want netdef_backend_c2 to build against the\n#   Caffe2 protobuf since it interfaces with that code.\nCOPY src/backends/caffe2/netdef_backend_c2.* /opt/pytorch/pytorch/caffe2/core/\n#   Build same as in pytorch container... except for the NO_DISTRIBUTED\n#   line where we turn off features not needed for trtserver\n#   This will build both the caffe2 libraries needed by the Caffe2 NetDef backend\n#   and the LibTorch library needed by the PyTorch backend.\nWORKDIR /opt/pytorch\nRUN pip uninstall -y torch\nRUN cd pytorch \\\n && TORCH_CUDA_ARCH_LIST=\"5.2 6.0 6.1 7.0 7.5+PTX\" CMAKE_PREFIX_PATH=\"$( dirname $( which conda ;) ;)/../\" USE_DISTRIBUTED=0 USE_MIOPEN=0 USE_NCCL=0 USE_OPENCV=0 USE_LEVELDB=0 USE_LMDB=0 USE_REDIS=0 BUILD_TEST=0 pip install --no-cache-dir -v .\n#  ###########################################################################\n#  # Onnx Runtime stage: Build Onnx Runtime on CUDA 10, CUDNN 7\n#  ###########################################################################\nFROM ${BASE_IMAGE} AS trtserver_onnx\n#   Currently the prebuilt Onnx Runtime library is built on CUDA 9, thus it\n#   needs to be built from source\n#   Onnx Runtime release version\nARG ONNX_RUNTIME_VERSION=0.4.0\n#   Get release version of Onnx Runtime\nWORKDIR /workspace\nRUN apt-get update \\\n && apt-get install --no-install-recommends git=1:2.39.2-1ubuntu1 -y\n#   Check out master until new release to support cloud-based filesystems\nRUN git clone --recursive https://github.com/Microsoft/onnxruntime\nENV PATH=\"/opt/cmake/bin:${PATH}\"\nARG SCRIPT_DIR=/workspace/onnxruntime/tools/ci_build/github/linux/docker/scripts\nRUN cp -r ${SCRIPT_DIR} /tmp/scripts \\\n && ${SCRIPT_DIR}/install_ubuntu.sh \\\n && ${SCRIPT_DIR}/install_deps.sh\n#   Allow configure to pick up GDK and CuDNN where it expects it.\n#   (Note: $CUDNN_VERSION is defined by NVidia's base image)\nRUN _CUDNN_VERSION=$( echo $CUDNN_VERSION | cut -d. -f1-2 ;) \\\n && mkdir -p /usr/local/cudnn-$_CUDNN_VERSION/cuda/include \\\n && ln -s /usr/include/cudnn.h /usr/local/cudnn-$_CUDNN_VERSION/cuda/include/cudnn.h \\\n && mkdir -p /usr/local/cudnn-$_CUDNN_VERSION/cuda/lib64 \\\n && ln -s /etc/alternatives/libcudnn_so /usr/local/cudnn-$_CUDNN_VERSION/cuda/lib64/libcudnn.so\n#   Build and Install LLVM\nARG LLVM_VERSION=6.0.1\nRUN cd /tmp \\\n && wget --no-verbose http://releases.llvm.org/$LLVM_VERSION/llvm-$LLVM_VERSION.src.tar.xz \\\n && xz -d llvm-$LLVM_VERSION.src.tar.xz \\\n && tar xvf llvm-$LLVM_VERSION.src.tar \\\n && cd llvm-$LLVM_VERSION.src \\\n && mkdir -p build \\\n && cd build \\\n && cmake .. -DCMAKE_BUILD_TYPE=Release \\\n && cmake --build . -- -j$( nproc ;) \\\n && cmake -DCMAKE_INSTALL_PREFIX=/usr/local/llvm-$LLVM_VERSION -DBUILD_TYPE=Release -P cmake_install.cmake \\\n && cd /tmp \\\n && rm -rf llvm*\nENV LD_LIBRARY_PATH=\"/usr/local/openblas/lib:$LD_LIBRARY_PATH\"\n#   Build files will be in /workspace/build\nARG COMMON_BUILD_ARGS=\"--skip_submodule_sync --parallel --build_shared_lib --use_openmp\"\nRUN mkdir -p /workspace/build\nRUN python3 /workspace/onnxruntime/tools/ci_build/build.py --build_dir /workspace/build --config Release $COMMON_BUILD_ARGS --use_cuda --cuda_home /usr/local/cuda --cudnn_home /usr/local/cudnn-$( echo $CUDNN_VERSION | cut -d. -f1-2 ;)/cuda --update --build\n#  ###########################################################################\n#  # Build stage: Build inference server\n#  ###########################################################################\nFROM ${BASE_IMAGE} AS trtserver_build\nARG TRTIS_VERSION=1.4.0dev\nARG TRTIS_CONTAINER_VERSION=19.07dev\n#   libgoogle-glog0v5 is needed by caffe2 libraries.\nRUN apt-get update \\\n && apt-get install --no-install-recommends software-properties-common=0.99.35 autoconf=2.71-3 automake=1:1.16.5-1.3 build-essential=12.9ubuntu3 cmake=3.25.1-1 git=1:2.39.2-1ubuntu1 libgoogle-glog0v5 libre2-dev=20230201-1 libssl-dev=3.0.8-1ubuntu1 libtool=2.4.7-5 -y\n#   libcurl4-openSSL-dev is needed for GCS\nRUN if [ $( cat /etc/os-release | grep 'VERSION_ID=\"16.04\"' | wc -l ;) -ne 0 ] ; then apt-get update \\\n && apt-get install --no-install-recommends libcurl3-dev -y ; elif [ $( cat /etc/os-release | grep 'VERSION_ID=\"18.04\"' | wc -l ;) -ne 0 ] ; then apt-get update \\\n && apt-get install --no-install-recommends libcurl4-openssl-dev=7.88.1-7ubuntu1 -y ; else echo \"Ubuntu version must be either 16.04 or 18.04\" \\\n && exit 1 ; fi\n#   TensorFlow libraries. Install the monolithic libtensorflow_cc and\n#   create a link libtensorflow_framework.so -> libtensorflow_cc.so so\n#   that custom tensorflow operations work correctly. Custom TF\n#   operations link against libtensorflow_framework.so so it must be\n#   present (and its functionality is provided by libtensorflow_cc.so).\nCOPY --from=trtserver_tf /usr/local/lib/tensorflow/libtensorflow_cc.so /opt/tensorrtserver/lib/\nRUN cd /opt/tensorrtserver/lib \\\n && ln -s libtensorflow_cc.so libtensorflow_framework.so\n#   Caffe2 libraries\nCOPY --from=trtserver_caffe2 /opt/conda/lib/python3.6/site-packages/torch/lib/libcaffe2_detectron_ops_gpu.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/python3.6/site-packages/torch/lib/libcaffe2.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/python3.6/site-packages/torch/lib/libcaffe2_gpu.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/python3.6/site-packages/torch/lib/libc10.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/python3.6/site-packages/torch/lib/libc10_cuda.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/libmkl_avx2.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/libmkl_core.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/libmkl_def.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/libmkl_gnu_thread.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/libmkl_intel_lp64.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/libmkl_rt.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/libmkl_vml_def.so /opt/tensorrtserver/lib/\n#   LibTorch headers and library\nCOPY --from=trtserver_caffe2 /opt/conda/lib/python3.6/site-packages/torch/include /opt/tensorrtserver/include/torch\nCOPY --from=trtserver_caffe2 /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so.1 /opt/tensorrtserver/lib/\nRUN cd /opt/tensorrtserver/lib \\\n && ln -s libtorch.so.1 libtorch.so\nCOPY --from=trtserver_caffe2 /opt/conda/lib/python3.6/site-packages/torch/lib/libthnvrtc.so /opt/tensorrtserver/lib/\n#   Onnx Runtime headers and library\nARG ONNX_RUNTIME_VERSION=0.4.0\nCOPY --from=trtserver_onnx /workspace/onnxruntime/include/onnxruntime /opt/tensorrtserver/include/onnxruntime/\nCOPY --from=trtserver_onnx /workspace/build/Release/libonnxruntime.so.${ONNX_RUNTIME_VERSION} /opt/tensorrtserver/lib/\nRUN cd /opt/tensorrtserver/lib \\\n && ln -s libonnxruntime.so.${ONNX_RUNTIME_VERSION} libonnxruntime.so\n#   Copy entire repo into container even though some is not needed for\n#   build itself... because we want to be able to copyright check on\n#   files that aren't directly needed for build.\nWORKDIR /workspace\nRUN rm -fr *\nCOPY . .\n#   Build the server.\n#\n#   - Need to find CUDA stubs if they are available since some backends\n#   may need to link against them. This is identical to the login in TF\n#   container nvbuild.sh\n#\n#   - The build can fail the first time due to a protobuf_generate_cpp\n#   error that doesn't repeat on subsequent builds, which is why there\n#   are 2 make invocations below.\nRUN LIBCUDA_FOUND=$( ldconfig -p | grep -v compat | awk '{print $1}' | grep libcuda.so | wc -l ;) \\\n && if [[ \"$LIBCUDA_FOUND\" -eq 0 ]] ; then export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64/stubs ;ln -fs /usr/local/cuda/lib64/stubs/libcuda.so /usr/local/cuda/lib64/stubs/libcuda.so.1 ; fi \\\n && echo $LD_LIBRARY_PATH \\\n && rm -fr builddir \\\n && mkdir -p builddir \\\n && (cd builddir \\\n && cmake -DCMAKE_BUILD_TYPE=Release -DTRTIS_ENABLE_METRICS=ON -DTRTIS_ENABLE_GCS=ON -DTRTIS_ENABLE_CUSTOM=ON -DTRTIS_ENABLE_TENSORFLOW=ON -DTRTIS_ENABLE_TENSORRT=ON -DTRTIS_ENABLE_CAFFE2=ON -DTRTIS_ENABLE_ONNXRUNTIME=ON -DTRTIS_ENABLE_PYTORCH=ON -DTRTIS_ONNXRUNTIME_INCLUDE_PATHS=\"/opt/tensorrtserver/include/onnxruntime\" -DTRTIS_PYTORCH_INCLUDE_PATHS=\"/opt/tensorrtserver/include/torch\" -DTRTIS_EXTRA_LIB_PATHS=\"/opt/tensorrtserver/lib\" ../build \\\n && (make -j16 trtis || true ) \\\n && make -j16 trtis \\\n && mkdir -p /opt/tensorrtserver/include \\\n && cp -r trtis/install/bin /opt/tensorrtserver/. \\\n && cp -r trtis/install/lib /opt/tensorrtserver/. \\\n && cp -r trtis/install/include /opt/tensorrtserver/include/trtserver ) \\\n && (cd /opt/tensorrtserver \\\n && ln -s /workspace/qa qa )\nENV TENSORRT_SERVER_VERSION=\"${TRTIS_VERSION}\"\nENV NVIDIA_TENSORRT_SERVER_VERSION=\"${TRTIS_CONTAINER_VERSION}\"\nENV LD_LIBRARY_PATH=\"/opt/tensorrtserver/lib:${LD_LIBRARY_PATH}\"\nENV PATH=\"/opt/tensorrtserver/bin:${PATH}\"\nCOPY nvidia_entrypoint.sh /opt/tensorrtserver\nENTRYPOINT [\"/opt/tensorrtserver/nvidia_entrypoint.sh\"]\n#  ###########################################################################\n#  #  Production stage: Create container with just inference server executable\n#  ###########################################################################\nFROM ${BASE_IMAGE}\nARG TRTIS_VERSION=1.4.0dev\nARG TRTIS_CONTAINER_VERSION=19.07dev\nENV TENSORRT_SERVER_VERSION=\"${TRTIS_VERSION}\"\nENV NVIDIA_TENSORRT_SERVER_VERSION=\"${TRTIS_CONTAINER_VERSION}\"\nLABEL com.nvidia.tensorrtserver.version=\"${TENSORRT_SERVER_VERSION}\"\nENV LD_LIBRARY_PATH=\"/opt/tensorrtserver/lib:${LD_LIBRARY_PATH}\"\nENV PATH=\"/opt/tensorrtserver/bin:${PATH}\"\nENV TF_ADJUST_HUE_FUSED=\"1\"\nENV TF_ADJUST_SATURATION_FUSED=\"1\"\nENV TF_ENABLE_WINOGRAD_NONFUSED=\"1\"\nENV TF_AUTOTUNE_THRESHOLD=\"2\"\n#   Needed by Caffe2 libraries to avoid:\n#   Intel MKL FATAL ERROR: Cannot load libmkl_intel_thread.so\nENV MKL_THREADING_LAYER=\"GNU\"\n#   Create a user that can be used to run the tensorrt-server as\n#   non-root. Make sure that this user to given ID 1000.\nENV TENSORRT_SERVER_USER=\"tensorrt-server\"\nRUN id -u $TENSORRT_SERVER_USER > /dev/null 2>&1 || useradd $TENSORRT_SERVER_USER \\\n && [ `id -u $TENSORRT_SERVER_USER ` -eq 1000 ] \\\n && [ `id -g $TENSORRT_SERVER_USER ` -eq 1000 ]\n#   libgoogle-glog0v5 is needed by caffe2 libraries.\nRUN apt-get update \\\n && apt-get install --no-install-recommends libgoogle-glog0v5 libre2-1v5 -y\nWORKDIR /opt/tensorrtserver\nRUN rm -fr /opt/tensorrtserver/*\nCOPY LICENSE .\nCOPY --from=trtserver_onnx /workspace/onnxruntime/LICENSE LICENSE.onnxruntime\nCOPY --from=trtserver_tf /opt/tensorflow/LICENSE LICENSE.tensorflow\nCOPY --from=trtserver_caffe2 /opt/pytorch/pytorch/LICENSE LICENSE.pytorch\nCOPY --from=trtserver_build /opt/tensorrtserver/bin/trtserver bin/\nCOPY --from=trtserver_build /opt/tensorrtserver/lib lib\nCOPY --from=trtserver_build /opt/tensorrtserver/include include\nRUN chmod ugo-w+rx /opt/tensorrtserver/lib/*.so\n#   Extra defensive wiring for CUDA Compat lib\nRUN ln -sf ${_CUDA_COMPAT_PATH}/lib.real ${_CUDA_COMPAT_PATH}/lib \\\n && echo ${_CUDA_COMPAT_PATH}/lib > /etc/ld.so.conf.d/00-cuda-compat.conf \\\n && ldconfig \\\n && rm -f ${_CUDA_COMPAT_PATH}/lib\nCOPY nvidia_entrypoint.sh /opt/tensorrtserver\nENTRYPOINT [\"/opt/tensorrtserver/nvidia_entrypoint.sh\"]\nARG NVIDIA_BUILD_ID\nENV NVIDIA_BUILD_ID=\"${NVIDIA_BUILD_ID:-<unknown>}\"\nLABEL com.nvidia.build.id=\"${NVIDIA_BUILD_ID}\"\nARG NVIDIA_BUILD_REF\nLABEL com.nvidia.build.ref=\"${NVIDIA_BUILD_REF}\"\nRUN groupadd --system docker-user ; useradd --system --gid docker-user docker-user\nUSER docker-user\n# Please add your HEALTHCHECK here!!!\n","originalDockerfile":"#  Copyright (c) 2018-2019, NVIDIA CORPORATION. All rights reserved.\n#\n#  Redistribution and use in source and binary forms, with or without\n#  modification, are permitted provided that the following conditions\n#  are met:\n#   * Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   * Redistributions in binary form must reproduce the above copyright\n#     notice, this list of conditions and the following disclaimer in the\n#     documentation and/or other materials provided with the distribution.\n#   * Neither the name of NVIDIA CORPORATION nor the names of its\n#     contributors may be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\n#  EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n#  PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n#  CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n#  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n#  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n#  OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n#  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n#  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n#\n#  Multistage build.\n#\nARG BASE_IMAGE=nvcr.io/nvidia/tensorrtserver:19.05-py3\nARG PYTORCH_IMAGE=nvcr.io/nvidia/pytorch:19.05-py3\nARG TENSORFLOW_IMAGE=nvcr.io/nvidia/tensorflow:19.05-py3\n# ###########################################################################\n# # TensorFlow stage: Use TensorFlow container to build\n# ###########################################################################\nFROM ${TENSORFLOW_IMAGE} AS trtserver_tf\n#  Modify the TF model loader to allow us to set the default GPU for\n#  multi-GPU support\nCOPY tools/patch/tensorflow /tmp/trtis/tools/patch/tensorflow\nRUN sha1sum -c /tmp/trtis/tools/patch/tensorflow/checksums \\\n && patch -i /tmp/trtis/tools/patch/tensorflow/cc/saved_model/loader.cc /opt/tensorflow/tensorflow/cc/saved_model/loader.cc \\\n && patch -i /tmp/trtis/tools/patch/tensorflow/BUILD /opt/tensorflow/tensorflow/BUILD \\\n && patch -i /tmp/trtis/tools/patch/tensorflow/tf_version_script.lds /opt/tensorflow/tensorflow/tf_version_script.lds \\\n && patch -i /tmp/trtis/tools/patch/tensorflow/nvbuild.sh /opt/tensorflow/nvbuild.sh \\\n && patch -i /tmp/trtis/tools/patch/tensorflow/nvbuildopts /opt/tensorflow/nvbuildopts \\\n && patch -i /tmp/trtis/tools/patch/tensorflow/bazel_build.sh /opt/tensorflow/bazel_build.sh\n#  Copy tensorflow_backend_tf into TensorFlow so it builds into the\n#  monolithic libtensorflow_cc library. We want tensorflow_backend_tf\n#  to build against the TensorFlow protobuf since it interfaces with\n#  that code.\nCOPY src/backends/tensorflow/tensorflow_backend_tf.* /opt/tensorflow/tensorflow/\n#  Build TensorFlow library for TRTIS\nWORKDIR /opt/tensorflow\nRUN ./nvbuild.sh --python3.5\n# ###########################################################################\n# # PyTorch stage: Use PyTorch container for Caffe2 and libtorch\n# ###########################################################################\nFROM ${PYTORCH_IMAGE} AS trtserver_caffe2\n#  Copy netdef_backend_c2 into Caffe2 core so it builds into the\n#  libcaffe2 library. We want netdef_backend_c2 to build against the\n#  Caffe2 protobuf since it interfaces with that code.\nCOPY src/backends/caffe2/netdef_backend_c2.* /opt/pytorch/pytorch/caffe2/core/\n#  Build same as in pytorch container... except for the NO_DISTRIBUTED\n#  line where we turn off features not needed for trtserver\n#  This will build both the caffe2 libraries needed by the Caffe2 NetDef backend\n#  and the LibTorch library needed by the PyTorch backend.\nWORKDIR /opt/pytorch\nRUN pip uninstall -y torch\nRUN cd pytorch \\\n && TORCH_CUDA_ARCH_LIST=\"5.2 6.0 6.1 7.0 7.5+PTX\" CMAKE_PREFIX_PATH=\"$( dirname $( which conda ;) ;)/../\" USE_DISTRIBUTED=0 USE_MIOPEN=0 USE_NCCL=0 USE_OPENCV=0 USE_LEVELDB=0 USE_LMDB=0 USE_REDIS=0 BUILD_TEST=0 pip install --no-cache-dir -v .\n# ###########################################################################\n# # Onnx Runtime stage: Build Onnx Runtime on CUDA 10, CUDNN 7\n# ###########################################################################\nFROM ${BASE_IMAGE} AS trtserver_onnx\n#  Currently the prebuilt Onnx Runtime library is built on CUDA 9, thus it\n#  needs to be built from source\n#  Onnx Runtime release version\nARG ONNX_RUNTIME_VERSION=0.4.0\n#  Get release version of Onnx Runtime\nWORKDIR /workspace\nRUN apt-get update \\\n && apt-get install --no-install-recommends git -y\n#  Check out master until new release to support cloud-based filesystems\nRUN git clone --recursive https://github.com/Microsoft/onnxruntime\nENV PATH=\"/opt/cmake/bin:${PATH}\"\nARG SCRIPT_DIR=/workspace/onnxruntime/tools/ci_build/github/linux/docker/scripts\nRUN cp -r ${SCRIPT_DIR} /tmp/scripts \\\n && ${SCRIPT_DIR}/install_ubuntu.sh \\\n && ${SCRIPT_DIR}/install_deps.sh\n#  Allow configure to pick up GDK and CuDNN where it expects it.\n#  (Note: $CUDNN_VERSION is defined by NVidia's base image)\nRUN _CUDNN_VERSION=$( echo $CUDNN_VERSION | cut -d. -f1-2 ;) \\\n && mkdir -p /usr/local/cudnn-$_CUDNN_VERSION/cuda/include \\\n && ln -s /usr/include/cudnn.h /usr/local/cudnn-$_CUDNN_VERSION/cuda/include/cudnn.h \\\n && mkdir -p /usr/local/cudnn-$_CUDNN_VERSION/cuda/lib64 \\\n && ln -s /etc/alternatives/libcudnn_so /usr/local/cudnn-$_CUDNN_VERSION/cuda/lib64/libcudnn.so\n#  Build and Install LLVM\nARG LLVM_VERSION=6.0.1\nRUN cd /tmp \\\n && wget --no-verbose http://releases.llvm.org/$LLVM_VERSION/llvm-$LLVM_VERSION.src.tar.xz \\\n && xz -d llvm-$LLVM_VERSION.src.tar.xz \\\n && tar xvf llvm-$LLVM_VERSION.src.tar \\\n && cd llvm-$LLVM_VERSION.src \\\n && mkdir -p build \\\n && cd build \\\n && cmake .. -DCMAKE_BUILD_TYPE=Release \\\n && cmake --build . -- -j$( nproc ;) \\\n && cmake -DCMAKE_INSTALL_PREFIX=/usr/local/llvm-$LLVM_VERSION -DBUILD_TYPE=Release -P cmake_install.cmake \\\n && cd /tmp \\\n && rm -rf llvm*\nENV LD_LIBRARY_PATH=\"/usr/local/openblas/lib:$LD_LIBRARY_PATH\"\n#  Build files will be in /workspace/build\nARG COMMON_BUILD_ARGS=\"--skip_submodule_sync --parallel --build_shared_lib --use_openmp\"\nRUN mkdir -p /workspace/build\nRUN python3 /workspace/onnxruntime/tools/ci_build/build.py --build_dir /workspace/build --config Release $COMMON_BUILD_ARGS --use_cuda --cuda_home /usr/local/cuda --cudnn_home /usr/local/cudnn-$( echo $CUDNN_VERSION | cut -d. -f1-2 ;)/cuda --update --build\n# ###########################################################################\n# # Build stage: Build inference server\n# ###########################################################################\nFROM ${BASE_IMAGE} AS trtserver_build\nARG TRTIS_VERSION=1.4.0dev\nARG TRTIS_CONTAINER_VERSION=19.07dev\n#  libgoogle-glog0v5 is needed by caffe2 libraries.\nRUN apt-get update \\\n && apt-get install --no-install-recommends software-properties-common autoconf automake build-essential cmake git libgoogle-glog0v5 libre2-dev libssl-dev libtool -y\n#  libcurl4-openSSL-dev is needed for GCS\nRUN if [ $( cat /etc/os-release | grep 'VERSION_ID=\"16.04\"' | wc -l ;) -ne 0 ] ; then apt-get update \\\n && apt-get install --no-install-recommends libcurl3-dev -y ; elif [ $( cat /etc/os-release | grep 'VERSION_ID=\"18.04\"' | wc -l ;) -ne 0 ] ; then apt-get update \\\n && apt-get install --no-install-recommends libcurl4-openssl-dev -y ; else echo \"Ubuntu version must be either 16.04 or 18.04\" \\\n && exit 1 ; fi\n#  TensorFlow libraries. Install the monolithic libtensorflow_cc and\n#  create a link libtensorflow_framework.so -> libtensorflow_cc.so so\n#  that custom tensorflow operations work correctly. Custom TF\n#  operations link against libtensorflow_framework.so so it must be\n#  present (and its functionality is provided by libtensorflow_cc.so).\nCOPY --from=trtserver_tf /usr/local/lib/tensorflow/libtensorflow_cc.so /opt/tensorrtserver/lib/\nRUN cd /opt/tensorrtserver/lib \\\n && ln -s libtensorflow_cc.so libtensorflow_framework.so\n#  Caffe2 libraries\nCOPY --from=trtserver_caffe2 /opt/conda/lib/python3.6/site-packages/torch/lib/libcaffe2_detectron_ops_gpu.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/python3.6/site-packages/torch/lib/libcaffe2.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/python3.6/site-packages/torch/lib/libcaffe2_gpu.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/python3.6/site-packages/torch/lib/libc10.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/python3.6/site-packages/torch/lib/libc10_cuda.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/libmkl_avx2.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/libmkl_core.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/libmkl_def.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/libmkl_gnu_thread.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/libmkl_intel_lp64.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/libmkl_rt.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/libmkl_vml_def.so /opt/tensorrtserver/lib/\n#  LibTorch headers and library\nCOPY --from=trtserver_caffe2 /opt/conda/lib/python3.6/site-packages/torch/include /opt/tensorrtserver/include/torch\nCOPY --from=trtserver_caffe2 /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so.1 /opt/tensorrtserver/lib/\nRUN cd /opt/tensorrtserver/lib \\\n && ln -s libtorch.so.1 libtorch.so\nCOPY --from=trtserver_caffe2 /opt/conda/lib/python3.6/site-packages/torch/lib/libthnvrtc.so /opt/tensorrtserver/lib/\n#  Onnx Runtime headers and library\nARG ONNX_RUNTIME_VERSION=0.4.0\nCOPY --from=trtserver_onnx /workspace/onnxruntime/include/onnxruntime /opt/tensorrtserver/include/onnxruntime/\nCOPY --from=trtserver_onnx /workspace/build/Release/libonnxruntime.so.${ONNX_RUNTIME_VERSION} /opt/tensorrtserver/lib/\nRUN cd /opt/tensorrtserver/lib \\\n && ln -s libonnxruntime.so.${ONNX_RUNTIME_VERSION} libonnxruntime.so\n#  Copy entire repo into container even though some is not needed for\n#  build itself... because we want to be able to copyright check on\n#  files that aren't directly needed for build.\nWORKDIR /workspace\nRUN rm -fr *\nCOPY . .\n#  Build the server.\n#\n#  - Need to find CUDA stubs if they are available since some backends\n#  may need to link against them. This is identical to the login in TF\n#  container nvbuild.sh\n#\n#  - The build can fail the first time due to a protobuf_generate_cpp\n#  error that doesn't repeat on subsequent builds, which is why there\n#  are 2 make invocations below.\nRUN LIBCUDA_FOUND=$( ldconfig -p | grep -v compat | awk '{print $1}' | grep libcuda.so | wc -l ;) \\\n && if [[ \"$LIBCUDA_FOUND\" -eq 0 ]] ; then export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64/stubs ;ln -fs /usr/local/cuda/lib64/stubs/libcuda.so /usr/local/cuda/lib64/stubs/libcuda.so.1 ; fi \\\n && echo $LD_LIBRARY_PATH \\\n && rm -fr builddir \\\n && mkdir -p builddir \\\n && (cd builddir \\\n && cmake -DCMAKE_BUILD_TYPE=Release -DTRTIS_ENABLE_METRICS=ON -DTRTIS_ENABLE_GCS=ON -DTRTIS_ENABLE_CUSTOM=ON -DTRTIS_ENABLE_TENSORFLOW=ON -DTRTIS_ENABLE_TENSORRT=ON -DTRTIS_ENABLE_CAFFE2=ON -DTRTIS_ENABLE_ONNXRUNTIME=ON -DTRTIS_ENABLE_PYTORCH=ON -DTRTIS_ONNXRUNTIME_INCLUDE_PATHS=\"/opt/tensorrtserver/include/onnxruntime\" -DTRTIS_PYTORCH_INCLUDE_PATHS=\"/opt/tensorrtserver/include/torch\" -DTRTIS_EXTRA_LIB_PATHS=\"/opt/tensorrtserver/lib\" ../build \\\n && (make -j16 trtis || true ) \\\n && make -j16 trtis \\\n && mkdir -p /opt/tensorrtserver/include \\\n && cp -r trtis/install/bin /opt/tensorrtserver/. \\\n && cp -r trtis/install/lib /opt/tensorrtserver/. \\\n && cp -r trtis/install/include /opt/tensorrtserver/include/trtserver ) \\\n && (cd /opt/tensorrtserver \\\n && ln -s /workspace/qa qa )\nENV TENSORRT_SERVER_VERSION=\"${TRTIS_VERSION}\"\nENV NVIDIA_TENSORRT_SERVER_VERSION=\"${TRTIS_CONTAINER_VERSION}\"\nENV LD_LIBRARY_PATH=\"/opt/tensorrtserver/lib:${LD_LIBRARY_PATH}\"\nENV PATH=\"/opt/tensorrtserver/bin:${PATH}\"\nCOPY nvidia_entrypoint.sh /opt/tensorrtserver\nENTRYPOINT [\"/opt/tensorrtserver/nvidia_entrypoint.sh\"]\n# ###########################################################################\n# #  Production stage: Create container with just inference server executable\n# ###########################################################################\nFROM ${BASE_IMAGE}\nARG TRTIS_VERSION=1.4.0dev\nARG TRTIS_CONTAINER_VERSION=19.07dev\nENV TENSORRT_SERVER_VERSION=\"${TRTIS_VERSION}\"\nENV NVIDIA_TENSORRT_SERVER_VERSION=\"${TRTIS_CONTAINER_VERSION}\"\nLABEL com.nvidia.tensorrtserver.version=\"${TENSORRT_SERVER_VERSION}\"\nENV LD_LIBRARY_PATH=\"/opt/tensorrtserver/lib:${LD_LIBRARY_PATH}\"\nENV PATH=\"/opt/tensorrtserver/bin:${PATH}\"\nENV TF_ADJUST_HUE_FUSED=\"1\"\nENV TF_ADJUST_SATURATION_FUSED=\"1\"\nENV TF_ENABLE_WINOGRAD_NONFUSED=\"1\"\nENV TF_AUTOTUNE_THRESHOLD=\"2\"\n#  Needed by Caffe2 libraries to avoid:\n#  Intel MKL FATAL ERROR: Cannot load libmkl_intel_thread.so\nENV MKL_THREADING_LAYER=\"GNU\"\n#  Create a user that can be used to run the tensorrt-server as\n#  non-root. Make sure that this user to given ID 1000.\nENV TENSORRT_SERVER_USER=\"tensorrt-server\"\nRUN id -u $TENSORRT_SERVER_USER > /dev/null 2>&1 || useradd $TENSORRT_SERVER_USER \\\n && [ `id -u $TENSORRT_SERVER_USER ` -eq 1000 ] \\\n && [ `id -g $TENSORRT_SERVER_USER ` -eq 1000 ]\n#  libgoogle-glog0v5 is needed by caffe2 libraries.\nRUN apt-get update \\\n && apt-get install --no-install-recommends libgoogle-glog0v5 libre2-1v5 -y\nWORKDIR /opt/tensorrtserver\nRUN rm -fr /opt/tensorrtserver/*\nCOPY LICENSE .\nCOPY --from=trtserver_onnx /workspace/onnxruntime/LICENSE LICENSE.onnxruntime\nCOPY --from=trtserver_tf /opt/tensorflow/LICENSE LICENSE.tensorflow\nCOPY --from=trtserver_caffe2 /opt/pytorch/pytorch/LICENSE LICENSE.pytorch\nCOPY --from=trtserver_build /opt/tensorrtserver/bin/trtserver bin/\nCOPY --from=trtserver_build /opt/tensorrtserver/lib lib\nCOPY --from=trtserver_build /opt/tensorrtserver/include include\nRUN chmod ugo-w+rx /opt/tensorrtserver/lib/*.so\n#  Extra defensive wiring for CUDA Compat lib\nRUN ln -sf ${_CUDA_COMPAT_PATH}/lib.real ${_CUDA_COMPAT_PATH}/lib \\\n && echo ${_CUDA_COMPAT_PATH}/lib > /etc/ld.so.conf.d/00-cuda-compat.conf \\\n && ldconfig \\\n && rm -f ${_CUDA_COMPAT_PATH}/lib\nCOPY nvidia_entrypoint.sh /opt/tensorrtserver\nENTRYPOINT [\"/opt/tensorrtserver/nvidia_entrypoint.sh\"]\nARG NVIDIA_BUILD_ID\nENV NVIDIA_BUILD_ID=\"${NVIDIA_BUILD_ID:-<unknown>}\"\nLABEL com.nvidia.build.id=\"${NVIDIA_BUILD_ID}\"\nARG NVIDIA_BUILD_REF\nLABEL com.nvidia.build.ref=\"${NVIDIA_BUILD_REF}\"\n","injectedSmells":[],"originalDockerfileHash":"4fd64fc4afb5f4d9012d77b9b9a81489","successfullyInjectedSmells":[],"originalDockerfileUglified":"#   Copyright (c) 2018-2019, NVIDIA CORPORATION. All rights reserved.\n#\n#   Redistribution and use in source and binary forms, with or without\n#   modification, are permitted provided that the following conditions\n#   are met:\n#    * Redistributions of source code must retain the above copyright\n#      notice, this list of conditions and the following disclaimer.\n#    * Redistributions in binary form must reproduce the above copyright\n#      notice, this list of conditions and the following disclaimer in the\n#      documentation and/or other materials provided with the distribution.\n#    * Neither the name of NVIDIA CORPORATION nor the names of its\n#      contributors may be used to endorse or promote products derived\n#      from this software without specific prior written permission.\n#\n#   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\n#   EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n#   IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n#   PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n#   CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n#   EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n#   PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n#   PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n#   OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n#   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n#   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n#\n#   Multistage build.\n#\nARG BASE_IMAGE=nvcr.io/nvidia/tensorrtserver:19.05-py3\nARG PYTORCH_IMAGE=nvcr.io/nvidia/pytorch:19.05-py3\nARG TENSORFLOW_IMAGE=nvcr.io/nvidia/tensorflow:19.05-py3\n#  ###########################################################################\n#  # TensorFlow stage: Use TensorFlow container to build\n#  ###########################################################################\nFROM ${TENSORFLOW_IMAGE} AS trtserver_tf\n#   Modify the TF model loader to allow us to set the default GPU for\n#   multi-GPU support\nCOPY tools/patch/tensorflow /tmp/trtis/tools/patch/tensorflow\nRUN sha1sum -c /tmp/trtis/tools/patch/tensorflow/checksums \\\n && patch -i /tmp/trtis/tools/patch/tensorflow/cc/saved_model/loader.cc /opt/tensorflow/tensorflow/cc/saved_model/loader.cc \\\n && patch -i /tmp/trtis/tools/patch/tensorflow/BUILD /opt/tensorflow/tensorflow/BUILD \\\n && patch -i /tmp/trtis/tools/patch/tensorflow/tf_version_script.lds /opt/tensorflow/tensorflow/tf_version_script.lds \\\n && patch -i /tmp/trtis/tools/patch/tensorflow/nvbuild.sh /opt/tensorflow/nvbuild.sh \\\n && patch -i /tmp/trtis/tools/patch/tensorflow/nvbuildopts /opt/tensorflow/nvbuildopts \\\n && patch -i /tmp/trtis/tools/patch/tensorflow/bazel_build.sh /opt/tensorflow/bazel_build.sh\n#   Copy tensorflow_backend_tf into TensorFlow so it builds into the\n#   monolithic libtensorflow_cc library. We want tensorflow_backend_tf\n#   to build against the TensorFlow protobuf since it interfaces with\n#   that code.\nCOPY src/backends/tensorflow/tensorflow_backend_tf.* /opt/tensorflow/tensorflow/\n#   Build TensorFlow library for TRTIS\nWORKDIR /opt/tensorflow\nRUN ./nvbuild.sh --python3.5\n#  ###########################################################################\n#  # PyTorch stage: Use PyTorch container for Caffe2 and libtorch\n#  ###########################################################################\nFROM ${PYTORCH_IMAGE} AS trtserver_caffe2\n#   Copy netdef_backend_c2 into Caffe2 core so it builds into the\n#   libcaffe2 library. We want netdef_backend_c2 to build against the\n#   Caffe2 protobuf since it interfaces with that code.\nCOPY src/backends/caffe2/netdef_backend_c2.* /opt/pytorch/pytorch/caffe2/core/\n#   Build same as in pytorch container... except for the NO_DISTRIBUTED\n#   line where we turn off features not needed for trtserver\n#   This will build both the caffe2 libraries needed by the Caffe2 NetDef backend\n#   and the LibTorch library needed by the PyTorch backend.\nWORKDIR /opt/pytorch\nRUN pip uninstall -y torch\nRUN cd pytorch \\\n && TORCH_CUDA_ARCH_LIST=\"5.2 6.0 6.1 7.0 7.5+PTX\" CMAKE_PREFIX_PATH=\"$( dirname $( which conda ;) ;)/../\" USE_DISTRIBUTED=0 USE_MIOPEN=0 USE_NCCL=0 USE_OPENCV=0 USE_LEVELDB=0 USE_LMDB=0 USE_REDIS=0 BUILD_TEST=0 pip install --no-cache-dir -v .\n#  ###########################################################################\n#  # Onnx Runtime stage: Build Onnx Runtime on CUDA 10, CUDNN 7\n#  ###########################################################################\nFROM ${BASE_IMAGE} AS trtserver_onnx\n#   Currently the prebuilt Onnx Runtime library is built on CUDA 9, thus it\n#   needs to be built from source\n#   Onnx Runtime release version\nARG ONNX_RUNTIME_VERSION=0.4.0\n#   Get release version of Onnx Runtime\nWORKDIR /workspace\nRUN apt-get update \\\n && apt-get install --no-install-recommends git -y\n#   Check out master until new release to support cloud-based filesystems\nRUN git clone --recursive https://github.com/Microsoft/onnxruntime\nENV PATH=\"/opt/cmake/bin:${PATH}\"\nARG SCRIPT_DIR=/workspace/onnxruntime/tools/ci_build/github/linux/docker/scripts\nRUN cp -r ${SCRIPT_DIR} /tmp/scripts \\\n && ${SCRIPT_DIR}/install_ubuntu.sh \\\n && ${SCRIPT_DIR}/install_deps.sh\n#   Allow configure to pick up GDK and CuDNN where it expects it.\n#   (Note: $CUDNN_VERSION is defined by NVidia's base image)\nRUN _CUDNN_VERSION=$( echo $CUDNN_VERSION | cut -d. -f1-2 ;) \\\n && mkdir -p /usr/local/cudnn-$_CUDNN_VERSION/cuda/include \\\n && ln -s /usr/include/cudnn.h /usr/local/cudnn-$_CUDNN_VERSION/cuda/include/cudnn.h \\\n && mkdir -p /usr/local/cudnn-$_CUDNN_VERSION/cuda/lib64 \\\n && ln -s /etc/alternatives/libcudnn_so /usr/local/cudnn-$_CUDNN_VERSION/cuda/lib64/libcudnn.so\n#   Build and Install LLVM\nARG LLVM_VERSION=6.0.1\nRUN cd /tmp \\\n && wget --no-verbose http://releases.llvm.org/$LLVM_VERSION/llvm-$LLVM_VERSION.src.tar.xz \\\n && xz -d llvm-$LLVM_VERSION.src.tar.xz \\\n && tar xvf llvm-$LLVM_VERSION.src.tar \\\n && cd llvm-$LLVM_VERSION.src \\\n && mkdir -p build \\\n && cd build \\\n && cmake .. -DCMAKE_BUILD_TYPE=Release \\\n && cmake --build . -- -j$( nproc ;) \\\n && cmake -DCMAKE_INSTALL_PREFIX=/usr/local/llvm-$LLVM_VERSION -DBUILD_TYPE=Release -P cmake_install.cmake \\\n && cd /tmp \\\n && rm -rf llvm*\nENV LD_LIBRARY_PATH=\"/usr/local/openblas/lib:$LD_LIBRARY_PATH\"\n#   Build files will be in /workspace/build\nARG COMMON_BUILD_ARGS=\"--skip_submodule_sync --parallel --build_shared_lib --use_openmp\"\nRUN mkdir -p /workspace/build\nRUN python3 /workspace/onnxruntime/tools/ci_build/build.py --build_dir /workspace/build --config Release $COMMON_BUILD_ARGS --use_cuda --cuda_home /usr/local/cuda --cudnn_home /usr/local/cudnn-$( echo $CUDNN_VERSION | cut -d. -f1-2 ;)/cuda --update --build\n#  ###########################################################################\n#  # Build stage: Build inference server\n#  ###########################################################################\nFROM ${BASE_IMAGE} AS trtserver_build\nARG TRTIS_VERSION=1.4.0dev\nARG TRTIS_CONTAINER_VERSION=19.07dev\n#   libgoogle-glog0v5 is needed by caffe2 libraries.\nRUN apt-get update \\\n && apt-get install --no-install-recommends software-properties-common autoconf automake build-essential cmake git libgoogle-glog0v5 libre2-dev libssl-dev libtool -y\n#   libcurl4-openSSL-dev is needed for GCS\nRUN if [ $( cat /etc/os-release | grep 'VERSION_ID=\"16.04\"' | wc -l ;) -ne 0 ] ; then apt-get update \\\n && apt-get install --no-install-recommends libcurl3-dev -y ; elif [ $( cat /etc/os-release | grep 'VERSION_ID=\"18.04\"' | wc -l ;) -ne 0 ] ; then apt-get update \\\n && apt-get install --no-install-recommends libcurl4-openssl-dev -y ; else echo \"Ubuntu version must be either 16.04 or 18.04\" \\\n && exit 1 ; fi\n#   TensorFlow libraries. Install the monolithic libtensorflow_cc and\n#   create a link libtensorflow_framework.so -> libtensorflow_cc.so so\n#   that custom tensorflow operations work correctly. Custom TF\n#   operations link against libtensorflow_framework.so so it must be\n#   present (and its functionality is provided by libtensorflow_cc.so).\nCOPY --from=trtserver_tf /usr/local/lib/tensorflow/libtensorflow_cc.so /opt/tensorrtserver/lib/\nRUN cd /opt/tensorrtserver/lib \\\n && ln -s libtensorflow_cc.so libtensorflow_framework.so\n#   Caffe2 libraries\nCOPY --from=trtserver_caffe2 /opt/conda/lib/python3.6/site-packages/torch/lib/libcaffe2_detectron_ops_gpu.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/python3.6/site-packages/torch/lib/libcaffe2.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/python3.6/site-packages/torch/lib/libcaffe2_gpu.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/python3.6/site-packages/torch/lib/libc10.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/python3.6/site-packages/torch/lib/libc10_cuda.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/libmkl_avx2.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/libmkl_core.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/libmkl_def.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/libmkl_gnu_thread.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/libmkl_intel_lp64.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/libmkl_rt.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/libmkl_vml_def.so /opt/tensorrtserver/lib/\n#   LibTorch headers and library\nCOPY --from=trtserver_caffe2 /opt/conda/lib/python3.6/site-packages/torch/include /opt/tensorrtserver/include/torch\nCOPY --from=trtserver_caffe2 /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so.1 /opt/tensorrtserver/lib/\nRUN cd /opt/tensorrtserver/lib \\\n && ln -s libtorch.so.1 libtorch.so\nCOPY --from=trtserver_caffe2 /opt/conda/lib/python3.6/site-packages/torch/lib/libthnvrtc.so /opt/tensorrtserver/lib/\n#   Onnx Runtime headers and library\nARG ONNX_RUNTIME_VERSION=0.4.0\nCOPY --from=trtserver_onnx /workspace/onnxruntime/include/onnxruntime /opt/tensorrtserver/include/onnxruntime/\nCOPY --from=trtserver_onnx /workspace/build/Release/libonnxruntime.so.${ONNX_RUNTIME_VERSION} /opt/tensorrtserver/lib/\nRUN cd /opt/tensorrtserver/lib \\\n && ln -s libonnxruntime.so.${ONNX_RUNTIME_VERSION} libonnxruntime.so\n#   Copy entire repo into container even though some is not needed for\n#   build itself... because we want to be able to copyright check on\n#   files that aren't directly needed for build.\nWORKDIR /workspace\nRUN rm -fr *\nCOPY . .\n#   Build the server.\n#\n#   - Need to find CUDA stubs if they are available since some backends\n#   may need to link against them. This is identical to the login in TF\n#   container nvbuild.sh\n#\n#   - The build can fail the first time due to a protobuf_generate_cpp\n#   error that doesn't repeat on subsequent builds, which is why there\n#   are 2 make invocations below.\nRUN LIBCUDA_FOUND=$( ldconfig -p | grep -v compat | awk '{print $1}' | grep libcuda.so | wc -l ;) \\\n && if [[ \"$LIBCUDA_FOUND\" -eq 0 ]] ; then export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64/stubs ;ln -fs /usr/local/cuda/lib64/stubs/libcuda.so /usr/local/cuda/lib64/stubs/libcuda.so.1 ; fi \\\n && echo $LD_LIBRARY_PATH \\\n && rm -fr builddir \\\n && mkdir -p builddir \\\n && (cd builddir \\\n && cmake -DCMAKE_BUILD_TYPE=Release -DTRTIS_ENABLE_METRICS=ON -DTRTIS_ENABLE_GCS=ON -DTRTIS_ENABLE_CUSTOM=ON -DTRTIS_ENABLE_TENSORFLOW=ON -DTRTIS_ENABLE_TENSORRT=ON -DTRTIS_ENABLE_CAFFE2=ON -DTRTIS_ENABLE_ONNXRUNTIME=ON -DTRTIS_ENABLE_PYTORCH=ON -DTRTIS_ONNXRUNTIME_INCLUDE_PATHS=\"/opt/tensorrtserver/include/onnxruntime\" -DTRTIS_PYTORCH_INCLUDE_PATHS=\"/opt/tensorrtserver/include/torch\" -DTRTIS_EXTRA_LIB_PATHS=\"/opt/tensorrtserver/lib\" ../build \\\n && (make -j16 trtis || true ) \\\n && make -j16 trtis \\\n && mkdir -p /opt/tensorrtserver/include \\\n && cp -r trtis/install/bin /opt/tensorrtserver/. \\\n && cp -r trtis/install/lib /opt/tensorrtserver/. \\\n && cp -r trtis/install/include /opt/tensorrtserver/include/trtserver ) \\\n && (cd /opt/tensorrtserver \\\n && ln -s /workspace/qa qa )\nENV TENSORRT_SERVER_VERSION=\"${TRTIS_VERSION}\"\nENV NVIDIA_TENSORRT_SERVER_VERSION=\"${TRTIS_CONTAINER_VERSION}\"\nENV LD_LIBRARY_PATH=\"/opt/tensorrtserver/lib:${LD_LIBRARY_PATH}\"\nENV PATH=\"/opt/tensorrtserver/bin:${PATH}\"\nCOPY nvidia_entrypoint.sh /opt/tensorrtserver\nENTRYPOINT [\"/opt/tensorrtserver/nvidia_entrypoint.sh\"]\n#  ###########################################################################\n#  #  Production stage: Create container with just inference server executable\n#  ###########################################################################\nFROM ${BASE_IMAGE}\nARG TRTIS_VERSION=1.4.0dev\nARG TRTIS_CONTAINER_VERSION=19.07dev\nENV TENSORRT_SERVER_VERSION=\"${TRTIS_VERSION}\"\nENV NVIDIA_TENSORRT_SERVER_VERSION=\"${TRTIS_CONTAINER_VERSION}\"\nLABEL com.nvidia.tensorrtserver.version=\"${TENSORRT_SERVER_VERSION}\"\nENV LD_LIBRARY_PATH=\"/opt/tensorrtserver/lib:${LD_LIBRARY_PATH}\"\nENV PATH=\"/opt/tensorrtserver/bin:${PATH}\"\nENV TF_ADJUST_HUE_FUSED=\"1\"\nENV TF_ADJUST_SATURATION_FUSED=\"1\"\nENV TF_ENABLE_WINOGRAD_NONFUSED=\"1\"\nENV TF_AUTOTUNE_THRESHOLD=\"2\"\n#   Needed by Caffe2 libraries to avoid:\n#   Intel MKL FATAL ERROR: Cannot load libmkl_intel_thread.so\nENV MKL_THREADING_LAYER=\"GNU\"\n#   Create a user that can be used to run the tensorrt-server as\n#   non-root. Make sure that this user to given ID 1000.\nENV TENSORRT_SERVER_USER=\"tensorrt-server\"\nRUN id -u $TENSORRT_SERVER_USER > /dev/null 2>&1 || useradd $TENSORRT_SERVER_USER \\\n && [ `id -u $TENSORRT_SERVER_USER ` -eq 1000 ] \\\n && [ `id -g $TENSORRT_SERVER_USER ` -eq 1000 ]\n#   libgoogle-glog0v5 is needed by caffe2 libraries.\nRUN apt-get update \\\n && apt-get install --no-install-recommends libgoogle-glog0v5 libre2-1v5 -y\nWORKDIR /opt/tensorrtserver\nRUN rm -fr /opt/tensorrtserver/*\nCOPY LICENSE .\nCOPY --from=trtserver_onnx /workspace/onnxruntime/LICENSE LICENSE.onnxruntime\nCOPY --from=trtserver_tf /opt/tensorflow/LICENSE LICENSE.tensorflow\nCOPY --from=trtserver_caffe2 /opt/pytorch/pytorch/LICENSE LICENSE.pytorch\nCOPY --from=trtserver_build /opt/tensorrtserver/bin/trtserver bin/\nCOPY --from=trtserver_build /opt/tensorrtserver/lib lib\nCOPY --from=trtserver_build /opt/tensorrtserver/include include\nRUN chmod ugo-w+rx /opt/tensorrtserver/lib/*.so\n#   Extra defensive wiring for CUDA Compat lib\nRUN ln -sf ${_CUDA_COMPAT_PATH}/lib.real ${_CUDA_COMPAT_PATH}/lib \\\n && echo ${_CUDA_COMPAT_PATH}/lib > /etc/ld.so.conf.d/00-cuda-compat.conf \\\n && ldconfig \\\n && rm -f ${_CUDA_COMPAT_PATH}/lib\nCOPY nvidia_entrypoint.sh /opt/tensorrtserver\nENTRYPOINT [\"/opt/tensorrtserver/nvidia_entrypoint.sh\"]\nARG NVIDIA_BUILD_ID\nENV NVIDIA_BUILD_ID=\"${NVIDIA_BUILD_ID:-<unknown>}\"\nLABEL com.nvidia.build.id=\"${NVIDIA_BUILD_ID}\"\nARG NVIDIA_BUILD_REF\nLABEL com.nvidia.build.ref=\"${NVIDIA_BUILD_REF}\"\n","originalDockerfileUglifiedHash":"05b1ae87c61f556fe1e8524cefa0b9b6","fileName":"/ICSME-replicationpackage/dataset/smelly_dockerfiles_bianncle/390278858251636bbc6696731667256181858221.dockerfile"}