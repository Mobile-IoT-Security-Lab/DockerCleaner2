{"seed":2857282692,"processedDockerfileHash":"b7999164353462aaf54f32a80a7fdc62","fixedSmells":["use-no-install-recommends","do-not-use-apt-get-update-alone","pin-package-manager-versions-apt-get","pin-package-manager-versions-pip","pin-package-manager-versions-npm","pin-package-manager-versions-gem","pin-package-manager-versions-apk","use-copy-instead-of-add","use-wget-instead-of-add","do-not-have-secrets","have-a-healthcheck","have-a-user"],"successfullyFixedSmells":["have-a-healthcheck","have-a-user"],"processedDockerfile":"#   0. The barest Python: used in dev and prod\nFROM python:3.7.2-slim-stretch AS pybase\n#   We probably don't want these, long-term.\n#   nano: because we edit files on production\n#   postgresql-client: because we poll the DB:\n#   * on prod before ./manage.py migrate\n#   * on unittest before ./manage.py test\n#   git: used for importmodulefromgithub\n#   curl: handy for testing, NLTK download, Watchman download; not worth uninstalling each time\n#   unzip: to build Watchman ... and [adamhooper, 2019-02-21] I'm afraid\n#          to uninstall it after build, in case one of our Python deps shells to it\nRUN mkdir -p /usr/share/man/man1 /usr/share/man/man7 \\\n && apt-get update \\\n && apt-get install --no-install-recommends git nano postgresql-client unzip curl -y \\\n && rm -rf /var/lib/apt/lists/*\n#   Download NLTK stuff\n#\n#   NLTK expects its data to stay zipped\nRUN mkdir -p /usr/share/nltk_data \\\n && cd /usr/share/nltk_data \\\n && mkdir -p sentiment corpora \\\n && curl https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/stopwords.zip > corpora/stopwords.zip \\\n && curl https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/sentiment/vader_lexicon.zip > sentiment/vader_lexicon.zip\nRUN pip install pipenv==2018.11.26\n#   Set up /app\nRUN mkdir /app\nWORKDIR /app\n#   0.1 Pydev: just for the development environment\nFROM pybase AS pydev\n#   Need build-essential for:\n#   * regex (TODO nix the dep or make it support manylinux .whl)\n#   * Twisted - https://twistedmatrix.com/trac/ticket/7945\n#   * fastparquet\n#   * python-snappy\n#   * yajl-py\n#   * fb-re2\n#   * watchman (until someone packages binaries)\n#   * pysycopg2 (binaries are evil because psycopg2 links SSL -- as does Python)\n#\n#   Also:\n#   * socat: for our dev environment: fetcher uses http://localhost:8000 for in-lesson files\nRUN mkdir -p /root/.local/share/virtualenvs \\\n && apt-get update \\\n && apt-get install --no-install-recommends build-essential libsnappy-dev libre2-dev libpq-dev libyajl-dev socat -y \\\n && rm -rf /var/lib/apt/lists/*\n#   build watchman. Someday let's hope someone publishes binaries\n#   https://facebook.github.io/watchman/docs/install.html\nRUN cd /tmp \\\n && curl -L https://github.com/facebook/watchman/archive/v4.9.0.zip > watchman-4.9.0.zip \\\n && unzip watchman-4.9.0.zip \\\n && cd watchman-4.9.0 \\\n && apt-get update \\\n && apt-get install --no-install-recommends libssl-dev autoconf automake libtool libpcre3-dev pkg-config -y \\\n && ./autogen.sh \\\n && ./configure --prefix=/usr \\\n && make -j4 \\\n && make install \\\n && cd /tmp \\\n && rm -rf /tmp/watchman-4.9.0* \\\n && apt-get remove --purge -y libssl-dev autoconf automake libtool libpcre3-dev pkg-config \\\n && apt-get autoremove --purge -y \\\n && rm -rf /var/lib/apt/lists/*\n#   Add a Python wrapper that will help PyCharm cooperate with pipenv\n#   See https://blog.jetbrains.com/pycharm/2015/12/using-docker-in-pycharm/ for\n#   PyCharm's expectations. Just set \"Python interpreter path\" to\n#   \"pipenv-run-python\" to ensure:\n#\n#   * `cd /app`: PyCharm mounts the source tree to `/opt/project` and overwrites\n#                the current working directory. We force `cd /app` to restore\n#                what the Dockerfile already specifies. That's important because\n#                `pipenv` looks for packages in a virtualenv named after the\n#                current working directory.\n#\n#   * `exec pipenv run python \"$@\"`: PyCharm does not let us specify a command\n#                                    for Docker to run. It only lets us specify\n#                                    \"Python interpreter path.\" This wrapper will\n#                                    provide the interface PyCharm expects, with\n#                                    the environment variables Python needs to\n#                                    find our virtualenv.\n#\n#   Why do we create the file with RUN instead of COPY? Because even in 2018,\n#   COPY does not copy the executable bit on Windows, so we need a RUN anyway\n#   to make it executable.\nRUN true \\\n && echo '#!/bin/sh\\ncd /app\\nexec pipenv run python \"$@\"' > /usr/bin/pipenv-run-python \\\n && chmod +x /usr/bin/pipenv-run-python\n#   1. Python deps -- which rarely change, so this part of the Dockerfile will be\n#   cached (when building locally)\nFROM pybase AS pybuild\n#   Install Python dependencies. They rarely change.\n#   For Docker images we install them to the local system, not to a virtualenv.\n#   Containers don't use pipenv.\nCOPY Pipfile Pipfile.lock /app/\n#   Need build-essential for:\n#   * regex (TODO nix the dep or make it support manylinux .whl)\n#   * Twisted - https://twistedmatrix.com/trac/ticket/7945\n#   * fastparquet\n#   * python-snappy\n#   * yajl-py\n#   * pysycopg2 (binaries are evil because psycopg2 links SSL -- as does Python)\n#   ... and we want to keep libsnappy and yajl around after the fact, too\nRUN true \\\n && apt-get update \\\n && apt-get install --no-install-recommends build-essential libsnappy1v5 libsnappy-dev libre2-3 libre2-dev libpq-dev libyajl2 libyajl-dev -y \\\n && pipenv install --dev --system --deploy \\\n && apt-get remove --purge -y build-essential libsnappy-dev libre2-dev libpq-dev \\\n && apt-get autoremove --purge -y \\\n && rm -rf /var/lib/apt/lists/*\n#   2. Node deps -- completely independent\n#   2.1 jsbase: what we use in dev-in-docker\nFROM node:11.14.0-slim AS jsbase\nRUN mkdir /app\nWORKDIR /app\n#   2.2 jsbuild: where we build JavaScript assets\nFROM jsbase AS jsbuild\nCOPY package.json package-lock.json /app/\nRUN npm install\nCOPY webpack.config.js setupJest.js /app/\nCOPY __mocks__/ /app/__mocks__/\nCOPY assets/ /app/assets/\n#   Inject unit tests into our continuous integration\n#   This catches mistakes that would otherwise foil us in bin/integration-test;\n#   and currently we rely on this line in our CI scripts (cloudbuild.yaml).\nRUN npm test\nRUN node_modules/.bin/webpack -p\n#   3. Three prod servers will all be based on the same stuff:\nFROM pybuild AS base\nCOPY cjworkbench/ /app/cjworkbench/\n#   TODO make server/ frontend-specific\nCOPY server/ /app/server/\n#   cron/, fetcher/ and renderer/ are referenced in settings.py, so they must be\n#   in all Django apps. TODO make them _not_ Django apps. (change ORM)\nCOPY cron/ /app/cron/\nCOPY fetcher/ /app/fetcher/\nCOPY renderer/ /app/renderer/\nCOPY bin/ /app/bin/\nCOPY manage.py /app/\n#   templates are used in renderer for notifications emails and in frontend for\n#   views. TODO move renderer templates elsewhere.\nCOPY templates/ /app/templates/\n#   3.1. migrate: runs ./manage.py migrate\nFROM base AS migrate\n#   assets/ is static files. migrate will upload them to minio.\nCOPY assets/ /app/assets/\nCOPY --from=jsbuild /app/assets/bundles/ /app/assets/bundles/\nCMD [\"bin/migrate-prod\"]\n#   3.2. fetcher: runs fetch\nFROM base AS fetcher\nCMD [\"./manage.py\", \"fetcher\"]\n#   3.3. fetcher: runs fetch\nFROM base AS renderer\nCMD [\"./manage.py\", \"renderer\"]\n#   3.4. cron: schedules fetches and runs cleanup SQL\nFROM base AS cron\nCMD [\"./manage.py\", \"cron\"]\n#   3.5. frontend: serves website\nFROM base AS frontend\nCOPY --from=jsbuild /app/webpack-stats.json /app/\n#   8080 is Kubernetes' conventional web-server port\nEXPOSE 8080/tcp\n#   TODO serve static files elsewhere\n#   Beware: our daphne does not serve static files! Use migrate-prod to push them\n#   to GCS and publish them there.\n#\n#   We set application-close-timeout to something enormous. Otherwise, Daphne will\n#   call `task.cancel()` on a long-running, closed connection. That's\n#   catastrophic: if the Websocket connection is subscribed to a group, then the\n#   group's messages will queue up until they fill our channel layer -- causing\n#   back-pressure, meaning _all Websocket connections stop working_. (At the same\n#   time, if the application never dies at all we have another error. So keep\n#   --application-close-timeout small enough that we'll get a warning when there's\n#   a bug in our code.)\nCMD [\"daphne\", \"-b\", \"0.0.0.0\", \"-p\", \"8080\", \"--application-close-timeout\", \"180\", \"cjworkbench.asgi:application\"]\nRUN groupadd --system docker-user ; useradd --system --gid docker-user docker-user\nUSER docker-user\n# Please add your HEALTHCHECK here!!!\n","originalDockerfile":"#  0. The barest Python: used in dev and prod\nFROM python:3.7.2-slim-stretch AS pybase\n#  We probably don't want these, long-term.\n#  nano: because we edit files on production\n#  postgresql-client: because we poll the DB:\n#  * on prod before ./manage.py migrate\n#  * on unittest before ./manage.py test\n#  git: used for importmodulefromgithub\n#  curl: handy for testing, NLTK download, Watchman download; not worth uninstalling each time\n#  unzip: to build Watchman ... and [adamhooper, 2019-02-21] I'm afraid\n#         to uninstall it after build, in case one of our Python deps shells to it\nRUN mkdir -p /usr/share/man/man1 /usr/share/man/man7 \\\n && apt-get update \\\n && apt-get install --no-install-recommends git nano postgresql-client unzip curl -y \\\n && rm -rf /var/lib/apt/lists/*\n#  Download NLTK stuff\n#\n#  NLTK expects its data to stay zipped\nRUN mkdir -p /usr/share/nltk_data \\\n && cd /usr/share/nltk_data \\\n && mkdir -p sentiment corpora \\\n && curl https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/stopwords.zip > corpora/stopwords.zip \\\n && curl https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/sentiment/vader_lexicon.zip > sentiment/vader_lexicon.zip\nRUN pip install pipenv==2018.11.26\n#  Set up /app\nRUN mkdir /app\nWORKDIR /app\n#  0.1 Pydev: just for the development environment\nFROM pybase AS pydev\n#  Need build-essential for:\n#  * regex (TODO nix the dep or make it support manylinux .whl)\n#  * Twisted - https://twistedmatrix.com/trac/ticket/7945\n#  * fastparquet\n#  * python-snappy\n#  * yajl-py\n#  * fb-re2\n#  * watchman (until someone packages binaries)\n#  * pysycopg2 (binaries are evil because psycopg2 links SSL -- as does Python)\n#\n#  Also:\n#  * socat: for our dev environment: fetcher uses http://localhost:8000 for in-lesson files\nRUN mkdir -p /root/.local/share/virtualenvs \\\n && apt-get update \\\n && apt-get install --no-install-recommends build-essential libsnappy-dev libre2-dev libpq-dev libyajl-dev socat -y \\\n && rm -rf /var/lib/apt/lists/*\n#  build watchman. Someday let's hope someone publishes binaries\n#  https://facebook.github.io/watchman/docs/install.html\nRUN cd /tmp \\\n && curl -L https://github.com/facebook/watchman/archive/v4.9.0.zip > watchman-4.9.0.zip \\\n && unzip watchman-4.9.0.zip \\\n && cd watchman-4.9.0 \\\n && apt-get update \\\n && apt-get install --no-install-recommends libssl-dev autoconf automake libtool libpcre3-dev pkg-config -y \\\n && ./autogen.sh \\\n && ./configure --prefix=/usr \\\n && make -j4 \\\n && make install \\\n && cd /tmp \\\n && rm -rf /tmp/watchman-4.9.0* \\\n && apt-get remove --purge -y libssl-dev autoconf automake libtool libpcre3-dev pkg-config \\\n && apt-get autoremove --purge -y \\\n && rm -rf /var/lib/apt/lists/*\n#  Add a Python wrapper that will help PyCharm cooperate with pipenv\n#  See https://blog.jetbrains.com/pycharm/2015/12/using-docker-in-pycharm/ for\n#  PyCharm's expectations. Just set \"Python interpreter path\" to\n#  \"pipenv-run-python\" to ensure:\n#\n#  * `cd /app`: PyCharm mounts the source tree to `/opt/project` and overwrites\n#               the current working directory. We force `cd /app` to restore\n#               what the Dockerfile already specifies. That's important because\n#               `pipenv` looks for packages in a virtualenv named after the\n#               current working directory.\n#\n#  * `exec pipenv run python \"$@\"`: PyCharm does not let us specify a command\n#                                   for Docker to run. It only lets us specify\n#                                   \"Python interpreter path.\" This wrapper will\n#                                   provide the interface PyCharm expects, with\n#                                   the environment variables Python needs to\n#                                   find our virtualenv.\n#\n#  Why do we create the file with RUN instead of COPY? Because even in 2018,\n#  COPY does not copy the executable bit on Windows, so we need a RUN anyway\n#  to make it executable.\nRUN true \\\n && echo '#!/bin/sh\\ncd /app\\nexec pipenv run python \"$@\"' > /usr/bin/pipenv-run-python \\\n && chmod +x /usr/bin/pipenv-run-python\n#  1. Python deps -- which rarely change, so this part of the Dockerfile will be\n#  cached (when building locally)\nFROM pybase AS pybuild\n#  Install Python dependencies. They rarely change.\n#  For Docker images we install them to the local system, not to a virtualenv.\n#  Containers don't use pipenv.\nCOPY Pipfile Pipfile.lock /app/\n#  Need build-essential for:\n#  * regex (TODO nix the dep or make it support manylinux .whl)\n#  * Twisted - https://twistedmatrix.com/trac/ticket/7945\n#  * fastparquet\n#  * python-snappy\n#  * yajl-py\n#  * pysycopg2 (binaries are evil because psycopg2 links SSL -- as does Python)\n#  ... and we want to keep libsnappy and yajl around after the fact, too\nRUN true \\\n && apt-get update \\\n && apt-get install --no-install-recommends build-essential libsnappy1v5 libsnappy-dev libre2-3 libre2-dev libpq-dev libyajl2 libyajl-dev -y \\\n && pipenv install --dev --system --deploy \\\n && apt-get remove --purge -y build-essential libsnappy-dev libre2-dev libpq-dev \\\n && apt-get autoremove --purge -y \\\n && rm -rf /var/lib/apt/lists/*\n#  2. Node deps -- completely independent\n#  2.1 jsbase: what we use in dev-in-docker\nFROM node:11.14.0-slim AS jsbase\nRUN mkdir /app\nWORKDIR /app\n#  2.2 jsbuild: where we build JavaScript assets\nFROM jsbase AS jsbuild\nCOPY package.json package-lock.json /app/\nRUN npm install\nCOPY webpack.config.js setupJest.js /app/\nCOPY __mocks__/ /app/__mocks__/\nCOPY assets/ /app/assets/\n#  Inject unit tests into our continuous integration\n#  This catches mistakes that would otherwise foil us in bin/integration-test;\n#  and currently we rely on this line in our CI scripts (cloudbuild.yaml).\nRUN npm test\nRUN node_modules/.bin/webpack -p\n#  3. Three prod servers will all be based on the same stuff:\nFROM pybuild AS base\nCOPY cjworkbench/ /app/cjworkbench/\n#  TODO make server/ frontend-specific\nCOPY server/ /app/server/\n#  cron/, fetcher/ and renderer/ are referenced in settings.py, so they must be\n#  in all Django apps. TODO make them _not_ Django apps. (change ORM)\nCOPY cron/ /app/cron/\nCOPY fetcher/ /app/fetcher/\nCOPY renderer/ /app/renderer/\nCOPY bin/ /app/bin/\nCOPY manage.py /app/\n#  templates are used in renderer for notifications emails and in frontend for\n#  views. TODO move renderer templates elsewhere.\nCOPY templates/ /app/templates/\n#  3.1. migrate: runs ./manage.py migrate\nFROM base AS migrate\n#  assets/ is static files. migrate will upload them to minio.\nCOPY assets/ /app/assets/\nCOPY --from=jsbuild /app/assets/bundles/ /app/assets/bundles/\nCMD [\"bin/migrate-prod\"]\n#  3.2. fetcher: runs fetch\nFROM base AS fetcher\nCMD [\"./manage.py\", \"fetcher\"]\n#  3.3. fetcher: runs fetch\nFROM base AS renderer\nCMD [\"./manage.py\", \"renderer\"]\n#  3.4. cron: schedules fetches and runs cleanup SQL\nFROM base AS cron\nCMD [\"./manage.py\", \"cron\"]\n#  3.5. frontend: serves website\nFROM base AS frontend\nCOPY --from=jsbuild /app/webpack-stats.json /app/\n#  8080 is Kubernetes' conventional web-server port\nEXPOSE 8080/tcp\n#  TODO serve static files elsewhere\n#  Beware: our daphne does not serve static files! Use migrate-prod to push them\n#  to GCS and publish them there.\n#\n#  We set application-close-timeout to something enormous. Otherwise, Daphne will\n#  call `task.cancel()` on a long-running, closed connection. That's\n#  catastrophic: if the Websocket connection is subscribed to a group, then the\n#  group's messages will queue up until they fill our channel layer -- causing\n#  back-pressure, meaning _all Websocket connections stop working_. (At the same\n#  time, if the application never dies at all we have another error. So keep\n#  --application-close-timeout small enough that we'll get a warning when there's\n#  a bug in our code.)\nCMD [\"daphne\", \"-b\", \"0.0.0.0\", \"-p\", \"8080\", \"--application-close-timeout\", \"180\", \"cjworkbench.asgi:application\"]\n","injectedSmells":[],"originalDockerfileHash":"40560b85f3cf31aa4a1ddde6aa4eac84","successfullyInjectedSmells":[],"originalDockerfileUglified":"#   0. The barest Python: used in dev and prod\nFROM python:3.7.2-slim-stretch AS pybase\n#   We probably don't want these, long-term.\n#   nano: because we edit files on production\n#   postgresql-client: because we poll the DB:\n#   * on prod before ./manage.py migrate\n#   * on unittest before ./manage.py test\n#   git: used for importmodulefromgithub\n#   curl: handy for testing, NLTK download, Watchman download; not worth uninstalling each time\n#   unzip: to build Watchman ... and [adamhooper, 2019-02-21] I'm afraid\n#          to uninstall it after build, in case one of our Python deps shells to it\nRUN mkdir -p /usr/share/man/man1 /usr/share/man/man7 \\\n && apt-get update \\\n && apt-get install --no-install-recommends git nano postgresql-client unzip curl -y \\\n && rm -rf /var/lib/apt/lists/*\n#   Download NLTK stuff\n#\n#   NLTK expects its data to stay zipped\nRUN mkdir -p /usr/share/nltk_data \\\n && cd /usr/share/nltk_data \\\n && mkdir -p sentiment corpora \\\n && curl https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/stopwords.zip > corpora/stopwords.zip \\\n && curl https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/sentiment/vader_lexicon.zip > sentiment/vader_lexicon.zip\nRUN pip install pipenv==2018.11.26\n#   Set up /app\nRUN mkdir /app\nWORKDIR /app\n#   0.1 Pydev: just for the development environment\nFROM pybase AS pydev\n#   Need build-essential for:\n#   * regex (TODO nix the dep or make it support manylinux .whl)\n#   * Twisted - https://twistedmatrix.com/trac/ticket/7945\n#   * fastparquet\n#   * python-snappy\n#   * yajl-py\n#   * fb-re2\n#   * watchman (until someone packages binaries)\n#   * pysycopg2 (binaries are evil because psycopg2 links SSL -- as does Python)\n#\n#   Also:\n#   * socat: for our dev environment: fetcher uses http://localhost:8000 for in-lesson files\nRUN mkdir -p /root/.local/share/virtualenvs \\\n && apt-get update \\\n && apt-get install --no-install-recommends build-essential libsnappy-dev libre2-dev libpq-dev libyajl-dev socat -y \\\n && rm -rf /var/lib/apt/lists/*\n#   build watchman. Someday let's hope someone publishes binaries\n#   https://facebook.github.io/watchman/docs/install.html\nRUN cd /tmp \\\n && curl -L https://github.com/facebook/watchman/archive/v4.9.0.zip > watchman-4.9.0.zip \\\n && unzip watchman-4.9.0.zip \\\n && cd watchman-4.9.0 \\\n && apt-get update \\\n && apt-get install --no-install-recommends libssl-dev autoconf automake libtool libpcre3-dev pkg-config -y \\\n && ./autogen.sh \\\n && ./configure --prefix=/usr \\\n && make -j4 \\\n && make install \\\n && cd /tmp \\\n && rm -rf /tmp/watchman-4.9.0* \\\n && apt-get remove --purge -y libssl-dev autoconf automake libtool libpcre3-dev pkg-config \\\n && apt-get autoremove --purge -y \\\n && rm -rf /var/lib/apt/lists/*\n#   Add a Python wrapper that will help PyCharm cooperate with pipenv\n#   See https://blog.jetbrains.com/pycharm/2015/12/using-docker-in-pycharm/ for\n#   PyCharm's expectations. Just set \"Python interpreter path\" to\n#   \"pipenv-run-python\" to ensure:\n#\n#   * `cd /app`: PyCharm mounts the source tree to `/opt/project` and overwrites\n#                the current working directory. We force `cd /app` to restore\n#                what the Dockerfile already specifies. That's important because\n#                `pipenv` looks for packages in a virtualenv named after the\n#                current working directory.\n#\n#   * `exec pipenv run python \"$@\"`: PyCharm does not let us specify a command\n#                                    for Docker to run. It only lets us specify\n#                                    \"Python interpreter path.\" This wrapper will\n#                                    provide the interface PyCharm expects, with\n#                                    the environment variables Python needs to\n#                                    find our virtualenv.\n#\n#   Why do we create the file with RUN instead of COPY? Because even in 2018,\n#   COPY does not copy the executable bit on Windows, so we need a RUN anyway\n#   to make it executable.\nRUN true \\\n && echo '#!/bin/sh\\ncd /app\\nexec pipenv run python \"$@\"' > /usr/bin/pipenv-run-python \\\n && chmod +x /usr/bin/pipenv-run-python\n#   1. Python deps -- which rarely change, so this part of the Dockerfile will be\n#   cached (when building locally)\nFROM pybase AS pybuild\n#   Install Python dependencies. They rarely change.\n#   For Docker images we install them to the local system, not to a virtualenv.\n#   Containers don't use pipenv.\nCOPY Pipfile Pipfile.lock /app/\n#   Need build-essential for:\n#   * regex (TODO nix the dep or make it support manylinux .whl)\n#   * Twisted - https://twistedmatrix.com/trac/ticket/7945\n#   * fastparquet\n#   * python-snappy\n#   * yajl-py\n#   * pysycopg2 (binaries are evil because psycopg2 links SSL -- as does Python)\n#   ... and we want to keep libsnappy and yajl around after the fact, too\nRUN true \\\n && apt-get update \\\n && apt-get install --no-install-recommends build-essential libsnappy1v5 libsnappy-dev libre2-3 libre2-dev libpq-dev libyajl2 libyajl-dev -y \\\n && pipenv install --dev --system --deploy \\\n && apt-get remove --purge -y build-essential libsnappy-dev libre2-dev libpq-dev \\\n && apt-get autoremove --purge -y \\\n && rm -rf /var/lib/apt/lists/*\n#   2. Node deps -- completely independent\n#   2.1 jsbase: what we use in dev-in-docker\nFROM node:11.14.0-slim AS jsbase\nRUN mkdir /app\nWORKDIR /app\n#   2.2 jsbuild: where we build JavaScript assets\nFROM jsbase AS jsbuild\nCOPY package.json package-lock.json /app/\nRUN npm install\nCOPY webpack.config.js setupJest.js /app/\nCOPY __mocks__/ /app/__mocks__/\nCOPY assets/ /app/assets/\n#   Inject unit tests into our continuous integration\n#   This catches mistakes that would otherwise foil us in bin/integration-test;\n#   and currently we rely on this line in our CI scripts (cloudbuild.yaml).\nRUN npm test\nRUN node_modules/.bin/webpack -p\n#   3. Three prod servers will all be based on the same stuff:\nFROM pybuild AS base\nCOPY cjworkbench/ /app/cjworkbench/\n#   TODO make server/ frontend-specific\nCOPY server/ /app/server/\n#   cron/, fetcher/ and renderer/ are referenced in settings.py, so they must be\n#   in all Django apps. TODO make them _not_ Django apps. (change ORM)\nCOPY cron/ /app/cron/\nCOPY fetcher/ /app/fetcher/\nCOPY renderer/ /app/renderer/\nCOPY bin/ /app/bin/\nCOPY manage.py /app/\n#   templates are used in renderer for notifications emails and in frontend for\n#   views. TODO move renderer templates elsewhere.\nCOPY templates/ /app/templates/\n#   3.1. migrate: runs ./manage.py migrate\nFROM base AS migrate\n#   assets/ is static files. migrate will upload them to minio.\nCOPY assets/ /app/assets/\nCOPY --from=jsbuild /app/assets/bundles/ /app/assets/bundles/\nCMD [\"bin/migrate-prod\"]\n#   3.2. fetcher: runs fetch\nFROM base AS fetcher\nCMD [\"./manage.py\", \"fetcher\"]\n#   3.3. fetcher: runs fetch\nFROM base AS renderer\nCMD [\"./manage.py\", \"renderer\"]\n#   3.4. cron: schedules fetches and runs cleanup SQL\nFROM base AS cron\nCMD [\"./manage.py\", \"cron\"]\n#   3.5. frontend: serves website\nFROM base AS frontend\nCOPY --from=jsbuild /app/webpack-stats.json /app/\n#   8080 is Kubernetes' conventional web-server port\nEXPOSE 8080/tcp\n#   TODO serve static files elsewhere\n#   Beware: our daphne does not serve static files! Use migrate-prod to push them\n#   to GCS and publish them there.\n#\n#   We set application-close-timeout to something enormous. Otherwise, Daphne will\n#   call `task.cancel()` on a long-running, closed connection. That's\n#   catastrophic: if the Websocket connection is subscribed to a group, then the\n#   group's messages will queue up until they fill our channel layer -- causing\n#   back-pressure, meaning _all Websocket connections stop working_. (At the same\n#   time, if the application never dies at all we have another error. So keep\n#   --application-close-timeout small enough that we'll get a warning when there's\n#   a bug in our code.)\nCMD [\"daphne\", \"-b\", \"0.0.0.0\", \"-p\", \"8080\", \"--application-close-timeout\", \"180\", \"cjworkbench.asgi:application\"]\n","originalDockerfileUglifiedHash":"826eab35ab891a7edf8b3704ae1b86b9","fileName":"/ICSME-replicationpackage/dataset/smelly_dockerfiles_bianncle/3857b09539accc2c2c03ac68d14a17c3978f101e.dockerfile"}