{"seed":3886792892,"processedDockerfileHash":"e1e6fdf090c96f3745331638b37984ad","fixedSmells":["use-no-install-recommends","do-not-use-apt-get-update-alone","pin-package-manager-versions-apt-get","pin-package-manager-versions-pip","pin-package-manager-versions-npm","pin-package-manager-versions-gem","pin-package-manager-versions-apk","use-copy-instead-of-add","use-wget-instead-of-add","do-not-have-secrets","have-a-healthcheck","have-a-user"],"successfullyFixedSmells":["use-copy-instead-of-add","have-a-healthcheck","have-a-user"],"processedDockerfile":"#   Daniel Malczyk\n#   ThinkBig Analytics, a Teradata Company\n#  image for a separate Hadoop/Spark container for Kylo\nFROM airhacks/java\nMAINTAINER Daniel Malczyk <dmalczyk@gmail.com>\n#   install dev tools\nRUN yum clean all ; rpm --rebuilddb ; yum install -y curl which tar sudo openssh-server openssh-clients rsync mysql ; yum clean all\n#   update libselinux. see https://github.com/sequenceiq/hadoop-docker/issues/14\nRUN yum update -y libselinux\n#   passwordless ssh\nRUN ssh-keygen -q -N \"\" -t dsa -f /etc/ssh/ssh_host_dsa_key\nRUN ssh-keygen -q -N \"\" -t rsa -f /etc/ssh/ssh_host_rsa_key\nRUN ssh-keygen -q -N \"\" -t rsa -f /root/.ssh/id_rsa\nRUN ssh-keygen -q -N \"\" -t ecdsa -f /etc/ssh/ssh_host_ecdsa_key\nRUN ssh-keygen -q -N \"\" -t ed25519 -f /etc/ssh/ssh_host_ed25519_key\nRUN cp /root/.ssh/id_rsa.pub /root/.ssh/authorized_keys\n#   download native support\nRUN mkdir -p /tmp/native\nRUN curl -L https://github.com/sequenceiq/docker-hadoop-build/releases/download/v2.7.1/hadoop-native-64-2.7.1.tgz | tar -xz -C /tmp/native\n#  Install hadoop to /usr/local/hadoop\nRUN curl -s http://www.eu.apache.org/dist/hadoop/common/hadoop-2.7.1/hadoop-2.7.1.tar.gz | tar -xz -C /usr/local/\nRUN cd /usr/local \\\n && ln -s ./hadoop-2.7.1 hadoop\nENV HADOOP_HOME=\"/usr/local/hadoop\"\nENV HADOOP_INSTALL=\"$HADOOP_HOME\"\nENV HADOOP_PREFIX=\"$HADOOP_HOME\"\nENV PATH=\"$PATH:$HADOOP_INSTALL/sbin\"\nENV HADOOP_MAPRED_HOME=\"$HADOOP_INSTALL\"\nENV HADOOP_COMMON_HOME=\"$HADOOP_INSTALL\"\nENV HADOOP_HDFS_HOME=\"$HADOOP_INSTALL\"\nENV YARN_HOME=\"$HADOOP_INSTALL\"\nENV PATH=\"$HADOOP_HOME/bin:$PATH\"\nRUN sed -i '/^export JAVA_HOME/ s:.*:export JAVA_HOME=/etc/alternatives/java_sdk\\nexport HADOOP_PREFIX=/usr/local/hadoop\\nexport HADOOP_HOME=/usr/local/hadoop\\n:' $HADOOP_PREFIX/etc/hadoop/hadoop-env.sh\nRUN sed -i '/^export HADOOP_CONF_DIR/ s:.*:export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop/:' $HADOOP_PREFIX/etc/hadoop/hadoop-env.sh\nRUN mkdir $HADOOP_PREFIX/input\nRUN cp $HADOOP_PREFIX/etc/hadoop/*.xml $HADOOP_PREFIX/input\n#   pseudo distributed\nCOPY conf/core-site.xml.template2 $HADOOP_PREFIX/etc/hadoop/\nRUN sed s/HOSTNAME/localhost/ /usr/local/hadoop/etc/hadoop/core-site.xml.template2 > /usr/local/hadoop/etc/hadoop/core-site.xml\nCOPY conf/hdfs-site.xml $HADOOP_PREFIX/etc/hadoop/hdfs-site.xml\nCOPY conf/mapred-site.xml $HADOOP_PREFIX/etc/hadoop/mapred-site.xml\nCOPY conf/yarn-site.xml $HADOOP_PREFIX/etc/hadoop/yarn-site.xml\nRUN $HADOOP_PREFIX/bin/hdfs namenode -format\n#   fixing the libhadoop.so like a boss\nRUN rm -rf /usr/local/hadoop/lib/native\nRUN mv /tmp/native /usr/local/hadoop/lib\nCOPY conf/ssh_config /root/.ssh/config\nRUN chmod 600 /root/.ssh/config\nRUN chown root:root /root/.ssh/config\n#   workingaround docker.io build error\nRUN ls -la /usr/local/hadoop/etc/hadoop/*-env.sh\nRUN chmod +x /usr/local/hadoop/etc/hadoop/*-env.sh\nRUN ls -la /usr/local/hadoop/etc/hadoop/*-env.sh\n#   fix the 254 error code\nRUN sed -i \"/^[^#]*UsePAM/ s/.*/#&/\" /etc/ssh/sshd_config\nRUN echo \"UsePAM no\" >> /etc/ssh/sshd_config\nRUN echo \"Port 2122\" >> /etc/ssh/sshd_config\nRUN /usr/sbin/sshd \\\n && $HADOOP_PREFIX/etc/hadoop/hadoop-env.sh \\\n && $HADOOP_PREFIX/sbin/start-dfs.sh \\\n && $HADOOP_PREFIX/bin/hdfs dfs -mkdir -p /user/root\nRUN /usr/sbin/sshd \\\n && $HADOOP_PREFIX/etc/hadoop/hadoop-env.sh \\\n && $HADOOP_PREFIX/sbin/start-dfs.sh \\\n && $HADOOP_PREFIX/bin/hdfs dfs -put $HADOOP_PREFIX/etc/hadoop/ input\n#  Install spark to /usr/local/spark\n#  support for Hadoop 2.6.0\nRUN curl -s http://d3kbcqa49mib13.cloudfront.net/spark-1.6.1-bin-hadoop2.6.tgz | tar -xz -C /usr/local/\nRUN cd /usr/local \\\n && ln -s spark-1.6.1-bin-hadoop2.6 spark\nENV SPARK_HOME=\"/usr/local/spark\"\nENV PATH=\"$SPARK_HOME/bin:$PATH\"\n#   add spark and hadoop path to PATH env variable for kylo user\nRUN echo \"export PATH=$PATH:/usr/java/default/bin:/usr/local/spark/bin:/usr/local/hadoop/bin\" >> /etc/profile\n#   Install hive\nRUN curl -s http://apache.mirrors.spacedump.net/hive/hive-2.1.1/apache-hive-2.1.1-bin.tar.gz | tar -xz -C /usr/local/\nRUN cd /usr/local \\\n && ln -s apache-hive-2.1.1-bin hive\nCOPY conf/hive-site.xml /usr/local/hive/conf\nRUN echo \"export HIVE_HOME=/usr/local/hive\" >> /etc/profile\nRUN echo \"export PATH=$PATH:/usr/local/hive/bin\" >> /etc/profile\nENV HIVE_HOME=\"/usr/local/hive\"\nENV PATH=\"$PATH:$HIVE_HOME/bin\"\n#   Create directory for hive logs\nRUN mkdir -p /var/log/hive\n#   Increase PermGen space for hiveserver2 to fix OOM pb.\nCOPY conf/hive-env.sh /usr/local/hive/conf/\nRUN echo \"HADOOP_HOME=/usr/local/hadoop\" >> /usr/local/hive/bin/hive-config.sh\n#   Download mysql jdbc driver and prepare hive metastore.\nRUN curl -s -o /usr/local/apache-hive-2.1.1-bin/lib/mysql-connector-java-5.1.41.jar http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.41/mysql-connector-java-5.1.41.jar\n#   create hiveserver2 service\nCOPY conf/hive-server2 /etc/init.d/\nRUN chmod +x /etc/init.d/hive-server2\n#  RUN chkconfig --add /etc/init.d/hive-server2\n#   ---- Hive installation finished -------\n#   Prepare spark-hive integration, so spark sql will use hive tables defined in hive metastore, see https://spark.apache.org/docs/1.6.0/sql-programming-guide.html#hive-tables\nRUN cp /usr/local/hadoop/etc/hadoop/hdfs-site.xml /usr/local/spark/conf\nRUN cp /usr/local/hive/conf/hive-site.xml /usr/local/spark/conf\nRUN cp /usr/local/hive/lib/mysql-connector-java-5.1.41.jar /usr/local/spark/lib\n#   ----- Spark-Hive integration finished ---------\n#   create nifi user and group\nRUN /bin/bash -c 'useradd -r -m -s /bin/bash nifi'\n#   create kylo user and group\nRUN /bin/bash -c 'useradd -r -m -s /bin/bash kylo'\n#   Add kylo and nifi user to supergroup otherwise kylo-spark-shell service which runs as kylo user will not be able to create database in hive.\nRUN groupadd supergroup\nRUN usermod -a -G supergroup kylo\nRUN usermod -a -G supergroup nifi\nCOPY scripts/hadoop_bootstrap.sh /etc/hadoop_bootstrap.sh\nRUN chown root.root /etc/hadoop_bootstrap.sh\nRUN chmod 700 /etc/hadoop_bootstrap.sh\nENV BOOTSTRAP=\"/etc/hadoop_bootstrap.sh\"\nENTRYPOINT [\"/etc/hadoop_bootstrap.sh\"]\n#   Hdfs ports\nEXPOSE 50010/tcp 50020/tcp 50070/tcp 50075/tcp 50090/tcp 8020/tcp 9000/tcp\n#   Mapred ports\nEXPOSE 10020/tcp 19888/tcp\n#  Yarn ports\nEXPOSE 8030/tcp 8031/tcp 8032/tcp 8033/tcp 8040/tcp 8042/tcp 8088/tcp\n#  Other ports\nEXPOSE 49707/tcp 2122/tcp\n#  Hive\nEXPOSE 10000/tcp\n#  Spark\nEXPOSE 8450/tcp 8451/tcp 4040/tcp\nRUN groupadd --system docker-user ; useradd --system --gid docker-user docker-user\nUSER docker-user\n# Please add your HEALTHCHECK here!!!\n","originalDockerfile":"#  Daniel Malczyk\n#  ThinkBig Analytics, a Teradata Company\n# image for a separate Hadoop/Spark container for Kylo\nFROM airhacks/java\nMAINTAINER Daniel Malczyk <dmalczyk@gmail.com>\n#  install dev tools\nRUN yum clean all ; rpm --rebuilddb ; yum install -y curl which tar sudo openssh-server openssh-clients rsync mysql ; yum clean all\n#  update libselinux. see https://github.com/sequenceiq/hadoop-docker/issues/14\nRUN yum update -y libselinux\n#  passwordless ssh\nRUN ssh-keygen -q -N \"\" -t dsa -f /etc/ssh/ssh_host_dsa_key\nRUN ssh-keygen -q -N \"\" -t rsa -f /etc/ssh/ssh_host_rsa_key\nRUN ssh-keygen -q -N \"\" -t rsa -f /root/.ssh/id_rsa\nRUN ssh-keygen -q -N \"\" -t ecdsa -f /etc/ssh/ssh_host_ecdsa_key\nRUN ssh-keygen -q -N \"\" -t ed25519 -f /etc/ssh/ssh_host_ed25519_key\nRUN cp /root/.ssh/id_rsa.pub /root/.ssh/authorized_keys\n#  download native support\nRUN mkdir -p /tmp/native\nRUN curl -L https://github.com/sequenceiq/docker-hadoop-build/releases/download/v2.7.1/hadoop-native-64-2.7.1.tgz | tar -xz -C /tmp/native\n# Install hadoop to /usr/local/hadoop\nRUN curl -s http://www.eu.apache.org/dist/hadoop/common/hadoop-2.7.1/hadoop-2.7.1.tar.gz | tar -xz -C /usr/local/\nRUN cd /usr/local \\\n && ln -s ./hadoop-2.7.1 hadoop\nENV HADOOP_HOME=\"/usr/local/hadoop\"\nENV HADOOP_INSTALL=\"$HADOOP_HOME\"\nENV HADOOP_PREFIX=\"$HADOOP_HOME\"\nENV PATH=\"$PATH:$HADOOP_INSTALL/sbin\"\nENV HADOOP_MAPRED_HOME=\"$HADOOP_INSTALL\"\nENV HADOOP_COMMON_HOME=\"$HADOOP_INSTALL\"\nENV HADOOP_HDFS_HOME=\"$HADOOP_INSTALL\"\nENV YARN_HOME=\"$HADOOP_INSTALL\"\nENV PATH=\"$HADOOP_HOME/bin:$PATH\"\nRUN sed -i '/^export JAVA_HOME/ s:.*:export JAVA_HOME=/etc/alternatives/java_sdk\\nexport HADOOP_PREFIX=/usr/local/hadoop\\nexport HADOOP_HOME=/usr/local/hadoop\\n:' $HADOOP_PREFIX/etc/hadoop/hadoop-env.sh\nRUN sed -i '/^export HADOOP_CONF_DIR/ s:.*:export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop/:' $HADOOP_PREFIX/etc/hadoop/hadoop-env.sh\nRUN mkdir $HADOOP_PREFIX/input\nRUN cp $HADOOP_PREFIX/etc/hadoop/*.xml $HADOOP_PREFIX/input\n#  pseudo distributed\nCOPY conf/core-site.xml.template2 $HADOOP_PREFIX/etc/hadoop/\nRUN sed s/HOSTNAME/localhost/ /usr/local/hadoop/etc/hadoop/core-site.xml.template2 > /usr/local/hadoop/etc/hadoop/core-site.xml\nADD conf/hdfs-site.xml $HADOOP_PREFIX/etc/hadoop/hdfs-site.xml\nADD conf/mapred-site.xml $HADOOP_PREFIX/etc/hadoop/mapred-site.xml\nADD conf/yarn-site.xml $HADOOP_PREFIX/etc/hadoop/yarn-site.xml\nRUN $HADOOP_PREFIX/bin/hdfs namenode -format\n#  fixing the libhadoop.so like a boss\nRUN rm -rf /usr/local/hadoop/lib/native\nRUN mv /tmp/native /usr/local/hadoop/lib\nADD conf/ssh_config /root/.ssh/config\nRUN chmod 600 /root/.ssh/config\nRUN chown root:root /root/.ssh/config\n#  workingaround docker.io build error\nRUN ls -la /usr/local/hadoop/etc/hadoop/*-env.sh\nRUN chmod +x /usr/local/hadoop/etc/hadoop/*-env.sh\nRUN ls -la /usr/local/hadoop/etc/hadoop/*-env.sh\n#  fix the 254 error code\nRUN sed -i \"/^[^#]*UsePAM/ s/.*/#&/\" /etc/ssh/sshd_config\nRUN echo \"UsePAM no\" >> /etc/ssh/sshd_config\nRUN echo \"Port 2122\" >> /etc/ssh/sshd_config\nRUN /usr/sbin/sshd \\\n && $HADOOP_PREFIX/etc/hadoop/hadoop-env.sh \\\n && $HADOOP_PREFIX/sbin/start-dfs.sh \\\n && $HADOOP_PREFIX/bin/hdfs dfs -mkdir -p /user/root\nRUN /usr/sbin/sshd \\\n && $HADOOP_PREFIX/etc/hadoop/hadoop-env.sh \\\n && $HADOOP_PREFIX/sbin/start-dfs.sh \\\n && $HADOOP_PREFIX/bin/hdfs dfs -put $HADOOP_PREFIX/etc/hadoop/ input\n# Install spark to /usr/local/spark\n# support for Hadoop 2.6.0\nRUN curl -s http://d3kbcqa49mib13.cloudfront.net/spark-1.6.1-bin-hadoop2.6.tgz | tar -xz -C /usr/local/\nRUN cd /usr/local \\\n && ln -s spark-1.6.1-bin-hadoop2.6 spark\nENV SPARK_HOME=\"/usr/local/spark\"\nENV PATH=\"$SPARK_HOME/bin:$PATH\"\n#  add spark and hadoop path to PATH env variable for kylo user\nRUN echo \"export PATH=$PATH:/usr/java/default/bin:/usr/local/spark/bin:/usr/local/hadoop/bin\" >> /etc/profile\n#  Install hive\nRUN curl -s http://apache.mirrors.spacedump.net/hive/hive-2.1.1/apache-hive-2.1.1-bin.tar.gz | tar -xz -C /usr/local/\nRUN cd /usr/local \\\n && ln -s apache-hive-2.1.1-bin hive\nCOPY conf/hive-site.xml /usr/local/hive/conf\nRUN echo \"export HIVE_HOME=/usr/local/hive\" >> /etc/profile\nRUN echo \"export PATH=$PATH:/usr/local/hive/bin\" >> /etc/profile\nENV HIVE_HOME=\"/usr/local/hive\"\nENV PATH=\"$PATH:$HIVE_HOME/bin\"\n#  Create directory for hive logs\nRUN mkdir -p /var/log/hive\n#  Increase PermGen space for hiveserver2 to fix OOM pb.\nCOPY conf/hive-env.sh /usr/local/hive/conf/\nRUN echo \"HADOOP_HOME=/usr/local/hadoop\" >> /usr/local/hive/bin/hive-config.sh\n#  Download mysql jdbc driver and prepare hive metastore.\nRUN curl -s -o /usr/local/apache-hive-2.1.1-bin/lib/mysql-connector-java-5.1.41.jar http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.41/mysql-connector-java-5.1.41.jar\n#  create hiveserver2 service\nCOPY conf/hive-server2 /etc/init.d/\nRUN chmod +x /etc/init.d/hive-server2\n# RUN chkconfig --add /etc/init.d/hive-server2\n#  ---- Hive installation finished -------\n#  Prepare spark-hive integration, so spark sql will use hive tables defined in hive metastore, see https://spark.apache.org/docs/1.6.0/sql-programming-guide.html#hive-tables\nRUN cp /usr/local/hadoop/etc/hadoop/hdfs-site.xml /usr/local/spark/conf\nRUN cp /usr/local/hive/conf/hive-site.xml /usr/local/spark/conf\nRUN cp /usr/local/hive/lib/mysql-connector-java-5.1.41.jar /usr/local/spark/lib\n#  ----- Spark-Hive integration finished ---------\n#  create nifi user and group\nRUN /bin/bash -c 'useradd -r -m -s /bin/bash nifi'\n#  create kylo user and group\nRUN /bin/bash -c 'useradd -r -m -s /bin/bash kylo'\n#  Add kylo and nifi user to supergroup otherwise kylo-spark-shell service which runs as kylo user will not be able to create database in hive.\nRUN groupadd supergroup\nRUN usermod -a -G supergroup kylo\nRUN usermod -a -G supergroup nifi\nCOPY scripts/hadoop_bootstrap.sh /etc/hadoop_bootstrap.sh\nRUN chown root.root /etc/hadoop_bootstrap.sh\nRUN chmod 700 /etc/hadoop_bootstrap.sh\nENV BOOTSTRAP=\"/etc/hadoop_bootstrap.sh\"\nENTRYPOINT [\"/etc/hadoop_bootstrap.sh\"]\n#  Hdfs ports\nEXPOSE 50010/tcp 50020/tcp 50070/tcp 50075/tcp 50090/tcp 8020/tcp 9000/tcp\n#  Mapred ports\nEXPOSE 10020/tcp 19888/tcp\n# Yarn ports\nEXPOSE 8030/tcp 8031/tcp 8032/tcp 8033/tcp 8040/tcp 8042/tcp 8088/tcp\n# Other ports\nEXPOSE 49707/tcp 2122/tcp\n# Hive\nEXPOSE 10000/tcp\n# Spark\nEXPOSE 8450/tcp 8451/tcp 4040/tcp\n","injectedSmells":[],"originalDockerfileHash":"f0533915f9b38e19055c4533e7abc0bc","successfullyInjectedSmells":[],"originalDockerfileUglified":"#   Daniel Malczyk\n#   ThinkBig Analytics, a Teradata Company\n#  image for a separate Hadoop/Spark container for Kylo\nFROM airhacks/java\nMAINTAINER Daniel Malczyk <dmalczyk@gmail.com>\n#   install dev tools\nRUN yum clean all ; rpm --rebuilddb ; yum install -y curl which tar sudo openssh-server openssh-clients rsync mysql ; yum clean all\n#   update libselinux. see https://github.com/sequenceiq/hadoop-docker/issues/14\nRUN yum update -y libselinux\n#   passwordless ssh\nRUN ssh-keygen -q -N \"\" -t dsa -f /etc/ssh/ssh_host_dsa_key\nRUN ssh-keygen -q -N \"\" -t rsa -f /etc/ssh/ssh_host_rsa_key\nRUN ssh-keygen -q -N \"\" -t rsa -f /root/.ssh/id_rsa\nRUN ssh-keygen -q -N \"\" -t ecdsa -f /etc/ssh/ssh_host_ecdsa_key\nRUN ssh-keygen -q -N \"\" -t ed25519 -f /etc/ssh/ssh_host_ed25519_key\nRUN cp /root/.ssh/id_rsa.pub /root/.ssh/authorized_keys\n#   download native support\nRUN mkdir -p /tmp/native\nRUN curl -L https://github.com/sequenceiq/docker-hadoop-build/releases/download/v2.7.1/hadoop-native-64-2.7.1.tgz | tar -xz -C /tmp/native\n#  Install hadoop to /usr/local/hadoop\nRUN curl -s http://www.eu.apache.org/dist/hadoop/common/hadoop-2.7.1/hadoop-2.7.1.tar.gz | tar -xz -C /usr/local/\nRUN cd /usr/local \\\n && ln -s ./hadoop-2.7.1 hadoop\nENV HADOOP_HOME=\"/usr/local/hadoop\"\nENV HADOOP_INSTALL=\"$HADOOP_HOME\"\nENV HADOOP_PREFIX=\"$HADOOP_HOME\"\nENV PATH=\"$PATH:$HADOOP_INSTALL/sbin\"\nENV HADOOP_MAPRED_HOME=\"$HADOOP_INSTALL\"\nENV HADOOP_COMMON_HOME=\"$HADOOP_INSTALL\"\nENV HADOOP_HDFS_HOME=\"$HADOOP_INSTALL\"\nENV YARN_HOME=\"$HADOOP_INSTALL\"\nENV PATH=\"$HADOOP_HOME/bin:$PATH\"\nRUN sed -i '/^export JAVA_HOME/ s:.*:export JAVA_HOME=/etc/alternatives/java_sdk\\nexport HADOOP_PREFIX=/usr/local/hadoop\\nexport HADOOP_HOME=/usr/local/hadoop\\n:' $HADOOP_PREFIX/etc/hadoop/hadoop-env.sh\nRUN sed -i '/^export HADOOP_CONF_DIR/ s:.*:export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop/:' $HADOOP_PREFIX/etc/hadoop/hadoop-env.sh\nRUN mkdir $HADOOP_PREFIX/input\nRUN cp $HADOOP_PREFIX/etc/hadoop/*.xml $HADOOP_PREFIX/input\n#   pseudo distributed\nCOPY conf/core-site.xml.template2 $HADOOP_PREFIX/etc/hadoop/\nRUN sed s/HOSTNAME/localhost/ /usr/local/hadoop/etc/hadoop/core-site.xml.template2 > /usr/local/hadoop/etc/hadoop/core-site.xml\nADD conf/hdfs-site.xml $HADOOP_PREFIX/etc/hadoop/hdfs-site.xml\nADD conf/mapred-site.xml $HADOOP_PREFIX/etc/hadoop/mapred-site.xml\nADD conf/yarn-site.xml $HADOOP_PREFIX/etc/hadoop/yarn-site.xml\nRUN $HADOOP_PREFIX/bin/hdfs namenode -format\n#   fixing the libhadoop.so like a boss\nRUN rm -rf /usr/local/hadoop/lib/native\nRUN mv /tmp/native /usr/local/hadoop/lib\nADD conf/ssh_config /root/.ssh/config\nRUN chmod 600 /root/.ssh/config\nRUN chown root:root /root/.ssh/config\n#   workingaround docker.io build error\nRUN ls -la /usr/local/hadoop/etc/hadoop/*-env.sh\nRUN chmod +x /usr/local/hadoop/etc/hadoop/*-env.sh\nRUN ls -la /usr/local/hadoop/etc/hadoop/*-env.sh\n#   fix the 254 error code\nRUN sed -i \"/^[^#]*UsePAM/ s/.*/#&/\" /etc/ssh/sshd_config\nRUN echo \"UsePAM no\" >> /etc/ssh/sshd_config\nRUN echo \"Port 2122\" >> /etc/ssh/sshd_config\nRUN /usr/sbin/sshd \\\n && $HADOOP_PREFIX/etc/hadoop/hadoop-env.sh \\\n && $HADOOP_PREFIX/sbin/start-dfs.sh \\\n && $HADOOP_PREFIX/bin/hdfs dfs -mkdir -p /user/root\nRUN /usr/sbin/sshd \\\n && $HADOOP_PREFIX/etc/hadoop/hadoop-env.sh \\\n && $HADOOP_PREFIX/sbin/start-dfs.sh \\\n && $HADOOP_PREFIX/bin/hdfs dfs -put $HADOOP_PREFIX/etc/hadoop/ input\n#  Install spark to /usr/local/spark\n#  support for Hadoop 2.6.0\nRUN curl -s http://d3kbcqa49mib13.cloudfront.net/spark-1.6.1-bin-hadoop2.6.tgz | tar -xz -C /usr/local/\nRUN cd /usr/local \\\n && ln -s spark-1.6.1-bin-hadoop2.6 spark\nENV SPARK_HOME=\"/usr/local/spark\"\nENV PATH=\"$SPARK_HOME/bin:$PATH\"\n#   add spark and hadoop path to PATH env variable for kylo user\nRUN echo \"export PATH=$PATH:/usr/java/default/bin:/usr/local/spark/bin:/usr/local/hadoop/bin\" >> /etc/profile\n#   Install hive\nRUN curl -s http://apache.mirrors.spacedump.net/hive/hive-2.1.1/apache-hive-2.1.1-bin.tar.gz | tar -xz -C /usr/local/\nRUN cd /usr/local \\\n && ln -s apache-hive-2.1.1-bin hive\nCOPY conf/hive-site.xml /usr/local/hive/conf\nRUN echo \"export HIVE_HOME=/usr/local/hive\" >> /etc/profile\nRUN echo \"export PATH=$PATH:/usr/local/hive/bin\" >> /etc/profile\nENV HIVE_HOME=\"/usr/local/hive\"\nENV PATH=\"$PATH:$HIVE_HOME/bin\"\n#   Create directory for hive logs\nRUN mkdir -p /var/log/hive\n#   Increase PermGen space for hiveserver2 to fix OOM pb.\nCOPY conf/hive-env.sh /usr/local/hive/conf/\nRUN echo \"HADOOP_HOME=/usr/local/hadoop\" >> /usr/local/hive/bin/hive-config.sh\n#   Download mysql jdbc driver and prepare hive metastore.\nRUN curl -s -o /usr/local/apache-hive-2.1.1-bin/lib/mysql-connector-java-5.1.41.jar http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.41/mysql-connector-java-5.1.41.jar\n#   create hiveserver2 service\nCOPY conf/hive-server2 /etc/init.d/\nRUN chmod +x /etc/init.d/hive-server2\n#  RUN chkconfig --add /etc/init.d/hive-server2\n#   ---- Hive installation finished -------\n#   Prepare spark-hive integration, so spark sql will use hive tables defined in hive metastore, see https://spark.apache.org/docs/1.6.0/sql-programming-guide.html#hive-tables\nRUN cp /usr/local/hadoop/etc/hadoop/hdfs-site.xml /usr/local/spark/conf\nRUN cp /usr/local/hive/conf/hive-site.xml /usr/local/spark/conf\nRUN cp /usr/local/hive/lib/mysql-connector-java-5.1.41.jar /usr/local/spark/lib\n#   ----- Spark-Hive integration finished ---------\n#   create nifi user and group\nRUN /bin/bash -c 'useradd -r -m -s /bin/bash nifi'\n#   create kylo user and group\nRUN /bin/bash -c 'useradd -r -m -s /bin/bash kylo'\n#   Add kylo and nifi user to supergroup otherwise kylo-spark-shell service which runs as kylo user will not be able to create database in hive.\nRUN groupadd supergroup\nRUN usermod -a -G supergroup kylo\nRUN usermod -a -G supergroup nifi\nCOPY scripts/hadoop_bootstrap.sh /etc/hadoop_bootstrap.sh\nRUN chown root.root /etc/hadoop_bootstrap.sh\nRUN chmod 700 /etc/hadoop_bootstrap.sh\nENV BOOTSTRAP=\"/etc/hadoop_bootstrap.sh\"\nENTRYPOINT [\"/etc/hadoop_bootstrap.sh\"]\n#   Hdfs ports\nEXPOSE 50010/tcp 50020/tcp 50070/tcp 50075/tcp 50090/tcp 8020/tcp 9000/tcp\n#   Mapred ports\nEXPOSE 10020/tcp 19888/tcp\n#  Yarn ports\nEXPOSE 8030/tcp 8031/tcp 8032/tcp 8033/tcp 8040/tcp 8042/tcp 8088/tcp\n#  Other ports\nEXPOSE 49707/tcp 2122/tcp\n#  Hive\nEXPOSE 10000/tcp\n#  Spark\nEXPOSE 8450/tcp 8451/tcp 4040/tcp\n","originalDockerfileUglifiedHash":"c0de8af4e162fea4f0299c10b0eae724","fileName":"/ICSME-replicationpackage/dataset/smelly_dockerfiles_bianncle/6bde516f03a1e8f86f0d9d342cbd018326f873fb.dockerfile"}