{"seed":4194543481,"processedDockerfileHash":"5192d337d3121c04713bdd13a65030fc","fixedSmells":["use-no-install-recommends","do-not-use-apt-get-update-alone","pin-package-manager-versions-apt-get","pin-package-manager-versions-pip","pin-package-manager-versions-npm","pin-package-manager-versions-gem","pin-package-manager-versions-apk","use-copy-instead-of-add","use-wget-instead-of-add","do-not-have-secrets","have-a-healthcheck","have-a-user"],"successfullyFixedSmells":["use-no-install-recommends","have-a-healthcheck","have-a-user"],"processedDockerfile":"FROM isuper/java-oracle:jdk_8\nMAINTAINER Segence <segence@segence.com>\nWORKDIR /tmp\nRUN apt-get update \\\n && apt-get install --no-install-recommends openssh-server wget vim iputils-ping telnet dnsutils bzip2 ntp -y\nRUN update-rc.d ntp defaults\nRUN groupadd hadoop\nRUN useradd -d /home/hadoop -g hadoop -m hadoop\n#   SSH without key\nRUN mkdir /home/hadoop/.ssh\nRUN ssh-keygen -t rsa -f /home/hadoop/.ssh/id_rsa -P '' \\\n && cat /home/hadoop/.ssh/id_rsa.pub >> /home/hadoop/.ssh/authorized_keys\n#   Installing Hadoop\nRUN wget http://apache.mirror.anlx.net/hadoop/common/hadoop-2.9.0/hadoop-2.9.0.tar.gz\nRUN tar -xzvf hadoop-2.9.0.tar.gz -C /usr/local/\nRUN mv /usr/local/hadoop-2.9.0 /usr/local/hadoop\nENV HADOOP_HOME=\"/usr/local/hadoop\"\nENV HADOOP_CONF_DIR=\"$HADOOP_HOME/etc/hadoop\"\nENV YARN_CONF_DIR=\"$HADOOP_HOME/etc/hadoop\"\n#   Installing Scala\nRUN wget http://downloads.lightbend.com/scala/2.11.11/scala-2.11.11.tgz\nRUN tar -xzvf scala-2.11.11.tgz -C /usr/local/\nRUN mv /usr/local/scala-2.11.11 /usr/local/scala\nRUN chown -R root:root /usr/local/scala\nENV SCALA_HOME=\"/usr/local/scala\"\n#   Installing Spark\nRUN wget http://d3kbcqa49mib13.cloudfront.net/spark-2.2.0-bin-without-hadoop.tgz\nRUN tar -xzvf spark-2.2.0-bin-without-hadoop.tgz -C /usr/local/\nRUN mv /usr/local/spark-2.2.0-bin-without-hadoop /usr/local/spark\nENV SPARK_HOME=\"/usr/local/spark\"\nENV LD_LIBRARY_PATH=\"$HADOOP_HOME/lib/native/:$LD_LIBRARY_PATH\"\n#   Configuring Hadoop classpath for Spark\nRUN echo \"export SPARK_DIST_CLASSPATH=$( $HADOOP_HOME/bin/hadoop classpath ;)\" > /usr/local/spark/conf/spark-env.sh\n#   Installing the R language\nRUN apt-get install --no-install-recommends libssl-dev libssh2-1-dev libcurl4-openssl-dev libssl-dev r-base -y\nRUN R -e \"install.packages('devtools', repos = 'http://cran.us.r-project.org')\"\nRUN R -e \"install.packages('knitr', repos = 'http://cran.us.r-project.org')\"\nRUN R -e \"install.packages('ggplot2', repos = 'http://cran.us.r-project.org')\"\nRUN R -e \"install.packages(c('devtools','mplot', 'googleVis'), repos = 'http://cran.us.r-project.org'); require(devtools); install_github('ramnathv/rCharts')\"\n#   Installing Zeppelin\nRUN wget http://mirrors.ukfast.co.uk/sites/ftp.apache.org/zeppelin/zeppelin-0.7.3/zeppelin-0.7.3-bin-netinst.tgz\nRUN tar -xzvf zeppelin-0.7.3-bin-netinst.tgz -C /usr/local/\nRUN mv /usr/local/zeppelin-0.7.3-bin-netinst /usr/local/zeppelin\nENV ZEPPELIN_HOME=\"/usr/local/zeppelin\"\nCOPY config/zeppelin-env.sh $ZEPPELIN_HOME/conf/zeppelin-env.sh\nCOPY config/zeppelin-site.xml $ZEPPELIN_HOME/conf/zeppelin-site.xml\nRUN chown -R hadoop:hadoop $ZEPPELIN_HOME\n#   Setting the PATH environment variable globally and for the Hadoop user\nENV PATH=\"$PATH:$JAVA_HOME/bin:/usr/local/hadoop/bin:/usr/local/hadoop/sbin:$SCALA_HOME/bin:$SPARK_HOME/bin:$ZEPPELIN_HOME/bin\"\nRUN echo \"PATH=$PATH:$JAVA_HOME/bin:/usr/local/hadoop/bin:/usr/local/hadoop/sbin:$SCALA_HOME/bin:$SPARK_HOME/bin\" >> /home/hadoop/.bashrc\n#   Hadoop configuration\nCOPY config/sshd_config /etc/ssh/sshd_config\nCOPY config/ssh_config /home/hadoop/.ssh/config\nCOPY config/hadoop-env.sh config/hdfs-site.xml config/hdfs-site.xml config/core-site.xml config/core-site.xml config/mapred-site.xml config/yarn-site.xml config/yarn-site.xml $HADOOP_CONF_DIR/\n#   Adding initialisation scripts\nRUN mkdir $HADOOP_HOME/bin/init\nCOPY init-scripts/init-hadoop.sh $HADOOP_HOME/bin/init/\nCOPY init-scripts/start-hadoop.sh init-scripts/stop-hadoop.sh $HADOOP_HOME/bin/init/\nCOPY init-scripts/hadoop /etc/init.d/\n#   Adding utilities\nRUN mkdir -p /home/hadoop/utils\nCOPY utils/run-wordcount.sh utils/format-namenode.sh /home/hadoop/utils/\n#   Replacing Hadoop slave file with provided one and changing logs directory\nRUN rm $HADOOP_CONF_DIR/slaves\nRUN ln -s /config/slaves $HADOOP_CONF_DIR/slaves\n#   Setting up log directories\nRUN ln -s /data/logs/hadoop $HADOOP_HOME/logs\nRUN ln -s $HADOOP_HOME/logs /var/log/hadoop\nRUN ln -s $ZEPPELIN_HOME/logs /var/log/zeppelin\n#   Set permissions on Hadoop home\nRUN chown -R hadoop:hadoop $HADOOP_HOME\nRUN chown -R hadoop:hadoop /home/hadoop\n#   Cleanup\nRUN rm -rf /tmp/*\nWORKDIR /root\nEXPOSE 2222/tcp 4040/tcp 8020/tcp 8030/tcp 8031/tcp 8032/tcp 8033/tcp 8042/tcp 8088/tcp 9001/tcp 50010/tcp 50020/tcp 50070/tcp 50075/tcp 50090/tcp 50100/tcp\nVOLUME /data\nVOLUME /config\nVOLUME /deployments\nENTRYPOINT [\"sh\", \"-c\", \"service\", \"ntp\", \"start\", \";\", \"$HADOOP_HOME/bin/init/init-hadoop.sh\", \";\", \"service\", \"ssh\", \"start\", \";\", \"bash\"]\nRUN groupadd --system docker-user ; useradd --system --gid docker-user docker-user\nUSER docker-user\n# Please add your HEALTHCHECK here!!!\n","originalDockerfile":"FROM isuper/java-oracle:jdk_8\nMAINTAINER Segence <segence@segence.com>\nWORKDIR /tmp\nRUN apt-get update \\\n && apt-get install openssh-server wget vim iputils-ping telnet dnsutils bzip2 ntp -y\nRUN update-rc.d ntp defaults\nRUN groupadd hadoop\nRUN useradd -d /home/hadoop -g hadoop -m hadoop\n#  SSH without key\nRUN mkdir /home/hadoop/.ssh\nRUN ssh-keygen -t rsa -f /home/hadoop/.ssh/id_rsa -P '' \\\n && cat /home/hadoop/.ssh/id_rsa.pub >> /home/hadoop/.ssh/authorized_keys\n#  Installing Hadoop\nRUN wget http://apache.mirror.anlx.net/hadoop/common/hadoop-2.9.0/hadoop-2.9.0.tar.gz\nRUN tar -xzvf hadoop-2.9.0.tar.gz -C /usr/local/\nRUN mv /usr/local/hadoop-2.9.0 /usr/local/hadoop\nENV HADOOP_HOME=\"/usr/local/hadoop\"\nENV HADOOP_CONF_DIR=\"$HADOOP_HOME/etc/hadoop\"\nENV YARN_CONF_DIR=\"$HADOOP_HOME/etc/hadoop\"\n#  Installing Scala\nRUN wget http://downloads.lightbend.com/scala/2.11.11/scala-2.11.11.tgz\nRUN tar -xzvf scala-2.11.11.tgz -C /usr/local/\nRUN mv /usr/local/scala-2.11.11 /usr/local/scala\nRUN chown -R root:root /usr/local/scala\nENV SCALA_HOME=\"/usr/local/scala\"\n#  Installing Spark\nRUN wget http://d3kbcqa49mib13.cloudfront.net/spark-2.2.0-bin-without-hadoop.tgz\nRUN tar -xzvf spark-2.2.0-bin-without-hadoop.tgz -C /usr/local/\nRUN mv /usr/local/spark-2.2.0-bin-without-hadoop /usr/local/spark\nENV SPARK_HOME=\"/usr/local/spark\"\nENV LD_LIBRARY_PATH=\"$HADOOP_HOME/lib/native/:$LD_LIBRARY_PATH\"\n#  Configuring Hadoop classpath for Spark\nRUN echo \"export SPARK_DIST_CLASSPATH=$( $HADOOP_HOME/bin/hadoop classpath ;)\" > /usr/local/spark/conf/spark-env.sh\n#  Installing the R language\nRUN apt-get install libssl-dev libssh2-1-dev libcurl4-openssl-dev libssl-dev r-base -y\nRUN R -e \"install.packages('devtools', repos = 'http://cran.us.r-project.org')\"\nRUN R -e \"install.packages('knitr', repos = 'http://cran.us.r-project.org')\"\nRUN R -e \"install.packages('ggplot2', repos = 'http://cran.us.r-project.org')\"\nRUN R -e \"install.packages(c('devtools','mplot', 'googleVis'), repos = 'http://cran.us.r-project.org'); require(devtools); install_github('ramnathv/rCharts')\"\n#  Installing Zeppelin\nRUN wget http://mirrors.ukfast.co.uk/sites/ftp.apache.org/zeppelin/zeppelin-0.7.3/zeppelin-0.7.3-bin-netinst.tgz\nRUN tar -xzvf zeppelin-0.7.3-bin-netinst.tgz -C /usr/local/\nRUN mv /usr/local/zeppelin-0.7.3-bin-netinst /usr/local/zeppelin\nENV ZEPPELIN_HOME=\"/usr/local/zeppelin\"\nCOPY config/zeppelin-env.sh $ZEPPELIN_HOME/conf/zeppelin-env.sh\nCOPY config/zeppelin-site.xml $ZEPPELIN_HOME/conf/zeppelin-site.xml\nRUN chown -R hadoop:hadoop $ZEPPELIN_HOME\n#  Setting the PATH environment variable globally and for the Hadoop user\nENV PATH=\"$PATH:$JAVA_HOME/bin:/usr/local/hadoop/bin:/usr/local/hadoop/sbin:$SCALA_HOME/bin:$SPARK_HOME/bin:$ZEPPELIN_HOME/bin\"\nRUN echo \"PATH=$PATH:$JAVA_HOME/bin:/usr/local/hadoop/bin:/usr/local/hadoop/sbin:$SCALA_HOME/bin:$SPARK_HOME/bin\" >> /home/hadoop/.bashrc\n#  Hadoop configuration\nCOPY config/sshd_config /etc/ssh/sshd_config\nCOPY config/ssh_config /home/hadoop/.ssh/config\nCOPY config/hadoop-env.sh config/hdfs-site.xml config/hdfs-site.xml config/core-site.xml config/core-site.xml config/mapred-site.xml config/yarn-site.xml config/yarn-site.xml $HADOOP_CONF_DIR/\n#  Adding initialisation scripts\nRUN mkdir $HADOOP_HOME/bin/init\nCOPY init-scripts/init-hadoop.sh $HADOOP_HOME/bin/init/\nCOPY init-scripts/start-hadoop.sh init-scripts/stop-hadoop.sh $HADOOP_HOME/bin/init/\nCOPY init-scripts/hadoop /etc/init.d/\n#  Adding utilities\nRUN mkdir -p /home/hadoop/utils\nCOPY utils/run-wordcount.sh utils/format-namenode.sh /home/hadoop/utils/\n#  Replacing Hadoop slave file with provided one and changing logs directory\nRUN rm $HADOOP_CONF_DIR/slaves\nRUN ln -s /config/slaves $HADOOP_CONF_DIR/slaves\n#  Setting up log directories\nRUN ln -s /data/logs/hadoop $HADOOP_HOME/logs\nRUN ln -s $HADOOP_HOME/logs /var/log/hadoop\nRUN ln -s $ZEPPELIN_HOME/logs /var/log/zeppelin\n#  Set permissions on Hadoop home\nRUN chown -R hadoop:hadoop $HADOOP_HOME\nRUN chown -R hadoop:hadoop /home/hadoop\n#  Cleanup\nRUN rm -rf /tmp/*\nWORKDIR /root\nEXPOSE 2222/tcp 4040/tcp 8020/tcp 8030/tcp 8031/tcp 8032/tcp 8033/tcp 8042/tcp 8088/tcp 9001/tcp 50010/tcp 50020/tcp 50070/tcp 50075/tcp 50090/tcp 50100/tcp\nVOLUME /data\nVOLUME /config\nVOLUME /deployments\nENTRYPOINT [\"sh\", \"-c\", \"service\", \"ntp\", \"start\", \";\", \"$HADOOP_HOME/bin/init/init-hadoop.sh\", \";\", \"service\", \"ssh\", \"start\", \";\", \"bash\"]\n","injectedSmells":[],"originalDockerfileHash":"47e406fdd2f439e2213a92c7fa03a779","successfullyInjectedSmells":[],"originalDockerfileUglified":"FROM isuper/java-oracle:jdk_8\nMAINTAINER Segence <segence@segence.com>\nWORKDIR /tmp\nRUN apt-get update \\\n && apt-get install openssh-server wget vim iputils-ping telnet dnsutils bzip2 ntp -y\nRUN update-rc.d ntp defaults\nRUN groupadd hadoop\nRUN useradd -d /home/hadoop -g hadoop -m hadoop\n#   SSH without key\nRUN mkdir /home/hadoop/.ssh\nRUN ssh-keygen -t rsa -f /home/hadoop/.ssh/id_rsa -P '' \\\n && cat /home/hadoop/.ssh/id_rsa.pub >> /home/hadoop/.ssh/authorized_keys\n#   Installing Hadoop\nRUN wget http://apache.mirror.anlx.net/hadoop/common/hadoop-2.9.0/hadoop-2.9.0.tar.gz\nRUN tar -xzvf hadoop-2.9.0.tar.gz -C /usr/local/\nRUN mv /usr/local/hadoop-2.9.0 /usr/local/hadoop\nENV HADOOP_HOME=\"/usr/local/hadoop\"\nENV HADOOP_CONF_DIR=\"$HADOOP_HOME/etc/hadoop\"\nENV YARN_CONF_DIR=\"$HADOOP_HOME/etc/hadoop\"\n#   Installing Scala\nRUN wget http://downloads.lightbend.com/scala/2.11.11/scala-2.11.11.tgz\nRUN tar -xzvf scala-2.11.11.tgz -C /usr/local/\nRUN mv /usr/local/scala-2.11.11 /usr/local/scala\nRUN chown -R root:root /usr/local/scala\nENV SCALA_HOME=\"/usr/local/scala\"\n#   Installing Spark\nRUN wget http://d3kbcqa49mib13.cloudfront.net/spark-2.2.0-bin-without-hadoop.tgz\nRUN tar -xzvf spark-2.2.0-bin-without-hadoop.tgz -C /usr/local/\nRUN mv /usr/local/spark-2.2.0-bin-without-hadoop /usr/local/spark\nENV SPARK_HOME=\"/usr/local/spark\"\nENV LD_LIBRARY_PATH=\"$HADOOP_HOME/lib/native/:$LD_LIBRARY_PATH\"\n#   Configuring Hadoop classpath for Spark\nRUN echo \"export SPARK_DIST_CLASSPATH=$( $HADOOP_HOME/bin/hadoop classpath ;)\" > /usr/local/spark/conf/spark-env.sh\n#   Installing the R language\nRUN apt-get install libssl-dev libssh2-1-dev libcurl4-openssl-dev libssl-dev r-base -y\nRUN R -e \"install.packages('devtools', repos = 'http://cran.us.r-project.org')\"\nRUN R -e \"install.packages('knitr', repos = 'http://cran.us.r-project.org')\"\nRUN R -e \"install.packages('ggplot2', repos = 'http://cran.us.r-project.org')\"\nRUN R -e \"install.packages(c('devtools','mplot', 'googleVis'), repos = 'http://cran.us.r-project.org'); require(devtools); install_github('ramnathv/rCharts')\"\n#   Installing Zeppelin\nRUN wget http://mirrors.ukfast.co.uk/sites/ftp.apache.org/zeppelin/zeppelin-0.7.3/zeppelin-0.7.3-bin-netinst.tgz\nRUN tar -xzvf zeppelin-0.7.3-bin-netinst.tgz -C /usr/local/\nRUN mv /usr/local/zeppelin-0.7.3-bin-netinst /usr/local/zeppelin\nENV ZEPPELIN_HOME=\"/usr/local/zeppelin\"\nCOPY config/zeppelin-env.sh $ZEPPELIN_HOME/conf/zeppelin-env.sh\nCOPY config/zeppelin-site.xml $ZEPPELIN_HOME/conf/zeppelin-site.xml\nRUN chown -R hadoop:hadoop $ZEPPELIN_HOME\n#   Setting the PATH environment variable globally and for the Hadoop user\nENV PATH=\"$PATH:$JAVA_HOME/bin:/usr/local/hadoop/bin:/usr/local/hadoop/sbin:$SCALA_HOME/bin:$SPARK_HOME/bin:$ZEPPELIN_HOME/bin\"\nRUN echo \"PATH=$PATH:$JAVA_HOME/bin:/usr/local/hadoop/bin:/usr/local/hadoop/sbin:$SCALA_HOME/bin:$SPARK_HOME/bin\" >> /home/hadoop/.bashrc\n#   Hadoop configuration\nCOPY config/sshd_config /etc/ssh/sshd_config\nCOPY config/ssh_config /home/hadoop/.ssh/config\nCOPY config/hadoop-env.sh config/hdfs-site.xml config/hdfs-site.xml config/core-site.xml config/core-site.xml config/mapred-site.xml config/yarn-site.xml config/yarn-site.xml $HADOOP_CONF_DIR/\n#   Adding initialisation scripts\nRUN mkdir $HADOOP_HOME/bin/init\nCOPY init-scripts/init-hadoop.sh $HADOOP_HOME/bin/init/\nCOPY init-scripts/start-hadoop.sh init-scripts/stop-hadoop.sh $HADOOP_HOME/bin/init/\nCOPY init-scripts/hadoop /etc/init.d/\n#   Adding utilities\nRUN mkdir -p /home/hadoop/utils\nCOPY utils/run-wordcount.sh utils/format-namenode.sh /home/hadoop/utils/\n#   Replacing Hadoop slave file with provided one and changing logs directory\nRUN rm $HADOOP_CONF_DIR/slaves\nRUN ln -s /config/slaves $HADOOP_CONF_DIR/slaves\n#   Setting up log directories\nRUN ln -s /data/logs/hadoop $HADOOP_HOME/logs\nRUN ln -s $HADOOP_HOME/logs /var/log/hadoop\nRUN ln -s $ZEPPELIN_HOME/logs /var/log/zeppelin\n#   Set permissions on Hadoop home\nRUN chown -R hadoop:hadoop $HADOOP_HOME\nRUN chown -R hadoop:hadoop /home/hadoop\n#   Cleanup\nRUN rm -rf /tmp/*\nWORKDIR /root\nEXPOSE 2222/tcp 4040/tcp 8020/tcp 8030/tcp 8031/tcp 8032/tcp 8033/tcp 8042/tcp 8088/tcp 9001/tcp 50010/tcp 50020/tcp 50070/tcp 50075/tcp 50090/tcp 50100/tcp\nVOLUME /data\nVOLUME /config\nVOLUME /deployments\nENTRYPOINT [\"sh\", \"-c\", \"service\", \"ntp\", \"start\", \";\", \"$HADOOP_HOME/bin/init/init-hadoop.sh\", \";\", \"service\", \"ssh\", \"start\", \";\", \"bash\"]\n","originalDockerfileUglifiedHash":"9f0ddcde2b9af0494d06ccf89ff789ce","fileName":"/ICSME-replicationpackage/dataset/smelly_dockerfiles_bianncle/6f64c8a779426ef29b3ba867ec27b0dbffc1a9f8.dockerfile"}