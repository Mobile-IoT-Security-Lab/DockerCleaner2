{"seed":3894328802,"processedDockerfileHash":"fb62833dbec3364c588bd6a4c2155559","fixedSmells":["use-no-install-recommends","do-not-use-apt-get-update-alone","pin-package-manager-versions-apt-get","pin-package-manager-versions-pip","pin-package-manager-versions-npm","pin-package-manager-versions-gem","pin-package-manager-versions-apk","use-copy-instead-of-add","use-wget-instead-of-add","do-not-have-secrets","have-a-healthcheck","have-a-user"],"successfullyFixedSmells":["use-no-install-recommends","pin-package-manager-versions-apt-get","pin-package-manager-versions-pip","use-copy-instead-of-add","have-a-healthcheck","have-a-user"],"processedDockerfile":"#   Setup an environment for running this book's examples\nFROM ubuntu\nMAINTAINER Russell Jurney, russell.jurney@gmail.com\nWORKDIR /root\n#   Update apt-get and install things\nRUN apt-get autoclean\nRUN apt-get update \\\n && apt-get install --no-install-recommends zip=3.0-13 unzip=6.0-27ubuntu1 curl=7.88.1-7ubuntu1 bzip2=1.0.8-5build1 python-dev build-essential=12.9ubuntu3 git=1:2.39.2-1ubuntu1 libssl1.0.0 libssl-dev=3.0.8-1ubuntu1 -y\n#   Setup Oracle Java8\nRUN apt-get install --no-install-recommends software-properties-common=0.99.35 debconf-utils=1.5.82 -y \\\n && add-apt-repository -y ppa:webupd8team/java \\\n && apt-get update \\\n && echo \"oracle-java8-installer shared/accepted-oracle-license-v1-1 select true\" | debconf-set-selections \\\n && apt-get install --no-install-recommends oracle-java8-installer -y\nENV JAVA_HOME=\"/usr/lib/jvm/java-8-oracle\"\n#   Download and install Anaconda Python\nRUN which wget &> /dev/null || apt-get install --no-install-recommends wget=1.20.3 ; wget --no-verbose --output-document /tmp/Anaconda3-4.2.0-Linux-x86_64.sh http://repo.continuum.io/archive/Anaconda3-4.2.0-Linux-x86_64.sh\nRUN bash /tmp/Anaconda3-4.2.0-Linux-x86_64.sh -b -p /root/anaconda\nENV PATH=\"/root/anaconda/bin:$PATH\"\n#\n#   Install git, clone repo, install Python dependencies\n#\nRUN git clone https://github.com/rjurney/Agile_Data_Code_2\nWORKDIR /root/Agile_Data_Code_2\nENV PROJECT_HOME=\"/Agile_Data_Code_2\"\nRUN pip install pip==23.1 --upgrade \\\n && pip install -r requirements.txt\nWORKDIR /root\n#\n#   Install Hadoop: may need to update this link... see http://hadoop.apache.org/releases.html\n#\nRUN which wget &> /dev/null || apt-get install --no-install-recommends wget=1.20.3 ; wget --no-verbose --output-document /tmp/hadoop-2.7.3.tar.gz http://apache.osuosl.org/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz\nRUN mkdir -p /root/hadoop \\\n && tar -xvf /tmp/hadoop-2.7.3.tar.gz -C hadoop --strip-components=1\nENV HADOOP_HOME=\"/root/hadoop\"\nENV PATH=\"$PATH:$HADOOP_HOME/bin\"\nENV HADOOP_CLASSPATH=\"/root/hadoop/etc/hadoop/:/root/hadoop/share/hadoop/common/lib/*:/root/hadoop/share/hadoop/common/*:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/*:/root/hadoop/share/hadoop/hdfs/*:/root/hadoop/share/hadoop/yarn/lib/*:/root/hadoop/share/hadoop/yarn/*:/root/hadoop/share/hadoop/mapreduce/lib/*:/root/hadoop/share/hadoop/mapreduce/*:/root/hadoop/etc/hadoop:/root/hadoop/share/hadoop/common/lib/*:/root/hadoop/share/hadoop/common/*:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/*:/root/hadoop/share/hadoop/hdfs/*:/root/hadoop/share/hadoop/yarn/lib/*:/root/hadoop/share/hadoop/yarn/*:/root/hadoop/share/hadoop/mapreduce/lib/*:/root/hadoop/share/hadoop/mapreduce/*:/root/hadoop/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar\"\nENV HADOOP_CONF_DIR=\"/root/hadoop/etc/hadoop\"\n#\n#   Install Spark: may need to update this link... see http://spark.apache.org/downloads.html\n#\nRUN which wget &> /dev/null || apt-get install --no-install-recommends wget=1.20.3 ; wget --no-verbose --output-document /tmp/spark-2.1.0-bin-without-hadoop.tgz http://d3kbcqa49mib13.cloudfront.net/spark-2.1.0-bin-without-hadoop.tgz\nRUN mkdir -p /root/spark \\\n && tar -xvf /tmp/spark-2.1.0-bin-without-hadoop.tgz -C spark --strip-components=1\nENV SPARK_HOME=\"/root/spark\"\nENV HADOOP_CONF_DIR=\"/root/hadoop/etc/hadoop/\"\nENV SPARK_DIST_CLASSPATH=\"/root/hadoop/etc/hadoop/:/root/hadoop/share/hadoop/common/lib/*:/root/hadoop/share/hadoop/common/*:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/*:/root/hadoop/share/hadoop/hdfs/*:/root/hadoop/share/hadoop/yarn/lib/*:/root/hadoop/share/hadoop/yarn/*:/root/hadoop/share/hadoop/mapreduce/lib/*:/root/hadoop/share/hadoop/mapreduce/*:/root/hadoop/etc/hadoop:/root/hadoop/share/hadoop/common/lib/*:/root/hadoop/share/hadoop/common/*:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/*:/root/hadoop/share/hadoop/hdfs/*:/root/hadoop/share/hadoop/yarn/lib/*:/root/hadoop/share/hadoop/yarn/*:/root/hadoop/share/hadoop/mapreduce/lib/*:/root/hadoop/share/hadoop/mapreduce/*:/root/hadoop/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar\"\nENV PATH=\"$PATH:/root/spark/bin\"\n#   Have to set spark.io.compression.codec in Spark local mode, give 8GB RAM\nRUN cp /root/spark/conf/spark-defaults.conf.template /root/spark/conf/spark-defaults.conf \\\n && echo 'spark.io.compression.codec org.apache.spark.io.SnappyCompressionCodec' >> /root/spark/conf/spark-defaults.conf \\\n && echo \"spark.driver.memory 8g\" >> /root/spark/conf/spark-defaults.conf\n#   Setup spark-env.sh to use Python 3\nRUN echo \"PYSPARK_PYTHON=python3\" >> /root/spark/conf/spark-env.sh \\\n && echo \"PYSPARK_DRIVER_PYTHON=python3\" >> /root/spark/conf/spark-env.sh\n#   Setup log4j config to reduce logging output\nRUN cp /root/spark/conf/log4j.properties.template /root/spark/conf/log4j.properties \\\n && sed -i 's/INFO/ERROR/g' /root/spark/conf/log4j.properties\n#\n#   Install Mongo, Mongo Java driver, and mongo-hadoop and start MongoDB\n#\nRUN echo \"deb [ arch=amd64,arm64 ] http://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.4 multiverse\" > /etc/apt/sources.list.d/mongodb-org-3.4.list\nRUN apt-get update \\\n && apt-get install --no-install-recommends mongodb-org -y --allow-unauthenticated \\\n && mkdir -p /data/db\n#   apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 0C49F3730359A14518585931BC711F9BA15703C6 && \\\nRUN /usr/bin/mongod --fork --logpath /var/log/mongodb.log\n#   Get the MongoDB Java Driver and put it in Agile_Data_Code_2\nRUN which wget &> /dev/null || apt-get install --no-install-recommends wget=1.20.3 ; wget --no-verbose --output-document /tmp/mongo-java-driver-3.4.0.jar http://central.maven.org/maven2/org/mongodb/mongo-java-driver/3.4.0/mongo-java-driver-3.4.0.jar\nRUN mv /tmp/mongo-java-driver-3.4.0.jar /root/Agile_Data_Code_2/lib/\n#   Install the mongo-hadoop project in the mongo-hadoop directory in the root of our project.\nRUN which wget &> /dev/null || apt-get install --no-install-recommends wget=1.20.3 ; wget --no-verbose --output-document /tmp/mongo-hadoop-r1.5.2.tar.gz https://github.com/mongodb/mongo-hadoop/archive/r1.5.2.tar.gz\nRUN mkdir -p /root/mongo-hadoop \\\n && tar -xvzf /tmp/mongo-hadoop-r1.5.2.tar.gz -C mongo-hadoop --strip-components=1 \\\n && rm -f /tmp/mongo-hadoop-r1.5.2.tar.gz\nWORKDIR /root/mongo-hadoop\nRUN /root/mongo-hadoop/gradlew jar\nWORKDIR /root\nRUN cp /root/mongo-hadoop/spark/build/libs/mongo-hadoop-spark-*.jar /root/Agile_Data_Code_2/lib/ \\\n && cp /root/mongo-hadoop/build/libs/mongo-hadoop-*.jar /root/Agile_Data_Code_2/lib/\n#   Install pymongo_spark\nWORKDIR /root/mongo-hadoop/spark/src/main/python\nRUN python setup.py install\nWORKDIR /root\nRUN cp /root/mongo-hadoop/spark/src/main/python/pymongo_spark.py /root/Agile_Data_Code_2/lib/\nENV PYTHONPATH=\"$PYTHONPATH:/root/Agile_Data_Code_2/lib\"\n#   Cleanup mongo-hadoop\nRUN rm -rf /root/mongo-hadoop\n#\n#   Install ElasticSearch in the elasticsearch directory in the root of our project, and the Elasticsearch for Hadoop package\n#\nRUN which wget &> /dev/null || apt-get install --no-install-recommends wget=1.20.3 ; wget --no-verbose --output-document /tmp/elasticsearch-5.1.1.tar.gz https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.1.1.tar.gz\nRUN mkdir /root/elasticsearch \\\n && tar -xvzf /tmp/elasticsearch-5.1.1.tar.gz -C elasticsearch --strip-components=1 \\\n && /root/elasticsearch/bin/elasticsearch -d \\\n && rm -f /tmp/elasticsearch-5.1.1.tar.gz\n#   Install Elasticsearch for Hadoop\nRUN which wget &> /dev/null || apt-get install --no-install-recommends wget=1.20.3 ; wget --no-verbose --output-document /tmp/elasticsearch-hadoop-5.1.1.zip http://download.elastic.co/hadoop/elasticsearch-hadoop-5.1.1.zip\nRUN unzip /tmp/elasticsearch-hadoop-5.1.1.zip \\\n && mv /root/elasticsearch-hadoop-5.1.1 /root/elasticsearch-hadoop \\\n && cp /root/elasticsearch-hadoop/dist/elasticsearch-hadoop-5.1.1.jar /root/Agile_Data_Code_2/lib/ \\\n && cp /root/elasticsearch-hadoop/dist/elasticsearch-spark-20_2.10-5.1.1.jar /root/Agile_Data_Code_2/lib/ \\\n && echo \"spark.speculation false\" >> /root/spark/conf/spark-defaults.conf \\\n && rm -f /tmp/elasticsearch-hadoop-5.1.1.zip \\\n && rm -rf /root/elasticsearch-hadoop\n#   Install and add snappy-java and lzo-java to our classpath below via spark.jars\nRUN which wget &> /dev/null || apt-get install --no-install-recommends wget=1.20.3 ; wget --no-verbose --output-document /tmp/snappy-java-1.1.2.6.jar http://central.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar\nRUN which wget &> /dev/null || apt-get install --no-install-recommends wget=1.20.3 ; wget --no-verbose --output-document /tmp/lzo-hadoop-1.0.5.jar http://central.maven.org/maven2/org/anarres/lzo/lzo-hadoop/1.0.5/lzo-hadoop-1.0.5.jar\nRUN mv /tmp/snappy-java-1.1.2.6.jar /root/Agile_Data_Code_2/lib/ \\\n && mv /tmp/lzo-hadoop-1.0.5.jar /root/Agile_Data_Code_2/lib/\n#   Setup mongo and elasticsearch jars for Spark\nRUN echo \"spark.jars /root/Agile_Data_Code_2/lib/mongo-hadoop-spark-1.5.2.jar,/root/Agile_Data_Code_2/lib/mongo-java-driver-3.4.0.jar,/root/Agile_Data_Code_2/lib/mongo-hadoop-1.5.2.jar,/root/Agile_Data_Code_2/lib/elasticsearch-spark-20_2.10-5.1.1.jar,/root/Agile_Data_Code_2/lib/snappy-java-1.1.2.6.jar,/root/Agile_Data_Code_2/lib/lzo-hadoop-1.0.5.jar\" >> /root/spark/conf/spark-defaults.conf\n#\n#   Install and setup Kafka\n#\nRUN which wget &> /dev/null || apt-get install --no-install-recommends wget=1.20.3 ; wget --no-verbose --output-document /tmp/kafka_2.11-0.10.1.1.tgz http://www-us.apache.org/dist/kafka/0.10.1.1/kafka_2.11-0.10.1.1.tgz\nRUN mkdir -p /root/kafka \\\n && tar -xvzf /tmp/kafka_2.11-0.10.1.1.tgz -C kafka --strip-components=1 \\\n && rm -f /tmp/kafka_2.11-0.10.1.1.tgz\n#   Run zookeeper (which kafka depends on), then Kafka\nRUN /root/kafka/bin/zookeeper-server-start.sh -daemon /root/kafka/config/zookeeper.properties \\\n && /root/kafka/bin/kafka-server-start.sh -daemon /root/kafka/config/server.properties\n#\n#   Install and set up Airflow\n#\n#   Install Apache Incubating Airflow\nRUN pip install airflow==0.6 \\\n && mkdir /root/airflow \\\n && mkdir /root/airflow/dags \\\n && mkdir /root/airflow/logs \\\n && mkdir /root/airflow/plugins \\\n && airflow initdb \\\n && airflow webserver -D \\\n && airflow scheduler -D &\n#\n#   Install and setup Zeppelin\n#\nWORKDIR /root\nRUN which wget &> /dev/null || apt-get install --no-install-recommends wget=1.20.3 ; wget --no-verbose --output-document /tmp/zeppelin-0.6.2-bin-all.tgz http://www-us.apache.org/dist/zeppelin/zeppelin-0.6.2/zeppelin-0.6.2-bin-all.tgz\nRUN mkdir -p /root/zeppelin \\\n && tar -xvzf /tmp/zeppelin-0.6.2-bin-all.tgz -C zeppelin --strip-components=1 \\\n && rm -f /tmp/zeppelin-0.6.2-bin-all.tgz\n#   Configure Zeppelin\nRUN cp /root/zeppelin/conf/zeppelin-env.sh.template /root/zeppelin/conf/zeppelin-env.sh \\\n && echo \"export SPARK_HOME=/root/spark\" >> /root/zeppelin/conf/zeppelin-env.sh \\\n && echo \"export SPARK_MASTER=local\" >> /root/zeppelin/conf/zeppelin-env.sh \\\n && echo \"export SPARK_CLASSPATH=\" >> /root/zeppelin/conf/zeppelin-env.sh\n#\n#   Download the data\n#\nWORKDIR /root/Agile_Data_Code_2/data\n#   On-time performance records\nRUN which wget &> /dev/null || apt-get install --no-install-recommends wget=1.20.3 ; wget --no-verbose --output-document /root/Agile_Data_Code_2/data/On_Time_On_Time_Performance_2015.csv.gz http://s3.amazonaws.com/agile_data_science/On_Time_On_Time_Performance_2015.csv.gz\n#   Openflights data\nRUN which wget &> /dev/null || apt-get install --no-install-recommends wget=1.20.3 ; wget --no-verbose --output-document /root/Agile_Data_Code_2/data/airports.dat https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat\nRUN which wget &> /dev/null || apt-get install --no-install-recommends wget=1.20.3 ; wget --no-verbose --output-document /root/Agile_Data_Code_2/data/airlines.dat https://raw.githubusercontent.com/jpatokal/openflights/master/data/airlines.dat\nRUN which wget &> /dev/null || apt-get install --no-install-recommends wget=1.20.3 ; wget --no-verbose --output-document /root/Agile_Data_Code_2/data/routes.dat https://raw.githubusercontent.com/jpatokal/openflights/master/data/routes.dat\nRUN which wget &> /dev/null || apt-get install --no-install-recommends wget=1.20.3 ; wget --no-verbose --output-document /root/Agile_Data_Code_2/data/countries.dat https://raw.githubusercontent.com/jpatokal/openflights/master/data/countries.dat\n#   FAA data\nRUN which wget &> /dev/null || apt-get install --no-install-recommends wget=1.20.3 ; wget --no-verbose --output-document /root/Agile_Data_Code_2/data/aircraft.txt http://av-info.faa.gov/data/ACRef/tab/aircraft.txt\nRUN which wget &> /dev/null || apt-get install --no-install-recommends wget=1.20.3 ; wget --no-verbose --output-document /root/Agile_Data_Code_2/data/ata.txt http://av-info.faa.gov/data/ACRef/tab/ata.txt\nRUN which wget &> /dev/null || apt-get install --no-install-recommends wget=1.20.3 ; wget --no-verbose --output-document /root/Agile_Data_Code_2/data/compt.txt http://av-info.faa.gov/data/ACRef/tab/compt.txt\nRUN which wget &> /dev/null || apt-get install --no-install-recommends wget=1.20.3 ; wget --no-verbose --output-document /root/Agile_Data_Code_2/data/engine.txt http://av-info.faa.gov/data/ACRef/tab/engine.txt\nRUN which wget &> /dev/null || apt-get install --no-install-recommends wget=1.20.3 ; wget --no-verbose --output-document /root/Agile_Data_Code_2/data/prop.txt http://av-info.faa.gov/data/ACRef/tab/prop.txt\n#   WBAN Master List\nRUN which wget &> /dev/null || apt-get install --no-install-recommends wget=1.20.3 ; wget --no-verbose --output-document /tmp/wbanmasterlist.psv.zip http://www.ncdc.noaa.gov/homr/file/wbanmasterlist.psv.zip\nRUN for i in $( seq -w 1 12 ;); do curl -Lko /tmp/QCLCD2015${i}.zip http://www.ncdc.noaa.gov/orders/qclcd/QCLCD2015${i}.zip \\\n && unzip -o /tmp/QCLCD2015${i}.zip \\\n && gzip 2015${i}*.txt \\\n && rm -f /tmp/QCLCD2015${i}.zip ; done\n#  ADD https://www.ncdc.noaa.gov/orders/qclcd/QCLCD201501.zip /tmp/QCLCD201501.zip\n#  ADD https://www.ncdc.noaa.gov/orders/qclcd/QCLCD201502.zip /tmp/QCLCD201502.zip\n#  ADD https://www.ncdc.noaa.gov/orders/qclcd/QCLCD201503.zip /tmp/QCLCD201503.zip\n#  ADD https://www.ncdc.noaa.gov/orders/qclcd/QCLCD201504.zip /tmp/QCLCD201504.zip\n#  ADD https://www.ncdc.noaa.gov/orders/qclcd/QCLCD201505.zip /tmp/QCLCD201505.zip\n#  ADD https://www.ncdc.noaa.gov/orders/qclcd/QCLCD201506.zip /tmp/QCLCD201506.zip\n#  ADD https://www.ncdc.noaa.gov/orders/qclcd/QCLCD201507.zip /tmp/QCLCD201507.zip\n#  ADD https://www.ncdc.noaa.gov/orders/qclcd/QCLCD201508.zip /tmp/QCLCD201508.zip\n#  ADD https://www.ncdc.noaa.gov/orders/qclcd/QCLCD201509.zip /tmp/QCLCD201509.zip\n#  ADD https://www.ncdc.noaa.gov/orders/qclcd/QCLCD201510.zip /tmp/QCLCD201510.zip\n#  ADD https://www.ncdc.noaa.gov/orders/qclcd/QCLCD201511.zip /tmp/QCLCD201511.zip\n#  ADD https://www.ncdc.noaa.gov/orders/qclcd/QCLCD201512.zip /tmp/QCLCD201512.zip\n#\n#  RUN unzip -o /tmp/wbanmasterlist.psv.zip && \\\n#      gzip wbanmasterlist.psv && \\\n#      rm -f /tmp/wbanmasterlist.psv.zip && \\\n#      unzip -o /tmp/QCLCD201501.zip && \\\n#      gzip 201501*.txt && \\\n#      rm -f /tmp/QCLCD201501.zip && \\\n#      unzip -o /tmp/QCLCD201502.zip && \\\n#      gzip 201502*.txt && \\\n#      rm -f /tmp/QCLCD201502.zip && \\\n#      unzip -o /tmp/QCLCD201503.zip && \\\n#      gzip 201503*.txt && \\\n#      rm -f /tmp/QCLCD201503.zip && \\\n#      unzip -o /tmp/QCLCD201504.zip && \\\n#      gzip 201504*.txt && \\\n#      rm -f /tmp/QCLCD201504.zip && \\\n#      unzip -o /tmp/QCLCD201505.zip && \\\n#      gzip 201505*.txt && \\\n#      rm -f /tmp/QCLCD201505.zip && \\\n#      unzip -o /tmp/QCLCD201506.zip && \\\n#      gzip 201506*.txt && \\\n#      rm -f /tmp/QCLCD201506.zip && \\\n#      unzip -o /tmp/QCLCD201507.zip && \\\n#      gzip 201507*.txt && \\\n#      rm -f /tmp/QCLCD201507.zip && \\\n#      unzip -o /tmp/QCLCD201508.zip && \\\n#      gzip 201508*.txt && \\\n#      rm -f /tmp/QCLCD201508.zip && \\\n#      unzip -o /tmp/QCLCD201509.zip && \\\n#      gzip 201509*.txt && \\\n#      rm -f /tmp/QCLCD201509.zip && \\\n#      unzip -o /tmp/QCLCD201510.zip && \\\n#      gzip 201510*.txt && \\\n#      rm -f /tmp/QCLCD201510.zip && \\\n#      unzip -o /tmp/QCLCD201511.zip && \\\n#      gzip 201511*.txt && \\\n#      rm -f /tmp/QCLCD201511.zip && \\\n#      unzip -o /tmp/QCLCD201512.zip && \\\n#      gzip 201512*.txt && \\\n#      rm -f /tmp/QCLCD201512.zip\n#   Back to /root\nWORKDIR /root\n#   Cleanup\nRUN apt-get clean \\\n && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*\n#   Done!\nRUN groupadd --system docker-user ; useradd --system --gid docker-user docker-user\nUSER docker-user\n# Please add your HEALTHCHECK here!!!\n","originalDockerfile":"#  Setup an environment for running this book's examples\nFROM ubuntu\nMAINTAINER Russell Jurney, russell.jurney@gmail.com\nWORKDIR /root\n#  Update apt-get and install things\nRUN apt-get autoclean\nRUN apt-get update \\\n && apt-get install zip unzip curl bzip2 python-dev build-essential git libssl1.0.0 libssl-dev -y\n#  Setup Oracle Java8\nRUN apt-get install software-properties-common debconf-utils -y \\\n && add-apt-repository -y ppa:webupd8team/java \\\n && apt-get update \\\n && echo \"oracle-java8-installer shared/accepted-oracle-license-v1-1 select true\" | debconf-set-selections \\\n && apt-get install oracle-java8-installer -y\nENV JAVA_HOME=\"/usr/lib/jvm/java-8-oracle\"\n#  Download and install Anaconda Python\nADD http://repo.continuum.io/archive/Anaconda3-4.2.0-Linux-x86_64.sh /tmp/Anaconda3-4.2.0-Linux-x86_64.sh\nRUN bash /tmp/Anaconda3-4.2.0-Linux-x86_64.sh -b -p /root/anaconda\nENV PATH=\"/root/anaconda/bin:$PATH\"\n#\n#  Install git, clone repo, install Python dependencies\n#\nRUN git clone https://github.com/rjurney/Agile_Data_Code_2\nWORKDIR /root/Agile_Data_Code_2\nENV PROJECT_HOME=\"/Agile_Data_Code_2\"\nRUN pip install pip --upgrade \\\n && pip install -r requirements.txt\nWORKDIR /root\n#\n#  Install Hadoop: may need to update this link... see http://hadoop.apache.org/releases.html\n#\nADD http://apache.osuosl.org/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz /tmp/hadoop-2.7.3.tar.gz\nRUN mkdir -p /root/hadoop \\\n && tar -xvf /tmp/hadoop-2.7.3.tar.gz -C hadoop --strip-components=1\nENV HADOOP_HOME=\"/root/hadoop\"\nENV PATH=\"$PATH:$HADOOP_HOME/bin\"\nENV HADOOP_CLASSPATH=\"/root/hadoop/etc/hadoop/:/root/hadoop/share/hadoop/common/lib/*:/root/hadoop/share/hadoop/common/*:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/*:/root/hadoop/share/hadoop/hdfs/*:/root/hadoop/share/hadoop/yarn/lib/*:/root/hadoop/share/hadoop/yarn/*:/root/hadoop/share/hadoop/mapreduce/lib/*:/root/hadoop/share/hadoop/mapreduce/*:/root/hadoop/etc/hadoop:/root/hadoop/share/hadoop/common/lib/*:/root/hadoop/share/hadoop/common/*:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/*:/root/hadoop/share/hadoop/hdfs/*:/root/hadoop/share/hadoop/yarn/lib/*:/root/hadoop/share/hadoop/yarn/*:/root/hadoop/share/hadoop/mapreduce/lib/*:/root/hadoop/share/hadoop/mapreduce/*:/root/hadoop/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar\"\nENV HADOOP_CONF_DIR=\"/root/hadoop/etc/hadoop\"\n#\n#  Install Spark: may need to update this link... see http://spark.apache.org/downloads.html\n#\nADD http://d3kbcqa49mib13.cloudfront.net/spark-2.1.0-bin-without-hadoop.tgz /tmp/spark-2.1.0-bin-without-hadoop.tgz\nRUN mkdir -p /root/spark \\\n && tar -xvf /tmp/spark-2.1.0-bin-without-hadoop.tgz -C spark --strip-components=1\nENV SPARK_HOME=\"/root/spark\"\nENV HADOOP_CONF_DIR=\"/root/hadoop/etc/hadoop/\"\nENV SPARK_DIST_CLASSPATH=\"/root/hadoop/etc/hadoop/:/root/hadoop/share/hadoop/common/lib/*:/root/hadoop/share/hadoop/common/*:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/*:/root/hadoop/share/hadoop/hdfs/*:/root/hadoop/share/hadoop/yarn/lib/*:/root/hadoop/share/hadoop/yarn/*:/root/hadoop/share/hadoop/mapreduce/lib/*:/root/hadoop/share/hadoop/mapreduce/*:/root/hadoop/etc/hadoop:/root/hadoop/share/hadoop/common/lib/*:/root/hadoop/share/hadoop/common/*:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/*:/root/hadoop/share/hadoop/hdfs/*:/root/hadoop/share/hadoop/yarn/lib/*:/root/hadoop/share/hadoop/yarn/*:/root/hadoop/share/hadoop/mapreduce/lib/*:/root/hadoop/share/hadoop/mapreduce/*:/root/hadoop/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar\"\nENV PATH=\"$PATH:/root/spark/bin\"\n#  Have to set spark.io.compression.codec in Spark local mode, give 8GB RAM\nRUN cp /root/spark/conf/spark-defaults.conf.template /root/spark/conf/spark-defaults.conf \\\n && echo 'spark.io.compression.codec org.apache.spark.io.SnappyCompressionCodec' >> /root/spark/conf/spark-defaults.conf \\\n && echo \"spark.driver.memory 8g\" >> /root/spark/conf/spark-defaults.conf\n#  Setup spark-env.sh to use Python 3\nRUN echo \"PYSPARK_PYTHON=python3\" >> /root/spark/conf/spark-env.sh \\\n && echo \"PYSPARK_DRIVER_PYTHON=python3\" >> /root/spark/conf/spark-env.sh\n#  Setup log4j config to reduce logging output\nRUN cp /root/spark/conf/log4j.properties.template /root/spark/conf/log4j.properties \\\n && sed -i 's/INFO/ERROR/g' /root/spark/conf/log4j.properties\n#\n#  Install Mongo, Mongo Java driver, and mongo-hadoop and start MongoDB\n#\nRUN echo \"deb [ arch=amd64,arm64 ] http://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.4 multiverse\" > /etc/apt/sources.list.d/mongodb-org-3.4.list\nRUN apt-get update \\\n && apt-get install mongodb-org -y --allow-unauthenticated \\\n && mkdir -p /data/db\n#  apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 0C49F3730359A14518585931BC711F9BA15703C6 && \\\nRUN /usr/bin/mongod --fork --logpath /var/log/mongodb.log\n#  Get the MongoDB Java Driver and put it in Agile_Data_Code_2\nADD http://central.maven.org/maven2/org/mongodb/mongo-java-driver/3.4.0/mongo-java-driver-3.4.0.jar /tmp/mongo-java-driver-3.4.0.jar\nRUN mv /tmp/mongo-java-driver-3.4.0.jar /root/Agile_Data_Code_2/lib/\n#  Install the mongo-hadoop project in the mongo-hadoop directory in the root of our project.\nADD https://github.com/mongodb/mongo-hadoop/archive/r1.5.2.tar.gz /tmp/mongo-hadoop-r1.5.2.tar.gz\nRUN mkdir -p /root/mongo-hadoop \\\n && tar -xvzf /tmp/mongo-hadoop-r1.5.2.tar.gz -C mongo-hadoop --strip-components=1 \\\n && rm -f /tmp/mongo-hadoop-r1.5.2.tar.gz\nWORKDIR /root/mongo-hadoop\nRUN /root/mongo-hadoop/gradlew jar\nWORKDIR /root\nRUN cp /root/mongo-hadoop/spark/build/libs/mongo-hadoop-spark-*.jar /root/Agile_Data_Code_2/lib/ \\\n && cp /root/mongo-hadoop/build/libs/mongo-hadoop-*.jar /root/Agile_Data_Code_2/lib/\n#  Install pymongo_spark\nWORKDIR /root/mongo-hadoop/spark/src/main/python\nRUN python setup.py install\nWORKDIR /root\nRUN cp /root/mongo-hadoop/spark/src/main/python/pymongo_spark.py /root/Agile_Data_Code_2/lib/\nENV PYTHONPATH=\"$PYTHONPATH:/root/Agile_Data_Code_2/lib\"\n#  Cleanup mongo-hadoop\nRUN rm -rf /root/mongo-hadoop\n#\n#  Install ElasticSearch in the elasticsearch directory in the root of our project, and the Elasticsearch for Hadoop package\n#\nADD https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.1.1.tar.gz /tmp/elasticsearch-5.1.1.tar.gz\nRUN mkdir /root/elasticsearch \\\n && tar -xvzf /tmp/elasticsearch-5.1.1.tar.gz -C elasticsearch --strip-components=1 \\\n && /root/elasticsearch/bin/elasticsearch -d \\\n && rm -f /tmp/elasticsearch-5.1.1.tar.gz\n#  Install Elasticsearch for Hadoop\nADD http://download.elastic.co/hadoop/elasticsearch-hadoop-5.1.1.zip /tmp/elasticsearch-hadoop-5.1.1.zip\nRUN unzip /tmp/elasticsearch-hadoop-5.1.1.zip \\\n && mv /root/elasticsearch-hadoop-5.1.1 /root/elasticsearch-hadoop \\\n && cp /root/elasticsearch-hadoop/dist/elasticsearch-hadoop-5.1.1.jar /root/Agile_Data_Code_2/lib/ \\\n && cp /root/elasticsearch-hadoop/dist/elasticsearch-spark-20_2.10-5.1.1.jar /root/Agile_Data_Code_2/lib/ \\\n && echo \"spark.speculation false\" >> /root/spark/conf/spark-defaults.conf \\\n && rm -f /tmp/elasticsearch-hadoop-5.1.1.zip \\\n && rm -rf /root/elasticsearch-hadoop\n#  Install and add snappy-java and lzo-java to our classpath below via spark.jars\nADD http://central.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar /tmp/snappy-java-1.1.2.6.jar\nADD http://central.maven.org/maven2/org/anarres/lzo/lzo-hadoop/1.0.5/lzo-hadoop-1.0.5.jar /tmp/lzo-hadoop-1.0.5.jar\nRUN mv /tmp/snappy-java-1.1.2.6.jar /root/Agile_Data_Code_2/lib/ \\\n && mv /tmp/lzo-hadoop-1.0.5.jar /root/Agile_Data_Code_2/lib/\n#  Setup mongo and elasticsearch jars for Spark\nRUN echo \"spark.jars /root/Agile_Data_Code_2/lib/mongo-hadoop-spark-1.5.2.jar,/root/Agile_Data_Code_2/lib/mongo-java-driver-3.4.0.jar,/root/Agile_Data_Code_2/lib/mongo-hadoop-1.5.2.jar,/root/Agile_Data_Code_2/lib/elasticsearch-spark-20_2.10-5.1.1.jar,/root/Agile_Data_Code_2/lib/snappy-java-1.1.2.6.jar,/root/Agile_Data_Code_2/lib/lzo-hadoop-1.0.5.jar\" >> /root/spark/conf/spark-defaults.conf\n#\n#  Install and setup Kafka\n#\nADD http://www-us.apache.org/dist/kafka/0.10.1.1/kafka_2.11-0.10.1.1.tgz /tmp/kafka_2.11-0.10.1.1.tgz\nRUN mkdir -p /root/kafka \\\n && tar -xvzf /tmp/kafka_2.11-0.10.1.1.tgz -C kafka --strip-components=1 \\\n && rm -f /tmp/kafka_2.11-0.10.1.1.tgz\n#  Run zookeeper (which kafka depends on), then Kafka\nRUN /root/kafka/bin/zookeeper-server-start.sh -daemon /root/kafka/config/zookeeper.properties \\\n && /root/kafka/bin/kafka-server-start.sh -daemon /root/kafka/config/server.properties\n#\n#  Install and set up Airflow\n#\n#  Install Apache Incubating Airflow\nRUN pip install airflow \\\n && mkdir /root/airflow \\\n && mkdir /root/airflow/dags \\\n && mkdir /root/airflow/logs \\\n && mkdir /root/airflow/plugins \\\n && airflow initdb \\\n && airflow webserver -D \\\n && airflow scheduler -D &\n#\n#  Install and setup Zeppelin\n#\nWORKDIR /root\nADD http://www-us.apache.org/dist/zeppelin/zeppelin-0.6.2/zeppelin-0.6.2-bin-all.tgz /tmp/zeppelin-0.6.2-bin-all.tgz\nRUN mkdir -p /root/zeppelin \\\n && tar -xvzf /tmp/zeppelin-0.6.2-bin-all.tgz -C zeppelin --strip-components=1 \\\n && rm -f /tmp/zeppelin-0.6.2-bin-all.tgz\n#  Configure Zeppelin\nRUN cp /root/zeppelin/conf/zeppelin-env.sh.template /root/zeppelin/conf/zeppelin-env.sh \\\n && echo \"export SPARK_HOME=/root/spark\" >> /root/zeppelin/conf/zeppelin-env.sh \\\n && echo \"export SPARK_MASTER=local\" >> /root/zeppelin/conf/zeppelin-env.sh \\\n && echo \"export SPARK_CLASSPATH=\" >> /root/zeppelin/conf/zeppelin-env.sh\n#\n#  Download the data\n#\nWORKDIR /root/Agile_Data_Code_2/data\n#  On-time performance records\nADD http://s3.amazonaws.com/agile_data_science/On_Time_On_Time_Performance_2015.csv.gz /root/Agile_Data_Code_2/data/On_Time_On_Time_Performance_2015.csv.gz\n#  Openflights data\nADD https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat /root/Agile_Data_Code_2/data/airports.dat\nADD https://raw.githubusercontent.com/jpatokal/openflights/master/data/airlines.dat /root/Agile_Data_Code_2/data/airlines.dat\nADD https://raw.githubusercontent.com/jpatokal/openflights/master/data/routes.dat /root/Agile_Data_Code_2/data/routes.dat\nADD https://raw.githubusercontent.com/jpatokal/openflights/master/data/countries.dat /root/Agile_Data_Code_2/data/countries.dat\n#  FAA data\nADD http://av-info.faa.gov/data/ACRef/tab/aircraft.txt /root/Agile_Data_Code_2/data/aircraft.txt\nADD http://av-info.faa.gov/data/ACRef/tab/ata.txt /root/Agile_Data_Code_2/data/ata.txt\nADD http://av-info.faa.gov/data/ACRef/tab/compt.txt /root/Agile_Data_Code_2/data/compt.txt\nADD http://av-info.faa.gov/data/ACRef/tab/engine.txt /root/Agile_Data_Code_2/data/engine.txt\nADD http://av-info.faa.gov/data/ACRef/tab/prop.txt /root/Agile_Data_Code_2/data/prop.txt\n#  WBAN Master List\nADD http://www.ncdc.noaa.gov/homr/file/wbanmasterlist.psv.zip /tmp/wbanmasterlist.psv.zip\nRUN for i in $( seq -w 1 12 ;); do curl -Lko /tmp/QCLCD2015${i}.zip http://www.ncdc.noaa.gov/orders/qclcd/QCLCD2015${i}.zip \\\n && unzip -o /tmp/QCLCD2015${i}.zip \\\n && gzip 2015${i}*.txt \\\n && rm -f /tmp/QCLCD2015${i}.zip ; done\n# ADD https://www.ncdc.noaa.gov/orders/qclcd/QCLCD201501.zip /tmp/QCLCD201501.zip\n# ADD https://www.ncdc.noaa.gov/orders/qclcd/QCLCD201502.zip /tmp/QCLCD201502.zip\n# ADD https://www.ncdc.noaa.gov/orders/qclcd/QCLCD201503.zip /tmp/QCLCD201503.zip\n# ADD https://www.ncdc.noaa.gov/orders/qclcd/QCLCD201504.zip /tmp/QCLCD201504.zip\n# ADD https://www.ncdc.noaa.gov/orders/qclcd/QCLCD201505.zip /tmp/QCLCD201505.zip\n# ADD https://www.ncdc.noaa.gov/orders/qclcd/QCLCD201506.zip /tmp/QCLCD201506.zip\n# ADD https://www.ncdc.noaa.gov/orders/qclcd/QCLCD201507.zip /tmp/QCLCD201507.zip\n# ADD https://www.ncdc.noaa.gov/orders/qclcd/QCLCD201508.zip /tmp/QCLCD201508.zip\n# ADD https://www.ncdc.noaa.gov/orders/qclcd/QCLCD201509.zip /tmp/QCLCD201509.zip\n# ADD https://www.ncdc.noaa.gov/orders/qclcd/QCLCD201510.zip /tmp/QCLCD201510.zip\n# ADD https://www.ncdc.noaa.gov/orders/qclcd/QCLCD201511.zip /tmp/QCLCD201511.zip\n# ADD https://www.ncdc.noaa.gov/orders/qclcd/QCLCD201512.zip /tmp/QCLCD201512.zip\n#\n# RUN unzip -o /tmp/wbanmasterlist.psv.zip && \\\n#     gzip wbanmasterlist.psv && \\\n#     rm -f /tmp/wbanmasterlist.psv.zip && \\\n#     unzip -o /tmp/QCLCD201501.zip && \\\n#     gzip 201501*.txt && \\\n#     rm -f /tmp/QCLCD201501.zip && \\\n#     unzip -o /tmp/QCLCD201502.zip && \\\n#     gzip 201502*.txt && \\\n#     rm -f /tmp/QCLCD201502.zip && \\\n#     unzip -o /tmp/QCLCD201503.zip && \\\n#     gzip 201503*.txt && \\\n#     rm -f /tmp/QCLCD201503.zip && \\\n#     unzip -o /tmp/QCLCD201504.zip && \\\n#     gzip 201504*.txt && \\\n#     rm -f /tmp/QCLCD201504.zip && \\\n#     unzip -o /tmp/QCLCD201505.zip && \\\n#     gzip 201505*.txt && \\\n#     rm -f /tmp/QCLCD201505.zip && \\\n#     unzip -o /tmp/QCLCD201506.zip && \\\n#     gzip 201506*.txt && \\\n#     rm -f /tmp/QCLCD201506.zip && \\\n#     unzip -o /tmp/QCLCD201507.zip && \\\n#     gzip 201507*.txt && \\\n#     rm -f /tmp/QCLCD201507.zip && \\\n#     unzip -o /tmp/QCLCD201508.zip && \\\n#     gzip 201508*.txt && \\\n#     rm -f /tmp/QCLCD201508.zip && \\\n#     unzip -o /tmp/QCLCD201509.zip && \\\n#     gzip 201509*.txt && \\\n#     rm -f /tmp/QCLCD201509.zip && \\\n#     unzip -o /tmp/QCLCD201510.zip && \\\n#     gzip 201510*.txt && \\\n#     rm -f /tmp/QCLCD201510.zip && \\\n#     unzip -o /tmp/QCLCD201511.zip && \\\n#     gzip 201511*.txt && \\\n#     rm -f /tmp/QCLCD201511.zip && \\\n#     unzip -o /tmp/QCLCD201512.zip && \\\n#     gzip 201512*.txt && \\\n#     rm -f /tmp/QCLCD201512.zip\n#  Back to /root\nWORKDIR /root\n#  Cleanup\nRUN apt-get clean \\\n && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*\n#  Done!\n","injectedSmells":[],"originalDockerfileHash":"56a91948e564cea13dbba95ee81ebde2","successfullyInjectedSmells":[],"originalDockerfileUglified":"#   Setup an environment for running this book's examples\nFROM ubuntu\nMAINTAINER Russell Jurney, russell.jurney@gmail.com\nWORKDIR /root\n#   Update apt-get and install things\nRUN apt-get autoclean\nRUN apt-get update \\\n && apt-get install zip unzip curl bzip2 python-dev build-essential git libssl1.0.0 libssl-dev -y\n#   Setup Oracle Java8\nRUN apt-get install software-properties-common debconf-utils -y \\\n && add-apt-repository -y ppa:webupd8team/java \\\n && apt-get update \\\n && echo \"oracle-java8-installer shared/accepted-oracle-license-v1-1 select true\" | debconf-set-selections \\\n && apt-get install oracle-java8-installer -y\nENV JAVA_HOME=\"/usr/lib/jvm/java-8-oracle\"\n#   Download and install Anaconda Python\nADD http://repo.continuum.io/archive/Anaconda3-4.2.0-Linux-x86_64.sh /tmp/Anaconda3-4.2.0-Linux-x86_64.sh\nRUN bash /tmp/Anaconda3-4.2.0-Linux-x86_64.sh -b -p /root/anaconda\nENV PATH=\"/root/anaconda/bin:$PATH\"\n#\n#   Install git, clone repo, install Python dependencies\n#\nRUN git clone https://github.com/rjurney/Agile_Data_Code_2\nWORKDIR /root/Agile_Data_Code_2\nENV PROJECT_HOME=\"/Agile_Data_Code_2\"\nRUN pip install pip --upgrade \\\n && pip install -r requirements.txt\nWORKDIR /root\n#\n#   Install Hadoop: may need to update this link... see http://hadoop.apache.org/releases.html\n#\nADD http://apache.osuosl.org/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz /tmp/hadoop-2.7.3.tar.gz\nRUN mkdir -p /root/hadoop \\\n && tar -xvf /tmp/hadoop-2.7.3.tar.gz -C hadoop --strip-components=1\nENV HADOOP_HOME=\"/root/hadoop\"\nENV PATH=\"$PATH:$HADOOP_HOME/bin\"\nENV HADOOP_CLASSPATH=\"/root/hadoop/etc/hadoop/:/root/hadoop/share/hadoop/common/lib/*:/root/hadoop/share/hadoop/common/*:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/*:/root/hadoop/share/hadoop/hdfs/*:/root/hadoop/share/hadoop/yarn/lib/*:/root/hadoop/share/hadoop/yarn/*:/root/hadoop/share/hadoop/mapreduce/lib/*:/root/hadoop/share/hadoop/mapreduce/*:/root/hadoop/etc/hadoop:/root/hadoop/share/hadoop/common/lib/*:/root/hadoop/share/hadoop/common/*:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/*:/root/hadoop/share/hadoop/hdfs/*:/root/hadoop/share/hadoop/yarn/lib/*:/root/hadoop/share/hadoop/yarn/*:/root/hadoop/share/hadoop/mapreduce/lib/*:/root/hadoop/share/hadoop/mapreduce/*:/root/hadoop/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar\"\nENV HADOOP_CONF_DIR=\"/root/hadoop/etc/hadoop\"\n#\n#   Install Spark: may need to update this link... see http://spark.apache.org/downloads.html\n#\nADD http://d3kbcqa49mib13.cloudfront.net/spark-2.1.0-bin-without-hadoop.tgz /tmp/spark-2.1.0-bin-without-hadoop.tgz\nRUN mkdir -p /root/spark \\\n && tar -xvf /tmp/spark-2.1.0-bin-without-hadoop.tgz -C spark --strip-components=1\nENV SPARK_HOME=\"/root/spark\"\nENV HADOOP_CONF_DIR=\"/root/hadoop/etc/hadoop/\"\nENV SPARK_DIST_CLASSPATH=\"/root/hadoop/etc/hadoop/:/root/hadoop/share/hadoop/common/lib/*:/root/hadoop/share/hadoop/common/*:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/*:/root/hadoop/share/hadoop/hdfs/*:/root/hadoop/share/hadoop/yarn/lib/*:/root/hadoop/share/hadoop/yarn/*:/root/hadoop/share/hadoop/mapreduce/lib/*:/root/hadoop/share/hadoop/mapreduce/*:/root/hadoop/etc/hadoop:/root/hadoop/share/hadoop/common/lib/*:/root/hadoop/share/hadoop/common/*:/root/hadoop/share/hadoop/hdfs:/root/hadoop/share/hadoop/hdfs/lib/*:/root/hadoop/share/hadoop/hdfs/*:/root/hadoop/share/hadoop/yarn/lib/*:/root/hadoop/share/hadoop/yarn/*:/root/hadoop/share/hadoop/mapreduce/lib/*:/root/hadoop/share/hadoop/mapreduce/*:/root/hadoop/contrib/capacity-scheduler/*.jar:/root/hadoop/contrib/capacity-scheduler/*.jar\"\nENV PATH=\"$PATH:/root/spark/bin\"\n#   Have to set spark.io.compression.codec in Spark local mode, give 8GB RAM\nRUN cp /root/spark/conf/spark-defaults.conf.template /root/spark/conf/spark-defaults.conf \\\n && echo 'spark.io.compression.codec org.apache.spark.io.SnappyCompressionCodec' >> /root/spark/conf/spark-defaults.conf \\\n && echo \"spark.driver.memory 8g\" >> /root/spark/conf/spark-defaults.conf\n#   Setup spark-env.sh to use Python 3\nRUN echo \"PYSPARK_PYTHON=python3\" >> /root/spark/conf/spark-env.sh \\\n && echo \"PYSPARK_DRIVER_PYTHON=python3\" >> /root/spark/conf/spark-env.sh\n#   Setup log4j config to reduce logging output\nRUN cp /root/spark/conf/log4j.properties.template /root/spark/conf/log4j.properties \\\n && sed -i 's/INFO/ERROR/g' /root/spark/conf/log4j.properties\n#\n#   Install Mongo, Mongo Java driver, and mongo-hadoop and start MongoDB\n#\nRUN echo \"deb [ arch=amd64,arm64 ] http://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.4 multiverse\" > /etc/apt/sources.list.d/mongodb-org-3.4.list\nRUN apt-get update \\\n && apt-get install mongodb-org -y --allow-unauthenticated \\\n && mkdir -p /data/db\n#   apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 0C49F3730359A14518585931BC711F9BA15703C6 && \\\nRUN /usr/bin/mongod --fork --logpath /var/log/mongodb.log\n#   Get the MongoDB Java Driver and put it in Agile_Data_Code_2\nADD http://central.maven.org/maven2/org/mongodb/mongo-java-driver/3.4.0/mongo-java-driver-3.4.0.jar /tmp/mongo-java-driver-3.4.0.jar\nRUN mv /tmp/mongo-java-driver-3.4.0.jar /root/Agile_Data_Code_2/lib/\n#   Install the mongo-hadoop project in the mongo-hadoop directory in the root of our project.\nADD https://github.com/mongodb/mongo-hadoop/archive/r1.5.2.tar.gz /tmp/mongo-hadoop-r1.5.2.tar.gz\nRUN mkdir -p /root/mongo-hadoop \\\n && tar -xvzf /tmp/mongo-hadoop-r1.5.2.tar.gz -C mongo-hadoop --strip-components=1 \\\n && rm -f /tmp/mongo-hadoop-r1.5.2.tar.gz\nWORKDIR /root/mongo-hadoop\nRUN /root/mongo-hadoop/gradlew jar\nWORKDIR /root\nRUN cp /root/mongo-hadoop/spark/build/libs/mongo-hadoop-spark-*.jar /root/Agile_Data_Code_2/lib/ \\\n && cp /root/mongo-hadoop/build/libs/mongo-hadoop-*.jar /root/Agile_Data_Code_2/lib/\n#   Install pymongo_spark\nWORKDIR /root/mongo-hadoop/spark/src/main/python\nRUN python setup.py install\nWORKDIR /root\nRUN cp /root/mongo-hadoop/spark/src/main/python/pymongo_spark.py /root/Agile_Data_Code_2/lib/\nENV PYTHONPATH=\"$PYTHONPATH:/root/Agile_Data_Code_2/lib\"\n#   Cleanup mongo-hadoop\nRUN rm -rf /root/mongo-hadoop\n#\n#   Install ElasticSearch in the elasticsearch directory in the root of our project, and the Elasticsearch for Hadoop package\n#\nADD https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.1.1.tar.gz /tmp/elasticsearch-5.1.1.tar.gz\nRUN mkdir /root/elasticsearch \\\n && tar -xvzf /tmp/elasticsearch-5.1.1.tar.gz -C elasticsearch --strip-components=1 \\\n && /root/elasticsearch/bin/elasticsearch -d \\\n && rm -f /tmp/elasticsearch-5.1.1.tar.gz\n#   Install Elasticsearch for Hadoop\nADD http://download.elastic.co/hadoop/elasticsearch-hadoop-5.1.1.zip /tmp/elasticsearch-hadoop-5.1.1.zip\nRUN unzip /tmp/elasticsearch-hadoop-5.1.1.zip \\\n && mv /root/elasticsearch-hadoop-5.1.1 /root/elasticsearch-hadoop \\\n && cp /root/elasticsearch-hadoop/dist/elasticsearch-hadoop-5.1.1.jar /root/Agile_Data_Code_2/lib/ \\\n && cp /root/elasticsearch-hadoop/dist/elasticsearch-spark-20_2.10-5.1.1.jar /root/Agile_Data_Code_2/lib/ \\\n && echo \"spark.speculation false\" >> /root/spark/conf/spark-defaults.conf \\\n && rm -f /tmp/elasticsearch-hadoop-5.1.1.zip \\\n && rm -rf /root/elasticsearch-hadoop\n#   Install and add snappy-java and lzo-java to our classpath below via spark.jars\nADD http://central.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar /tmp/snappy-java-1.1.2.6.jar\nADD http://central.maven.org/maven2/org/anarres/lzo/lzo-hadoop/1.0.5/lzo-hadoop-1.0.5.jar /tmp/lzo-hadoop-1.0.5.jar\nRUN mv /tmp/snappy-java-1.1.2.6.jar /root/Agile_Data_Code_2/lib/ \\\n && mv /tmp/lzo-hadoop-1.0.5.jar /root/Agile_Data_Code_2/lib/\n#   Setup mongo and elasticsearch jars for Spark\nRUN echo \"spark.jars /root/Agile_Data_Code_2/lib/mongo-hadoop-spark-1.5.2.jar,/root/Agile_Data_Code_2/lib/mongo-java-driver-3.4.0.jar,/root/Agile_Data_Code_2/lib/mongo-hadoop-1.5.2.jar,/root/Agile_Data_Code_2/lib/elasticsearch-spark-20_2.10-5.1.1.jar,/root/Agile_Data_Code_2/lib/snappy-java-1.1.2.6.jar,/root/Agile_Data_Code_2/lib/lzo-hadoop-1.0.5.jar\" >> /root/spark/conf/spark-defaults.conf\n#\n#   Install and setup Kafka\n#\nADD http://www-us.apache.org/dist/kafka/0.10.1.1/kafka_2.11-0.10.1.1.tgz /tmp/kafka_2.11-0.10.1.1.tgz\nRUN mkdir -p /root/kafka \\\n && tar -xvzf /tmp/kafka_2.11-0.10.1.1.tgz -C kafka --strip-components=1 \\\n && rm -f /tmp/kafka_2.11-0.10.1.1.tgz\n#   Run zookeeper (which kafka depends on), then Kafka\nRUN /root/kafka/bin/zookeeper-server-start.sh -daemon /root/kafka/config/zookeeper.properties \\\n && /root/kafka/bin/kafka-server-start.sh -daemon /root/kafka/config/server.properties\n#\n#   Install and set up Airflow\n#\n#   Install Apache Incubating Airflow\nRUN pip install airflow \\\n && mkdir /root/airflow \\\n && mkdir /root/airflow/dags \\\n && mkdir /root/airflow/logs \\\n && mkdir /root/airflow/plugins \\\n && airflow initdb \\\n && airflow webserver -D \\\n && airflow scheduler -D &\n#\n#   Install and setup Zeppelin\n#\nWORKDIR /root\nADD http://www-us.apache.org/dist/zeppelin/zeppelin-0.6.2/zeppelin-0.6.2-bin-all.tgz /tmp/zeppelin-0.6.2-bin-all.tgz\nRUN mkdir -p /root/zeppelin \\\n && tar -xvzf /tmp/zeppelin-0.6.2-bin-all.tgz -C zeppelin --strip-components=1 \\\n && rm -f /tmp/zeppelin-0.6.2-bin-all.tgz\n#   Configure Zeppelin\nRUN cp /root/zeppelin/conf/zeppelin-env.sh.template /root/zeppelin/conf/zeppelin-env.sh \\\n && echo \"export SPARK_HOME=/root/spark\" >> /root/zeppelin/conf/zeppelin-env.sh \\\n && echo \"export SPARK_MASTER=local\" >> /root/zeppelin/conf/zeppelin-env.sh \\\n && echo \"export SPARK_CLASSPATH=\" >> /root/zeppelin/conf/zeppelin-env.sh\n#\n#   Download the data\n#\nWORKDIR /root/Agile_Data_Code_2/data\n#   On-time performance records\nADD http://s3.amazonaws.com/agile_data_science/On_Time_On_Time_Performance_2015.csv.gz /root/Agile_Data_Code_2/data/On_Time_On_Time_Performance_2015.csv.gz\n#   Openflights data\nADD https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat /root/Agile_Data_Code_2/data/airports.dat\nADD https://raw.githubusercontent.com/jpatokal/openflights/master/data/airlines.dat /root/Agile_Data_Code_2/data/airlines.dat\nADD https://raw.githubusercontent.com/jpatokal/openflights/master/data/routes.dat /root/Agile_Data_Code_2/data/routes.dat\nADD https://raw.githubusercontent.com/jpatokal/openflights/master/data/countries.dat /root/Agile_Data_Code_2/data/countries.dat\n#   FAA data\nADD http://av-info.faa.gov/data/ACRef/tab/aircraft.txt /root/Agile_Data_Code_2/data/aircraft.txt\nADD http://av-info.faa.gov/data/ACRef/tab/ata.txt /root/Agile_Data_Code_2/data/ata.txt\nADD http://av-info.faa.gov/data/ACRef/tab/compt.txt /root/Agile_Data_Code_2/data/compt.txt\nADD http://av-info.faa.gov/data/ACRef/tab/engine.txt /root/Agile_Data_Code_2/data/engine.txt\nADD http://av-info.faa.gov/data/ACRef/tab/prop.txt /root/Agile_Data_Code_2/data/prop.txt\n#   WBAN Master List\nADD http://www.ncdc.noaa.gov/homr/file/wbanmasterlist.psv.zip /tmp/wbanmasterlist.psv.zip\nRUN for i in $( seq -w 1 12 ;); do curl -Lko /tmp/QCLCD2015${i}.zip http://www.ncdc.noaa.gov/orders/qclcd/QCLCD2015${i}.zip \\\n && unzip -o /tmp/QCLCD2015${i}.zip \\\n && gzip 2015${i}*.txt \\\n && rm -f /tmp/QCLCD2015${i}.zip ; done\n#  ADD https://www.ncdc.noaa.gov/orders/qclcd/QCLCD201501.zip /tmp/QCLCD201501.zip\n#  ADD https://www.ncdc.noaa.gov/orders/qclcd/QCLCD201502.zip /tmp/QCLCD201502.zip\n#  ADD https://www.ncdc.noaa.gov/orders/qclcd/QCLCD201503.zip /tmp/QCLCD201503.zip\n#  ADD https://www.ncdc.noaa.gov/orders/qclcd/QCLCD201504.zip /tmp/QCLCD201504.zip\n#  ADD https://www.ncdc.noaa.gov/orders/qclcd/QCLCD201505.zip /tmp/QCLCD201505.zip\n#  ADD https://www.ncdc.noaa.gov/orders/qclcd/QCLCD201506.zip /tmp/QCLCD201506.zip\n#  ADD https://www.ncdc.noaa.gov/orders/qclcd/QCLCD201507.zip /tmp/QCLCD201507.zip\n#  ADD https://www.ncdc.noaa.gov/orders/qclcd/QCLCD201508.zip /tmp/QCLCD201508.zip\n#  ADD https://www.ncdc.noaa.gov/orders/qclcd/QCLCD201509.zip /tmp/QCLCD201509.zip\n#  ADD https://www.ncdc.noaa.gov/orders/qclcd/QCLCD201510.zip /tmp/QCLCD201510.zip\n#  ADD https://www.ncdc.noaa.gov/orders/qclcd/QCLCD201511.zip /tmp/QCLCD201511.zip\n#  ADD https://www.ncdc.noaa.gov/orders/qclcd/QCLCD201512.zip /tmp/QCLCD201512.zip\n#\n#  RUN unzip -o /tmp/wbanmasterlist.psv.zip && \\\n#      gzip wbanmasterlist.psv && \\\n#      rm -f /tmp/wbanmasterlist.psv.zip && \\\n#      unzip -o /tmp/QCLCD201501.zip && \\\n#      gzip 201501*.txt && \\\n#      rm -f /tmp/QCLCD201501.zip && \\\n#      unzip -o /tmp/QCLCD201502.zip && \\\n#      gzip 201502*.txt && \\\n#      rm -f /tmp/QCLCD201502.zip && \\\n#      unzip -o /tmp/QCLCD201503.zip && \\\n#      gzip 201503*.txt && \\\n#      rm -f /tmp/QCLCD201503.zip && \\\n#      unzip -o /tmp/QCLCD201504.zip && \\\n#      gzip 201504*.txt && \\\n#      rm -f /tmp/QCLCD201504.zip && \\\n#      unzip -o /tmp/QCLCD201505.zip && \\\n#      gzip 201505*.txt && \\\n#      rm -f /tmp/QCLCD201505.zip && \\\n#      unzip -o /tmp/QCLCD201506.zip && \\\n#      gzip 201506*.txt && \\\n#      rm -f /tmp/QCLCD201506.zip && \\\n#      unzip -o /tmp/QCLCD201507.zip && \\\n#      gzip 201507*.txt && \\\n#      rm -f /tmp/QCLCD201507.zip && \\\n#      unzip -o /tmp/QCLCD201508.zip && \\\n#      gzip 201508*.txt && \\\n#      rm -f /tmp/QCLCD201508.zip && \\\n#      unzip -o /tmp/QCLCD201509.zip && \\\n#      gzip 201509*.txt && \\\n#      rm -f /tmp/QCLCD201509.zip && \\\n#      unzip -o /tmp/QCLCD201510.zip && \\\n#      gzip 201510*.txt && \\\n#      rm -f /tmp/QCLCD201510.zip && \\\n#      unzip -o /tmp/QCLCD201511.zip && \\\n#      gzip 201511*.txt && \\\n#      rm -f /tmp/QCLCD201511.zip && \\\n#      unzip -o /tmp/QCLCD201512.zip && \\\n#      gzip 201512*.txt && \\\n#      rm -f /tmp/QCLCD201512.zip\n#   Back to /root\nWORKDIR /root\n#   Cleanup\nRUN apt-get clean \\\n && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*\n#   Done!\n","originalDockerfileUglifiedHash":"a70e5faecc1461875f721a5e8cc7c6e3","fileName":"/ICSME-replicationpackage/dataset/smelly_dockerfiles_bianncle/bac308ae0a80625566cebdb1592a91bb3ea3a3cf.dockerfile"}