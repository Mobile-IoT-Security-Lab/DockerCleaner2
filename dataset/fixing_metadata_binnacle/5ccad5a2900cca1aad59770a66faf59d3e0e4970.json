{"seed":2244564314,"processedDockerfileHash":"a070e457602559a49032dd7e3c6fa122","fixedSmells":["use-no-install-recommends","do-not-use-apt-get-update-alone","pin-package-manager-versions-apt-get","pin-package-manager-versions-pip","pin-package-manager-versions-npm","pin-package-manager-versions-gem","pin-package-manager-versions-apk","use-copy-instead-of-add","use-wget-instead-of-add","do-not-have-secrets","have-a-healthcheck","have-a-user"],"successfullyFixedSmells":["have-a-healthcheck","have-a-user"],"processedDockerfile":"FROM centos:centos7\nARG DATAWAVE_COMMIT_ID\nARG DATAWAVE_BRANCH_NAME\nUSER root\n#   ENV params for hadoop here are a hack/fix for the \"chown: missing operand after '**/hadoop/logs'\"\n#   errors thrown by hadoop startup scripts for both the resourcemanager and historyserver. Those\n#   errors occur due to $USER being mysteriously null/undefined at the point that log file names are\n#   established and subsequently chowned in those scripts. But aside from the chown errors and a few\n#   irregularly-named log files, the null $USER issue doesn't seem to have any other negative impact.\n#   Mostly an annoyance and odd that it only seems to occur in Docker, which is why I'm documenting\n#   it here (issue has been observed w/ both centos6 and centos7 base images)\nENV YARN_IDENT_STRING=\"root\" \\\n    HADOOP_MAPRED_IDENT_STRING=\"root\"\n#   Build context should be the DataWave source root, minus .git and other dirs. See .dockerignore\nCOPY . /opt/datawave\n#   Install dependencies, configure password-less/zero-prompt SSH...\nRUN yum -y install openssl openssh openssh-server openssh-clients openssl-libs which bc wget git \\\n && yum clean all \\\n && ssh-keygen -q -N \"\" -t rsa -f ~/.ssh/id_rsa \\\n && cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys \\\n && chmod 0600 ~/.ssh/authorized_keys \\\n && ssh-keygen -q -N \"\" -t dsa -f /etc/ssh/ssh_host_dsa_key \\\n && ssh-keygen -q -N \"\" -t rsa -f /etc/ssh/ssh_host_rsa_key \\\n && ssh-keygen -q -N \"\" -t ecdsa -f /etc/ssh/ssh_host_ecdsa_key \\\n && ssh-keygen -q -N \"\" -t ed25519 -f /etc/ssh/ssh_host_ed25519_key \\\n && echo \"StrictHostKeyChecking no\" >> /etc/ssh/ssh_config \\\n && echo \"UserKnownHostsFile /dev/null\" >> /etc/ssh/ssh_config \\\n && echo \"LogLevel QUIET\" >> /etc/ssh/ssh_config\nWORKDIR /opt/datawave\n#   Create new Git repo for convenience...\nRUN rm -f .dockerignore \\\n && git init \\\n && git add . \\\n && git config user.email \"root@localhost.local\" \\\n && git config user.name \"Root User\" \\\n && git commit -m \"Source Branch :: $DATAWAVE_BRANCH_NAME :: Source Commit :: $DATAWAVE_COMMIT_ID\"\n#   This works exactly like the setup for a non-containerized instance of the datawave-quickstart\n#   environment. That is, ~/.bashrc and datawave-quickstart/bin/env.sh are sourced, bootstrapping\n#   the quickstart environment. Likewise, 'allInstall' and 'datawaveStart' wrapper functions are\n#   used to initialize services and their log dirs. Finally, web services are tested, services are\n#   stopped gracefully, and any cruft is purged.\n#   By design, if 'datawaveWebTest' exits with non-zero status, the docker build will fail\nRUN /bin/bash -c \"/usr/bin/nohup /usr/sbin/sshd -D &\" \\\n && echo \"export DW_ACCUMULO_DIST_URI=file://$( find /opt/datawave/contrib/datawave-quickstart/bin/services/accumulo -name 'accumulo*.tar.gz' ;)\" >> ~/.bashrc \\\n && echo \"export DW_ZOOKEEPER_DIST_URI=file://$( find /opt/datawave/contrib/datawave-quickstart/bin/services/accumulo -name 'zookeeper*.tar.gz' ;)\" >> ~/.bashrc \\\n && echo \"export DW_JAVA_DIST_URI=file://$( find /opt/datawave/contrib/datawave-quickstart/bin/services/java -name '*tar.gz' ;)\" >> ~/.bashrc \\\n && echo \"export DW_MAVEN_DIST_URI=file://$( find /opt/datawave/contrib/datawave-quickstart/bin/services/maven -name 'apache-maven*.tar.gz' ;)\" >> ~/.bashrc \\\n && echo \"export DW_HADOOP_DIST_URI=file://$( find /opt/datawave/contrib/datawave-quickstart/bin/services/hadoop -name 'hadoop*.tar.gz' ;)\" >> ~/.bashrc \\\n && echo \"export DW_WILDFLY_DIST_URI=file://$( find /opt/datawave/contrib/datawave-quickstart/bin/services/datawave -name 'wildfly*.tar.gz' ;)\" >> ~/.bashrc \\\n && echo \"source /opt/datawave/contrib/datawave-quickstart/bin/env.sh\" >> ~/.bashrc \\\n && /bin/bash -c \"source ~/.bashrc \\\n && allInstall \\\n && datawaveStart \\\n && datawaveWebTest --verbose \\\n && allStop\" \\\n && echo \"0.0.0.0\" > contrib/datawave-quickstart/accumulo/conf/monitor \\\n && rm -rf contrib/datawave-quickstart/datawave-ingest/logs/* \\\n && rm -rf contrib/datawave-quickstart/hadoop/logs/* \\\n && rm -rf contrib/datawave-quickstart/accumulo/logs/* \\\n && rm -rf contrib/datawave-quickstart/wildfly/standalone/log/*\n#   Lastly, establish volumes for data, logs & other directories, wire up\n#   the entrypoint & bootstrap scripts, expose ports, and set default CMD...\n#   Primary data volume (for HDFS, Accumulo, ZooKeeper, etc)\nVOLUME [\"/opt/datawave/contrib/datawave-quickstart/data\"]\n#   In case user wants to rebuild DW\nVOLUME [\"~/.m2/repository\"]\nVOLUME [\"/opt/datawave/contrib/datawave-quickstart/hadoop/logs\"]\nVOLUME [\"/opt/datawave/contrib/datawave-quickstart/accumulo/logs\"]\nVOLUME [\"/opt/datawave/contrib/datawave-quickstart/wildfly/standalone/log\"]\nVOLUME [\"/opt/datawave/contrib/datawave-quickstart/datawave-ingest/logs\"]\nEXPOSE 8443/tcp 9995/tcp 50070/tcp 8088/tcp 9000/tcp 2181/tcp\nWORKDIR /opt/datawave/contrib/datawave-quickstart\nRUN ln -s /opt/datawave/contrib/datawave-quickstart/docker/docker-entrypoint.sh /usr/local/bin/docker-entrypoint.sh \\\n && ln -s /opt/datawave/contrib/datawave-quickstart/docker/datawave-bootstrap.sh /usr/local/bin/datawave-bootstrap.sh\n#   The entrypoint script will ensure that sshd is started, and it'll simply 'exec \"$@\"' whatever command is passed\nENTRYPOINT [\"docker-entrypoint.sh\"]\n#   Default CMD uses the bootstrap script to start up DataWave's web services. Due to the --bash flag, it'll\n#   exec /bin/bash for the container process, intended for 'docker run -it ...' usage.\nCMD [\"datawave-bootstrap.sh\", \"--web\", \"--bash\"]\n#   Without the --bash flag, datawave-bootstrap.sh will go into an infinite loop to prevent the container\n#   from exiting, better for 'docker run -d ...' usage\nRUN groupadd --system docker-user ; useradd --system --gid docker-user docker-user\nUSER docker-user\n# Please add your HEALTHCHECK here!!!\n","originalDockerfile":"FROM centos:centos7\nARG DATAWAVE_COMMIT_ID\nARG DATAWAVE_BRANCH_NAME\nUSER root\n#  ENV params for hadoop here are a hack/fix for the \"chown: missing operand after '**/hadoop/logs'\"\n#  errors thrown by hadoop startup scripts for both the resourcemanager and historyserver. Those\n#  errors occur due to $USER being mysteriously null/undefined at the point that log file names are\n#  established and subsequently chowned in those scripts. But aside from the chown errors and a few\n#  irregularly-named log files, the null $USER issue doesn't seem to have any other negative impact.\n#  Mostly an annoyance and odd that it only seems to occur in Docker, which is why I'm documenting\n#  it here (issue has been observed w/ both centos6 and centos7 base images)\nENV YARN_IDENT_STRING=\"root\" \\\n    HADOOP_MAPRED_IDENT_STRING=\"root\"\n#  Build context should be the DataWave source root, minus .git and other dirs. See .dockerignore\nCOPY . /opt/datawave\n#  Install dependencies, configure password-less/zero-prompt SSH...\nRUN yum -y install openssl openssh openssh-server openssh-clients openssl-libs which bc wget git \\\n && yum clean all \\\n && ssh-keygen -q -N \"\" -t rsa -f ~/.ssh/id_rsa \\\n && cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys \\\n && chmod 0600 ~/.ssh/authorized_keys \\\n && ssh-keygen -q -N \"\" -t dsa -f /etc/ssh/ssh_host_dsa_key \\\n && ssh-keygen -q -N \"\" -t rsa -f /etc/ssh/ssh_host_rsa_key \\\n && ssh-keygen -q -N \"\" -t ecdsa -f /etc/ssh/ssh_host_ecdsa_key \\\n && ssh-keygen -q -N \"\" -t ed25519 -f /etc/ssh/ssh_host_ed25519_key \\\n && echo \"StrictHostKeyChecking no\" >> /etc/ssh/ssh_config \\\n && echo \"UserKnownHostsFile /dev/null\" >> /etc/ssh/ssh_config \\\n && echo \"LogLevel QUIET\" >> /etc/ssh/ssh_config\nWORKDIR /opt/datawave\n#  Create new Git repo for convenience...\nRUN rm -f .dockerignore \\\n && git init \\\n && git add . \\\n && git config user.email \"root@localhost.local\" \\\n && git config user.name \"Root User\" \\\n && git commit -m \"Source Branch :: $DATAWAVE_BRANCH_NAME :: Source Commit :: $DATAWAVE_COMMIT_ID\"\n#  This works exactly like the setup for a non-containerized instance of the datawave-quickstart\n#  environment. That is, ~/.bashrc and datawave-quickstart/bin/env.sh are sourced, bootstrapping\n#  the quickstart environment. Likewise, 'allInstall' and 'datawaveStart' wrapper functions are\n#  used to initialize services and their log dirs. Finally, web services are tested, services are\n#  stopped gracefully, and any cruft is purged.\n#  By design, if 'datawaveWebTest' exits with non-zero status, the docker build will fail\nRUN /bin/bash -c \"/usr/bin/nohup /usr/sbin/sshd -D &\" \\\n && echo \"export DW_ACCUMULO_DIST_URI=file://$( find /opt/datawave/contrib/datawave-quickstart/bin/services/accumulo -name 'accumulo*.tar.gz' ;)\" >> ~/.bashrc \\\n && echo \"export DW_ZOOKEEPER_DIST_URI=file://$( find /opt/datawave/contrib/datawave-quickstart/bin/services/accumulo -name 'zookeeper*.tar.gz' ;)\" >> ~/.bashrc \\\n && echo \"export DW_JAVA_DIST_URI=file://$( find /opt/datawave/contrib/datawave-quickstart/bin/services/java -name '*tar.gz' ;)\" >> ~/.bashrc \\\n && echo \"export DW_MAVEN_DIST_URI=file://$( find /opt/datawave/contrib/datawave-quickstart/bin/services/maven -name 'apache-maven*.tar.gz' ;)\" >> ~/.bashrc \\\n && echo \"export DW_HADOOP_DIST_URI=file://$( find /opt/datawave/contrib/datawave-quickstart/bin/services/hadoop -name 'hadoop*.tar.gz' ;)\" >> ~/.bashrc \\\n && echo \"export DW_WILDFLY_DIST_URI=file://$( find /opt/datawave/contrib/datawave-quickstart/bin/services/datawave -name 'wildfly*.tar.gz' ;)\" >> ~/.bashrc \\\n && echo \"source /opt/datawave/contrib/datawave-quickstart/bin/env.sh\" >> ~/.bashrc \\\n && /bin/bash -c \"source ~/.bashrc \\\n && allInstall \\\n && datawaveStart \\\n && datawaveWebTest --verbose \\\n && allStop\" \\\n && echo \"0.0.0.0\" > contrib/datawave-quickstart/accumulo/conf/monitor \\\n && rm -rf contrib/datawave-quickstart/datawave-ingest/logs/* \\\n && rm -rf contrib/datawave-quickstart/hadoop/logs/* \\\n && rm -rf contrib/datawave-quickstart/accumulo/logs/* \\\n && rm -rf contrib/datawave-quickstart/wildfly/standalone/log/*\n#  Lastly, establish volumes for data, logs & other directories, wire up\n#  the entrypoint & bootstrap scripts, expose ports, and set default CMD...\n#  Primary data volume (for HDFS, Accumulo, ZooKeeper, etc)\nVOLUME [\"/opt/datawave/contrib/datawave-quickstart/data\"]\n#  In case user wants to rebuild DW\nVOLUME [\"~/.m2/repository\"]\nVOLUME [\"/opt/datawave/contrib/datawave-quickstart/hadoop/logs\"]\nVOLUME [\"/opt/datawave/contrib/datawave-quickstart/accumulo/logs\"]\nVOLUME [\"/opt/datawave/contrib/datawave-quickstart/wildfly/standalone/log\"]\nVOLUME [\"/opt/datawave/contrib/datawave-quickstart/datawave-ingest/logs\"]\nEXPOSE 8443/tcp 9995/tcp 50070/tcp 8088/tcp 9000/tcp 2181/tcp\nWORKDIR /opt/datawave/contrib/datawave-quickstart\nRUN ln -s /opt/datawave/contrib/datawave-quickstart/docker/docker-entrypoint.sh /usr/local/bin/docker-entrypoint.sh \\\n && ln -s /opt/datawave/contrib/datawave-quickstart/docker/datawave-bootstrap.sh /usr/local/bin/datawave-bootstrap.sh\n#  The entrypoint script will ensure that sshd is started, and it'll simply 'exec \"$@\"' whatever command is passed\nENTRYPOINT [\"docker-entrypoint.sh\"]\n#  Default CMD uses the bootstrap script to start up DataWave's web services. Due to the --bash flag, it'll\n#  exec /bin/bash for the container process, intended for 'docker run -it ...' usage.\nCMD [\"datawave-bootstrap.sh\", \"--web\", \"--bash\"]\n#  Without the --bash flag, datawave-bootstrap.sh will go into an infinite loop to prevent the container\n#  from exiting, better for 'docker run -d ...' usage\n","injectedSmells":[],"originalDockerfileHash":"bf46d56a684a5453aa3eec7285c9555a","successfullyInjectedSmells":[],"originalDockerfileUglified":"FROM centos:centos7\nARG DATAWAVE_COMMIT_ID\nARG DATAWAVE_BRANCH_NAME\nUSER root\n#   ENV params for hadoop here are a hack/fix for the \"chown: missing operand after '**/hadoop/logs'\"\n#   errors thrown by hadoop startup scripts for both the resourcemanager and historyserver. Those\n#   errors occur due to $USER being mysteriously null/undefined at the point that log file names are\n#   established and subsequently chowned in those scripts. But aside from the chown errors and a few\n#   irregularly-named log files, the null $USER issue doesn't seem to have any other negative impact.\n#   Mostly an annoyance and odd that it only seems to occur in Docker, which is why I'm documenting\n#   it here (issue has been observed w/ both centos6 and centos7 base images)\nENV YARN_IDENT_STRING=\"root\" \\\n    HADOOP_MAPRED_IDENT_STRING=\"root\"\n#   Build context should be the DataWave source root, minus .git and other dirs. See .dockerignore\nCOPY . /opt/datawave\n#   Install dependencies, configure password-less/zero-prompt SSH...\nRUN yum -y install openssl openssh openssh-server openssh-clients openssl-libs which bc wget git \\\n && yum clean all \\\n && ssh-keygen -q -N \"\" -t rsa -f ~/.ssh/id_rsa \\\n && cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys \\\n && chmod 0600 ~/.ssh/authorized_keys \\\n && ssh-keygen -q -N \"\" -t dsa -f /etc/ssh/ssh_host_dsa_key \\\n && ssh-keygen -q -N \"\" -t rsa -f /etc/ssh/ssh_host_rsa_key \\\n && ssh-keygen -q -N \"\" -t ecdsa -f /etc/ssh/ssh_host_ecdsa_key \\\n && ssh-keygen -q -N \"\" -t ed25519 -f /etc/ssh/ssh_host_ed25519_key \\\n && echo \"StrictHostKeyChecking no\" >> /etc/ssh/ssh_config \\\n && echo \"UserKnownHostsFile /dev/null\" >> /etc/ssh/ssh_config \\\n && echo \"LogLevel QUIET\" >> /etc/ssh/ssh_config\nWORKDIR /opt/datawave\n#   Create new Git repo for convenience...\nRUN rm -f .dockerignore \\\n && git init \\\n && git add . \\\n && git config user.email \"root@localhost.local\" \\\n && git config user.name \"Root User\" \\\n && git commit -m \"Source Branch :: $DATAWAVE_BRANCH_NAME :: Source Commit :: $DATAWAVE_COMMIT_ID\"\n#   This works exactly like the setup for a non-containerized instance of the datawave-quickstart\n#   environment. That is, ~/.bashrc and datawave-quickstart/bin/env.sh are sourced, bootstrapping\n#   the quickstart environment. Likewise, 'allInstall' and 'datawaveStart' wrapper functions are\n#   used to initialize services and their log dirs. Finally, web services are tested, services are\n#   stopped gracefully, and any cruft is purged.\n#   By design, if 'datawaveWebTest' exits with non-zero status, the docker build will fail\nRUN /bin/bash -c \"/usr/bin/nohup /usr/sbin/sshd -D &\" \\\n && echo \"export DW_ACCUMULO_DIST_URI=file://$( find /opt/datawave/contrib/datawave-quickstart/bin/services/accumulo -name 'accumulo*.tar.gz' ;)\" >> ~/.bashrc \\\n && echo \"export DW_ZOOKEEPER_DIST_URI=file://$( find /opt/datawave/contrib/datawave-quickstart/bin/services/accumulo -name 'zookeeper*.tar.gz' ;)\" >> ~/.bashrc \\\n && echo \"export DW_JAVA_DIST_URI=file://$( find /opt/datawave/contrib/datawave-quickstart/bin/services/java -name '*tar.gz' ;)\" >> ~/.bashrc \\\n && echo \"export DW_MAVEN_DIST_URI=file://$( find /opt/datawave/contrib/datawave-quickstart/bin/services/maven -name 'apache-maven*.tar.gz' ;)\" >> ~/.bashrc \\\n && echo \"export DW_HADOOP_DIST_URI=file://$( find /opt/datawave/contrib/datawave-quickstart/bin/services/hadoop -name 'hadoop*.tar.gz' ;)\" >> ~/.bashrc \\\n && echo \"export DW_WILDFLY_DIST_URI=file://$( find /opt/datawave/contrib/datawave-quickstart/bin/services/datawave -name 'wildfly*.tar.gz' ;)\" >> ~/.bashrc \\\n && echo \"source /opt/datawave/contrib/datawave-quickstart/bin/env.sh\" >> ~/.bashrc \\\n && /bin/bash -c \"source ~/.bashrc \\\n && allInstall \\\n && datawaveStart \\\n && datawaveWebTest --verbose \\\n && allStop\" \\\n && echo \"0.0.0.0\" > contrib/datawave-quickstart/accumulo/conf/monitor \\\n && rm -rf contrib/datawave-quickstart/datawave-ingest/logs/* \\\n && rm -rf contrib/datawave-quickstart/hadoop/logs/* \\\n && rm -rf contrib/datawave-quickstart/accumulo/logs/* \\\n && rm -rf contrib/datawave-quickstart/wildfly/standalone/log/*\n#   Lastly, establish volumes for data, logs & other directories, wire up\n#   the entrypoint & bootstrap scripts, expose ports, and set default CMD...\n#   Primary data volume (for HDFS, Accumulo, ZooKeeper, etc)\nVOLUME [\"/opt/datawave/contrib/datawave-quickstart/data\"]\n#   In case user wants to rebuild DW\nVOLUME [\"~/.m2/repository\"]\nVOLUME [\"/opt/datawave/contrib/datawave-quickstart/hadoop/logs\"]\nVOLUME [\"/opt/datawave/contrib/datawave-quickstart/accumulo/logs\"]\nVOLUME [\"/opt/datawave/contrib/datawave-quickstart/wildfly/standalone/log\"]\nVOLUME [\"/opt/datawave/contrib/datawave-quickstart/datawave-ingest/logs\"]\nEXPOSE 8443/tcp 9995/tcp 50070/tcp 8088/tcp 9000/tcp 2181/tcp\nWORKDIR /opt/datawave/contrib/datawave-quickstart\nRUN ln -s /opt/datawave/contrib/datawave-quickstart/docker/docker-entrypoint.sh /usr/local/bin/docker-entrypoint.sh \\\n && ln -s /opt/datawave/contrib/datawave-quickstart/docker/datawave-bootstrap.sh /usr/local/bin/datawave-bootstrap.sh\n#   The entrypoint script will ensure that sshd is started, and it'll simply 'exec \"$@\"' whatever command is passed\nENTRYPOINT [\"docker-entrypoint.sh\"]\n#   Default CMD uses the bootstrap script to start up DataWave's web services. Due to the --bash flag, it'll\n#   exec /bin/bash for the container process, intended for 'docker run -it ...' usage.\nCMD [\"datawave-bootstrap.sh\", \"--web\", \"--bash\"]\n#   Without the --bash flag, datawave-bootstrap.sh will go into an infinite loop to prevent the container\n#   from exiting, better for 'docker run -d ...' usage\n","originalDockerfileUglifiedHash":"142173bdd3150078c1fcfd68d91d498d","fileName":"/ICSME-replicationpackage/dataset/smelly_dockerfiles_bianncle/5ccad5a2900cca1aad59770a66faf59d3e0e4970.dockerfile"}