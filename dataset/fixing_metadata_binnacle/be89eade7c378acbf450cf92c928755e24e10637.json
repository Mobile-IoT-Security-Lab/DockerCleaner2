{"seed":4286541954,"processedDockerfileHash":"c52834cf35f6e9bfcf258851e27f3817","fixedSmells":["use-no-install-recommends","do-not-use-apt-get-update-alone","pin-package-manager-versions-apt-get","pin-package-manager-versions-pip","pin-package-manager-versions-npm","pin-package-manager-versions-gem","pin-package-manager-versions-apk","use-copy-instead-of-add","use-wget-instead-of-add","do-not-have-secrets","have-a-healthcheck","have-a-user"],"successfullyFixedSmells":["have-a-healthcheck"],"processedDockerfile":"FROM quay.io/geomesa/accumulo-geomesa:__TAG__\nMAINTAINER GeoMesa team <geomesa@ccri.com>\nARG TAG\nARG GEOMESA_VERSION\nARG ACCUMULO_VERSION\nARG THRIFT_VERSION\nENV GEOMESA_VERSION=\"${GEOMESA_VERSION}\"\nUSER root\n#   Install Tini\nRUN wget --quiet https://github.com/krallin/tini/releases/download/v0.10.0/tini \\\n && echo \"1361527f39190a7338a0b434bd8c88ff7233ce7b9a4876f3315c22fce7eca1b0 *tini\" | sha256sum -c - \\\n && mv tini /usr/local/bin/tini \\\n && chmod +x /usr/local/bin/tini\n#   Configure environment\nENV CONDA_DIR=\"/opt/conda\"\nENV PATH=\"$CONDA_DIR/bin:$PATH\"\nENV SHELL=\"/bin/bash\"\nENV NB_USER=\"hdfs\"\nENV NB_UID=\"1000\"\nENV LC_ALL=\"en_US.UTF-8\"\nENV LANG=\"en_US.UTF-8\"\nENV LANGUAGE=\"en_US.UTF-8\"\nENV NB_HOME=\"/var/lib/hadoop-hdfs\"\n#   Setup work directory\nRUN mkdir $CONDA_DIR \\\n && chown hdfs:hdfs $CONDA_DIR\nUSER $NB_USER\n#   Setup hdfs home directory\nRUN mkdir -p ${NB_HOME}/work \\\n && mkdir ${NB_HOME}/.jupyter \\\n && mkdir ${NB_HOME}/work/js\nCOPY js/ ${NB_HOME}/work/js/\n#   copy the sample notebook\nUSER root\nCOPY GDELT+Analysis.ipynb ${NB_HOME}/work/\nRUN chmod a+rw ${NB_HOME}/work/*.ipynb\nUSER $NB_USER\n#   Install conda as hdfs\nRUN cd /tmp \\\n && wget --quiet https://repo.continuum.io/miniconda/Miniconda3-4.1.11-Linux-x86_64.sh \\\n && echo \"efd6a9362fc6b4085f599a881d20e57de628da8c1a898c08ec82874f3bad41bf *Miniconda3-4.1.11-Linux-x86_64.sh\" | sha256sum -c - \\\n && /bin/bash Miniconda3-4.1.11-Linux-x86_64.sh -f -b -p $CONDA_DIR \\\n && rm Miniconda3-4.1.11-Linux-x86_64.sh \\\n && $CONDA_DIR/bin/conda install --quiet --yes conda==4.1.11 \\\n && $CONDA_DIR/bin/conda config --system --add channels conda-forge \\\n && $CONDA_DIR/bin/conda config --system --set auto_update_conda false \\\n && conda clean -tipsy\n#   Install Jupyter notebook as hdfs\nRUN conda install --quiet --yes 'notebook=4.2*' jupyterhub=0.7 \\\n && conda clean -tipsy\nUSER root\nEXPOSE 8888/tcp\nWORKDIR ${NB_HOME}/work\n#   Configure container startup\nENTRYPOINT [\"tini\", \"--\"]\nCMD [\"start-notebook.sh\"]\n#   Add local files as late as possible to avoid cache busting\nCOPY start.sh /usr/local/bin/\nCOPY start-notebook.sh /usr/local/bin/\nCOPY start-singleuser.sh /usr/local/bin/\nCOPY jupyter_notebook_config.py ${NB_HOME}/.jupyter/\nRUN chown -R $NB_USER:hdfs ${NB_HOME}/.jupyter\n#   install toree\nCOPY toree-0.2.0.dev1.tar.gz /tmp\nRUN pip install /tmp/toree-0.2.0.dev1.tar.gz --upgrade --pre\n#   install Spark\nENV APACHE_SPARK_VERSION=\"2.0.2\"\nENV HADOOP_VERSION=\"2.8.4\"\nRUN cd /tmp \\\n && wget -q http://d3kbcqa49mib13.cloudfront.net/spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz \\\n && echo \"e6349dd38ded84831e3ff7d391ae7f2525c359fb452b0fc32ee2ab637673552a *spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz\" | sha256sum -c - \\\n && tar xzf spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz -C /usr/local \\\n && rm spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz\nRUN cd /usr/local \\\n && ln -s spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} spark\n#   Spark and Mesos config\nENV SPARK_HOME=\"/usr/local/spark\"\nENV PYTHONPATH=\"$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.3-src.zip\"\nENV MESOS_NATIVE_LIBRARY=\"/usr/local/lib/libmesos.so\"\n#   copy vegas libs to spark jars dir\nCOPY lib/* /usr/local/spark/jars/\n#   Switch back to hdfs to avoid accidental container runs as root\nUSER $NB_USER\nENV GEOMESA_SPARK_HOME=\"file:///opt/geomesa/dist/spark\"\nENV GEOMESA_SPARK_JARS=\"${GEOMESA_SPARK_HOME}/geomesa-accumulo-spark-runtime_2.11-${GEOMESA_VERSION}.jar,${GEOMESA_SPARK_HOME}/geomesa-spark-converter_2.11-${GEOMESA_VERSION}.jar,${GEOMESA_SPARK_HOME}/geomesa-spark-geotools_2.11-${GEOMESA_VERSION}.jar\"\n#   install GeoMesa kernel\nRUN jupyter toree install --replace --user --kernel_name=\"GeoMesa Spark\" --spark_home=${SPARK_HOME} --spark_opts=\"--driver-java-options=-Xmx4096M --driver-java-options=-Dlog4j.logLevel=info --master yarn --jars ${GEOMESA_SPARK_JARS}\"\n# Please add your HEALTHCHECK here!!!\n","originalDockerfile":"FROM quay.io/geomesa/accumulo-geomesa:__TAG__\nMAINTAINER GeoMesa team <geomesa@ccri.com>\nARG TAG\nARG GEOMESA_VERSION\nARG ACCUMULO_VERSION\nARG THRIFT_VERSION\nENV GEOMESA_VERSION=\"${GEOMESA_VERSION}\"\nUSER root\n#  Install Tini\nRUN wget --quiet https://github.com/krallin/tini/releases/download/v0.10.0/tini \\\n && echo \"1361527f39190a7338a0b434bd8c88ff7233ce7b9a4876f3315c22fce7eca1b0 *tini\" | sha256sum -c - \\\n && mv tini /usr/local/bin/tini \\\n && chmod +x /usr/local/bin/tini\n#  Configure environment\nENV CONDA_DIR=\"/opt/conda\"\nENV PATH=\"$CONDA_DIR/bin:$PATH\"\nENV SHELL=\"/bin/bash\"\nENV NB_USER=\"hdfs\"\nENV NB_UID=\"1000\"\nENV LC_ALL=\"en_US.UTF-8\"\nENV LANG=\"en_US.UTF-8\"\nENV LANGUAGE=\"en_US.UTF-8\"\nENV NB_HOME=\"/var/lib/hadoop-hdfs\"\n#  Setup work directory\nRUN mkdir $CONDA_DIR \\\n && chown hdfs:hdfs $CONDA_DIR\nUSER $NB_USER\n#  Setup hdfs home directory\nRUN mkdir -p ${NB_HOME}/work \\\n && mkdir ${NB_HOME}/.jupyter \\\n && mkdir ${NB_HOME}/work/js\nCOPY js/ ${NB_HOME}/work/js/\n#  copy the sample notebook\nUSER root\nCOPY GDELT+Analysis.ipynb ${NB_HOME}/work/\nRUN chmod a+rw ${NB_HOME}/work/*.ipynb\nUSER $NB_USER\n#  Install conda as hdfs\nRUN cd /tmp \\\n && wget --quiet https://repo.continuum.io/miniconda/Miniconda3-4.1.11-Linux-x86_64.sh \\\n && echo \"efd6a9362fc6b4085f599a881d20e57de628da8c1a898c08ec82874f3bad41bf *Miniconda3-4.1.11-Linux-x86_64.sh\" | sha256sum -c - \\\n && /bin/bash Miniconda3-4.1.11-Linux-x86_64.sh -f -b -p $CONDA_DIR \\\n && rm Miniconda3-4.1.11-Linux-x86_64.sh \\\n && $CONDA_DIR/bin/conda install --quiet --yes conda==4.1.11 \\\n && $CONDA_DIR/bin/conda config --system --add channels conda-forge \\\n && $CONDA_DIR/bin/conda config --system --set auto_update_conda false \\\n && conda clean -tipsy\n#  Install Jupyter notebook as hdfs\nRUN conda install --quiet --yes 'notebook=4.2*' jupyterhub=0.7 \\\n && conda clean -tipsy\nUSER root\nEXPOSE 8888/tcp\nWORKDIR ${NB_HOME}/work\n#  Configure container startup\nENTRYPOINT [\"tini\", \"--\"]\nCMD [\"start-notebook.sh\"]\n#  Add local files as late as possible to avoid cache busting\nCOPY start.sh /usr/local/bin/\nCOPY start-notebook.sh /usr/local/bin/\nCOPY start-singleuser.sh /usr/local/bin/\nCOPY jupyter_notebook_config.py ${NB_HOME}/.jupyter/\nRUN chown -R $NB_USER:hdfs ${NB_HOME}/.jupyter\n#  install toree\nCOPY toree-0.2.0.dev1.tar.gz /tmp\nRUN pip install /tmp/toree-0.2.0.dev1.tar.gz --upgrade --pre\n#  install Spark\nENV APACHE_SPARK_VERSION=\"2.0.2\"\nENV HADOOP_VERSION=\"2.8.4\"\nRUN cd /tmp \\\n && wget -q http://d3kbcqa49mib13.cloudfront.net/spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz \\\n && echo \"e6349dd38ded84831e3ff7d391ae7f2525c359fb452b0fc32ee2ab637673552a *spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz\" | sha256sum -c - \\\n && tar xzf spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz -C /usr/local \\\n && rm spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz\nRUN cd /usr/local \\\n && ln -s spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} spark\n#  Spark and Mesos config\nENV SPARK_HOME=\"/usr/local/spark\"\nENV PYTHONPATH=\"$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.3-src.zip\"\nENV MESOS_NATIVE_LIBRARY=\"/usr/local/lib/libmesos.so\"\n#  copy vegas libs to spark jars dir\nCOPY lib/* /usr/local/spark/jars/\n#  Switch back to hdfs to avoid accidental container runs as root\nUSER $NB_USER\nENV GEOMESA_SPARK_HOME=\"file:///opt/geomesa/dist/spark\"\nENV GEOMESA_SPARK_JARS=\"${GEOMESA_SPARK_HOME}/geomesa-accumulo-spark-runtime_2.11-${GEOMESA_VERSION}.jar,${GEOMESA_SPARK_HOME}/geomesa-spark-converter_2.11-${GEOMESA_VERSION}.jar,${GEOMESA_SPARK_HOME}/geomesa-spark-geotools_2.11-${GEOMESA_VERSION}.jar\"\n#  install GeoMesa kernel\nRUN jupyter toree install --replace --user --kernel_name=\"GeoMesa Spark\" --spark_home=${SPARK_HOME} --spark_opts=\"--driver-java-options=-Xmx4096M --driver-java-options=-Dlog4j.logLevel=info --master yarn --jars ${GEOMESA_SPARK_JARS}\"\n","injectedSmells":[],"originalDockerfileHash":"e30af9bea88fe30167e2f185ff7f7082","successfullyInjectedSmells":[],"originalDockerfileUglified":"FROM quay.io/geomesa/accumulo-geomesa:__TAG__\nMAINTAINER GeoMesa team <geomesa@ccri.com>\nARG TAG\nARG GEOMESA_VERSION\nARG ACCUMULO_VERSION\nARG THRIFT_VERSION\nENV GEOMESA_VERSION=\"${GEOMESA_VERSION}\"\nUSER root\n#   Install Tini\nRUN wget --quiet https://github.com/krallin/tini/releases/download/v0.10.0/tini \\\n && echo \"1361527f39190a7338a0b434bd8c88ff7233ce7b9a4876f3315c22fce7eca1b0 *tini\" | sha256sum -c - \\\n && mv tini /usr/local/bin/tini \\\n && chmod +x /usr/local/bin/tini\n#   Configure environment\nENV CONDA_DIR=\"/opt/conda\"\nENV PATH=\"$CONDA_DIR/bin:$PATH\"\nENV SHELL=\"/bin/bash\"\nENV NB_USER=\"hdfs\"\nENV NB_UID=\"1000\"\nENV LC_ALL=\"en_US.UTF-8\"\nENV LANG=\"en_US.UTF-8\"\nENV LANGUAGE=\"en_US.UTF-8\"\nENV NB_HOME=\"/var/lib/hadoop-hdfs\"\n#   Setup work directory\nRUN mkdir $CONDA_DIR \\\n && chown hdfs:hdfs $CONDA_DIR\nUSER $NB_USER\n#   Setup hdfs home directory\nRUN mkdir -p ${NB_HOME}/work \\\n && mkdir ${NB_HOME}/.jupyter \\\n && mkdir ${NB_HOME}/work/js\nCOPY js/ ${NB_HOME}/work/js/\n#   copy the sample notebook\nUSER root\nCOPY GDELT+Analysis.ipynb ${NB_HOME}/work/\nRUN chmod a+rw ${NB_HOME}/work/*.ipynb\nUSER $NB_USER\n#   Install conda as hdfs\nRUN cd /tmp \\\n && wget --quiet https://repo.continuum.io/miniconda/Miniconda3-4.1.11-Linux-x86_64.sh \\\n && echo \"efd6a9362fc6b4085f599a881d20e57de628da8c1a898c08ec82874f3bad41bf *Miniconda3-4.1.11-Linux-x86_64.sh\" | sha256sum -c - \\\n && /bin/bash Miniconda3-4.1.11-Linux-x86_64.sh -f -b -p $CONDA_DIR \\\n && rm Miniconda3-4.1.11-Linux-x86_64.sh \\\n && $CONDA_DIR/bin/conda install --quiet --yes conda==4.1.11 \\\n && $CONDA_DIR/bin/conda config --system --add channels conda-forge \\\n && $CONDA_DIR/bin/conda config --system --set auto_update_conda false \\\n && conda clean -tipsy\n#   Install Jupyter notebook as hdfs\nRUN conda install --quiet --yes 'notebook=4.2*' jupyterhub=0.7 \\\n && conda clean -tipsy\nUSER root\nEXPOSE 8888/tcp\nWORKDIR ${NB_HOME}/work\n#   Configure container startup\nENTRYPOINT [\"tini\", \"--\"]\nCMD [\"start-notebook.sh\"]\n#   Add local files as late as possible to avoid cache busting\nCOPY start.sh /usr/local/bin/\nCOPY start-notebook.sh /usr/local/bin/\nCOPY start-singleuser.sh /usr/local/bin/\nCOPY jupyter_notebook_config.py ${NB_HOME}/.jupyter/\nRUN chown -R $NB_USER:hdfs ${NB_HOME}/.jupyter\n#   install toree\nCOPY toree-0.2.0.dev1.tar.gz /tmp\nRUN pip install /tmp/toree-0.2.0.dev1.tar.gz --upgrade --pre\n#   install Spark\nENV APACHE_SPARK_VERSION=\"2.0.2\"\nENV HADOOP_VERSION=\"2.8.4\"\nRUN cd /tmp \\\n && wget -q http://d3kbcqa49mib13.cloudfront.net/spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz \\\n && echo \"e6349dd38ded84831e3ff7d391ae7f2525c359fb452b0fc32ee2ab637673552a *spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz\" | sha256sum -c - \\\n && tar xzf spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz -C /usr/local \\\n && rm spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz\nRUN cd /usr/local \\\n && ln -s spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} spark\n#   Spark and Mesos config\nENV SPARK_HOME=\"/usr/local/spark\"\nENV PYTHONPATH=\"$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.3-src.zip\"\nENV MESOS_NATIVE_LIBRARY=\"/usr/local/lib/libmesos.so\"\n#   copy vegas libs to spark jars dir\nCOPY lib/* /usr/local/spark/jars/\n#   Switch back to hdfs to avoid accidental container runs as root\nUSER $NB_USER\nENV GEOMESA_SPARK_HOME=\"file:///opt/geomesa/dist/spark\"\nENV GEOMESA_SPARK_JARS=\"${GEOMESA_SPARK_HOME}/geomesa-accumulo-spark-runtime_2.11-${GEOMESA_VERSION}.jar,${GEOMESA_SPARK_HOME}/geomesa-spark-converter_2.11-${GEOMESA_VERSION}.jar,${GEOMESA_SPARK_HOME}/geomesa-spark-geotools_2.11-${GEOMESA_VERSION}.jar\"\n#   install GeoMesa kernel\nRUN jupyter toree install --replace --user --kernel_name=\"GeoMesa Spark\" --spark_home=${SPARK_HOME} --spark_opts=\"--driver-java-options=-Xmx4096M --driver-java-options=-Dlog4j.logLevel=info --master yarn --jars ${GEOMESA_SPARK_JARS}\"\n","originalDockerfileUglifiedHash":"f2c77c195a714f19a98dcaa2dfd6bee6","fileName":"/ICSME-replicationpackage/dataset/smelly_dockerfiles_bianncle/be89eade7c378acbf450cf92c928755e24e10637.dockerfile"}