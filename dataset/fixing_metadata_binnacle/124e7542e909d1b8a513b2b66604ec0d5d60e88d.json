{"seed":510808509,"processedDockerfileHash":"56c4fb7e1eb6c6005b72ed9cd9f2c504","fixedSmells":["use-no-install-recommends","do-not-use-apt-get-update-alone","pin-package-manager-versions-apt-get","pin-package-manager-versions-pip","pin-package-manager-versions-npm","pin-package-manager-versions-gem","pin-package-manager-versions-apk","use-copy-instead-of-add","use-wget-instead-of-add","do-not-have-secrets","have-a-healthcheck","have-a-user"],"successfullyFixedSmells":["use-no-install-recommends","do-not-use-apt-get-update-alone","pin-package-manager-versions-apt-get","pin-package-manager-versions-pip","have-a-healthcheck","have-a-user"],"processedDockerfile":"FROM ubuntu:16.04\nMAINTAINER ramon@wartala.de\nRUN apt-get update \\\n && (apt-get update ;apt-get install --no-install-recommends build-essential=12.1ubuntu2 curl=7.47.0-1ubuntu2.19 cmake=3.5.1-1ubuntu3 git=1:2.7.4-0ubuntu1.10 wget=1.17.1-1ubuntu1.5 libatlas-base-dev=3.10.2-9 libboost-all-dev=1.58.0.1ubuntu1 libfreetype6-dev=2.6.1-0.1ubuntu2.5 libgflags-dev=2.1.2-3 libgoogle-glog-dev=0.3.4-0.1 libhdf5-serial-dev=1.8.16+docs-4ubuntu1.1 libleveldb-dev=1.18-5 liblmdb-dev=0.9.17-3 libopencv-dev=2.4.9.1+dfsg-1.5ubuntu1.1 libpng12-dev=1.2.54-1ubuntu1.1 libzmq3-dev=4.1.4-7ubuntu0.1 libprotobuf-dev=2.6.1-1.3 libsnappy-dev=1.1.3-2 pkg-config=0.29.1-0ubuntu1 protobuf-compiler=2.6.1-1.3 rsync=3.1.1-3ubuntu1.3 software-properties-common=0.96.20.10 unzip=6.0-20ubuntu1.1 groff=1.22.3-7 vim=2:7.4.1689-3ubuntu1.5 zlib1g-dev=1:1.2.8.dfsg-2ubuntu4.3 python-pydot=1.0.28-2 -y ) \\\n && apt-get clean \\\n && apt-get autoremove \\\n && rm -rf /var/lib/apt/lists/*\nRUN curl -O https://bootstrap.pypa.io/get-pip.py \\\n && python get-pip.py \\\n && rm get-pip.py\n#  RUN pip --no-cache-dir install \\\n#  \t\tpyopenssl \\\n#  \t\tndg-httpsclient \\\n#  \t\tpyasn1\nRUN apt-get update \\\n && (apt-get update ;apt-get install --no-install-recommends python-numpy=1:1.11.0-1ubuntu1 python-scipy=0.17.0-1 python-nose=1.3.7-1 python-h5py=2.6.0-1 python-skimage=0.10.1-2build1 python-matplotlib=1.5.1-1ubuntu1 python-pandas=0.17.1-3ubuntu2 python-sklearn=0.17.0-4 python-sympy=0.7.6.1-1 python-skimage=0.10.1-2build1 python-progressbar=2.3-2 -y ) \\\n && apt-get clean \\\n && apt-get autoremove \\\n && rm -rf /var/lib/apt/lists/*\nRUN pip install ipython==8.12.0 --no-cache-dir --upgrade \\\n && pip install Cython==0.29.34 ipykernel==6.22.0 jupyter==1.0.0 path.py==12.5.0 Pillow==9.5.0 pygments==2.15.0 six==1.16.0 sphinx==6.1.3 wheel==0.40.0 zmq==0.0.0 protobuf==4.22.3 future==0.18.3 --no-cache-dir \\\n && python -m ipykernel.kernelspec\n#   Install Caffe\nRUN echo \"Installiere Caffe...\"\nENV CAFFE_ROOT=\"/opt/caffe\"\nWORKDIR $CAFFE_ROOT\n#   FIXME: use ARG instead of ENV once DockerHub supports this\n#   https://github.com/docker/hub-feedback/issues/460\nENV CLONE_TAG=\"1.0\"\nRUN git clone -b ${CLONE_TAG} --depth 1 https://github.com/BVLC/caffe.git . \\\n && cd python \\\n && for req in $( cat requirements.txt ;) pydot; do pip install $req ; done \\\n && cd .. \\\n && mkdir build \\\n && cd build \\\n && cmake -DCPU_ONLY=1 .. \\\n && make -j\"$( nproc ;)\"\n#   Install Caffe2\nRUN echo \"Installiere Caffe2...\"\nENV CAFFE2_ROOT=\"/opt\"\nWORKDIR $CAFFE2_ROOT\nRUN cd $CAFFE2_ROOT\nRUN git clone https://github.com/caffe2/caffe2.git \\\n && cd caffe2 \\\n && git submodule deinit -f third_party/cub \\\n && rm -rf .git/modules/third_party/cub \\\n && git rm -f third_party/cub \\\n && git submodule update --recursive --remote \\\n && git submodule update --init \\\n && mkdir build \\\n && cd build \\\n && cmake .. -DUSE_CUDA=OFF -DUSE_NNPACK=OFF -DUSE_ROCKSDB=OFF \\\n && make -j\"$( nproc ;)\" install \\\n && ldconfig \\\n && cd .. RUN pip install --upgrade pip \\\n && pip install numpy==1.24.2 protobuf==4.22.3 hypothesis==6.72.0 tflearn==0.5.0\n#  ######### INSTALLATION STEPS ###################\n#  RUN cd caffe2 && \\\n#      make && \\\n#      cd build && \\\n#      make install\nENV PYTHONPATH=\"/usr/local\"\nENV LD_LIBRARY_PATH=\"/usr/local/lib:$LD_LIBRARY_PATH\"\nRUN python -c 'from caffe2.python import core' 2> /dev/null \\\n && echo \"Success\" || echo \"Failure\"\n#   Install TensorFlow\nRUN echo \"Installiere TensorFlow...\"\nARG TENSORFLOW_VERSION=1.1.0\nARG TENSORFLOW_ARCH=cpu\n#   Install TensorFlow\nRUN pip install https://storage.googleapis.com/tensorflow/linux/${TENSORFLOW_ARCH}/tensorflow-${TENSORFLOW_VERSION}-cp27-none-linux_x86_64.whl --no-cache-dir\n#   Install Java\nRUN echo \"Installiere Java 8...\"\nARG JAVA_MAJOR_VERSION=8\nARG JAVA_UPDATE_VERSION=131\nARG JAVA_BUILD_NUMBER=11\nENV JAVA_HOME=\"/usr/jdk1.${JAVA_MAJOR_VERSION}.0_${JAVA_UPDATE_VERSION}\"\nENV PATH=\"$PATH:$JAVA_HOME/bin\"\nRUN curl -sL --retry 3 --insecure --header \"Cookie: oraclelicense=accept-securebackup-cookie;\" \"http://download.oracle.com/otn-pub/java/jdk/${JAVA_MAJOR_VERSION}u${JAVA_UPDATE_VERSION}-b${JAVA_BUILD_NUMBER}/d54c1d3a095b4ff2b6607d096fa80163/server-jre-${JAVA_MAJOR_VERSION}u${JAVA_UPDATE_VERSION}-linux-x64.tar.gz\" | gunzip | tar x -C /usr/ \\\n && ln -s $JAVA_HOME /usr/java \\\n && rm -rf $JAVA_HOME/man\n#   Install HADOOP\nRUN echo \"Installiere Hadoop...\"\nENV HADOOP_VERSION=\"2.7.3\"\nENV HADOOP_HOME=\"/usr/hadoop-$HADOOP_VERSION\"\nENV HADOOP_CONF_DIR=\"$HADOOP_HOME/etc/hadoop\"\nENV PATH=\"$PATH:$HADOOP_HOME/bin\"\nRUN curl -sL --retry 3 \"http://archive.apache.org/dist/hadoop/common/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz\" | gunzip | tar -x -C /usr/ \\\n && rm -rf $HADOOP_HOME/share/doc \\\n && chown -R root:root $HADOOP_HOME\n#   Install SPARK\nRUN echo \"Installiere Spark...\"\nENV SPARK_VERSION=\"2.1.1\"\nENV SPARK_PACKAGE=\"spark-${SPARK_VERSION}-bin-without-hadoop\"\nENV SPARK_HOME=\"/usr/spark-${SPARK_VERSION}\"\nENV SPARK_DIST_CLASSPATH=\"$HADOOP_HOME/etc/hadoop/*:$HADOOP_HOME/share/hadoop/common/lib/*:$HADOOP_HOME/share/hadoop/common/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/hdfs/lib/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/yarn/lib/*:$HADOOP_HOME/share/hadoop/yarn/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*:$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/tools/lib/*\"\nENV PATH=\"$PATH:${SPARK_HOME}/bin\"\nRUN curl -sL --retry 3 \"http://d3kbcqa49mib13.cloudfront.net/${SPARK_PACKAGE}.tgz\" | gunzip | tar x -C /usr/ \\\n && mv /usr/$SPARK_PACKAGE $SPARK_HOME \\\n && chown -R root:root $SPARK_HOME\n#   Install BigDL\nRUN echo \"Installiere BigDL...\"\nRUN pip install BigDL==0.2.0 seaborn==0.12.2 wordcloud==1.8.2.2 --no-cache-dir\n#   Install awscli\nRUN echo \"Installiere awscli...\"\nRUN pip install awscli==1.27.114 --no-cache-dir\n#   Install google cloud SDK\nRUN echo \"Installiere Google Cloud SDK...\"\nENV CLOUD_SDK_VERSION=\"165.0.0\"\nRUN apt-get update -qqy \\\n && (apt-get update ;apt-get install --no-install-recommends curl=7.47.0-1ubuntu2.19 gcc=4:5.3.1-1ubuntu1 python-dev=2.7.12-1~16.04 python-setuptools=20.7.0-1 apt-transport-https=1.2.35 lsb-release=9.20160110ubuntu0.2 openssh-client=1:7.2p2-4ubuntu2.10 git=1:2.7.4-0ubuntu1.10 -qqy ) \\\n && easy_install -U pip \\\n && pip install crcmod==1.7 -U \\\n && export CLOUD_SDK_REPO=\"cloud-sdk-$( lsb_release -c -s ;)\" \\\n && echo \"deb https://packages.cloud.google.com/apt $CLOUD_SDK_REPO main\" > /etc/apt/sources.list.d/google-cloud-sdk.list \\\n && curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - \\\n && apt-get update \\\n && (apt-get update ;apt-get install --no-install-recommends google-cloud-sdk-app-engine-python google-cloud-sdk-app-engine-java google-cloud-sdk-app-engine-go google-cloud-sdk-datalab google-cloud-sdk-datastore-emulator google-cloud-sdk-pubsub-emulator google-cloud-sdk-bigtable-emulator google-cloud-sdk-cbt kubectl google-cloud-sdk=${CLOUD_SDK_VERSION}-0 -y ) \\\n && gcloud config set core/disable_usage_reporting true \\\n && gcloud config set component_manager/disable_update_check true \\\n && gcloud config set metrics/environment github_docker_image\n#   Install SparkNet\nRUN echo \"Installiere SparkNet...\"\nWORKDIR /opt\nRUN git clone https://github.com/amplab/SparkNet.git\nENV SPARKNET_HOME=\"/opt/SparkNet\"\nWORKDIR $SPARKNET_HOME\nRUN echo \"deb https://dl.bintray.com/sbt/debian /\" | tee -a /etc/apt/sources.list.d/sbt.list\nRUN apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 2EE0EA64E40A89B84B2DF73499E82A75642AC823\nRUN :\nRUN (apt-get update ;apt-get install --no-install-recommends sbt )\n#   Set up our notebook config.\nCOPY jupyter_notebook_config.py /root/.jupyter/\n#   Copy sample notebooks.\nRUN mkdir /root/notebooks\nRUN mkdir /root/notebooks/kapitel1\nRUN mkdir /root/notebooks/kapitel2\nRUN mkdir /root/notebooks/kapitel3\nRUN mkdir /root/notebooks/kapitel4\nRUN mkdir /root/notebooks/kapitel5\nRUN mkdir /root/notebooks/kapitel6\nRUN mkdir /root/notebooks/kapitel7\nRUN mkdir /root/notebooks/kapitel8\n#  COPY notebooks/kapitel1/* /root/notebooks/kapitel1/\n#  COPY notebooks/kapitel2/* /root/notebooks/kapitel2/\n#  COPY notebooks/kapitel3/* /root/notebooks/kapitel3/\nCOPY notebooks/kapitel4/* /root/notebooks/kapitel4/\nCOPY notebooks/kapitel5/* /root/notebooks/kapitel5/\nCOPY notebooks/kapitel6/* /root/notebooks/kapitel6/\n#  COPY notebooks/kapitel7/* /root/notebooks/kapitel7/\n#  COPY notebooks/kapitel8/* /root/notebooks/kapitel8/\n#   Jupyter has issues with being run directly:\n#     https://github.com/ipython/ipython/issues/7062\n#   We just add a little wrapper script.\nCOPY run_jupyter.sh /root/\n#   TensorBoard port\nRUN echo \"TensorBoard nutzt Port 6006...\"\nEXPOSE 6006/tcp\nRUN echo \"Jupyter Notebook startet auf Port 8888...\"\nEXPOSE 8888/tcp\nRUN echo \"SparkUI nutzt Port 4040 und 4041...\"\nEXPOSE 4040/tcp\nEXPOSE 4041/tcp\nWORKDIR \"/root\"\nCMD [\"/bin/bash\"]\nRUN groupadd --system docker-user ; useradd --system --gid docker-user docker-user\nUSER docker-user\n# Please add your HEALTHCHECK here!!!\n","originalDockerfile":"FROM ubuntu:16.04\nMAINTAINER ramon@wartala.de\nRUN apt-get update \\\n && apt-get install --no-install-recommends build-essential curl cmake git wget libatlas-base-dev libboost-all-dev libfreetype6-dev libgflags-dev libgoogle-glog-dev libhdf5-serial-dev libleveldb-dev liblmdb-dev libopencv-dev libpng12-dev libzmq3-dev libprotobuf-dev libsnappy-dev pkg-config protobuf-compiler rsync software-properties-common unzip groff vim zlib1g-dev python-pydot -y \\\n && apt-get clean \\\n && apt-get autoremove \\\n && rm -rf /var/lib/apt/lists/*\nRUN curl -O https://bootstrap.pypa.io/get-pip.py \\\n && python get-pip.py \\\n && rm get-pip.py\n# RUN pip --no-cache-dir install \\\n# \t\tpyopenssl \\\n# \t\tndg-httpsclient \\\n# \t\tpyasn1\nRUN apt-get update \\\n && apt-get install python-numpy python-scipy python-nose python-h5py python-skimage python-matplotlib python-pandas python-sklearn python-sympy python-skimage python-progressbar -y \\\n && apt-get clean \\\n && apt-get autoremove \\\n && rm -rf /var/lib/apt/lists/*\nRUN pip install ipython --no-cache-dir --upgrade \\\n && pip install Cython ipykernel jupyter path.py Pillow pygments six sphinx wheel zmq protobuf future --no-cache-dir \\\n && python -m ipykernel.kernelspec\n#  Install Caffe\nRUN echo \"Installiere Caffe...\"\nENV CAFFE_ROOT=\"/opt/caffe\"\nWORKDIR $CAFFE_ROOT\n#  FIXME: use ARG instead of ENV once DockerHub supports this\n#  https://github.com/docker/hub-feedback/issues/460\nENV CLONE_TAG=\"1.0\"\nRUN git clone -b ${CLONE_TAG} --depth 1 https://github.com/BVLC/caffe.git . \\\n && cd python \\\n && for req in $( cat requirements.txt ;) pydot; do pip install $req ; done \\\n && cd .. \\\n && mkdir build \\\n && cd build \\\n && cmake -DCPU_ONLY=1 .. \\\n && make -j\"$( nproc ;)\"\n#  Install Caffe2\nRUN echo \"Installiere Caffe2...\"\nENV CAFFE2_ROOT=\"/opt\"\nWORKDIR $CAFFE2_ROOT\nRUN cd $CAFFE2_ROOT\nRUN git clone https://github.com/caffe2/caffe2.git \\\n && cd caffe2 \\\n && git submodule deinit -f third_party/cub \\\n && rm -rf .git/modules/third_party/cub \\\n && git rm -f third_party/cub \\\n && git submodule update --recursive --remote \\\n && git submodule update --init \\\n && mkdir build \\\n && cd build \\\n && cmake .. -DUSE_CUDA=OFF -DUSE_NNPACK=OFF -DUSE_ROCKSDB=OFF \\\n && make -j\"$( nproc ;)\" install \\\n && ldconfig \\\n && cd .. RUN pip install --upgrade pip \\\n && pip install numpy protobuf hypothesis tflearn\n# ######### INSTALLATION STEPS ###################\n# RUN cd caffe2 && \\\n#     make && \\\n#     cd build && \\\n#     make install\nENV PYTHONPATH=\"/usr/local\"\nENV LD_LIBRARY_PATH=\"/usr/local/lib:$LD_LIBRARY_PATH\"\nRUN python -c 'from caffe2.python import core' 2> /dev/null \\\n && echo \"Success\" || echo \"Failure\"\n#  Install TensorFlow\nRUN echo \"Installiere TensorFlow...\"\nARG TENSORFLOW_VERSION=1.1.0\nARG TENSORFLOW_ARCH=cpu\n#  Install TensorFlow\nRUN pip install https://storage.googleapis.com/tensorflow/linux/${TENSORFLOW_ARCH}/tensorflow-${TENSORFLOW_VERSION}-cp27-none-linux_x86_64.whl --no-cache-dir\n#  Install Java\nRUN echo \"Installiere Java 8...\"\nARG JAVA_MAJOR_VERSION=8\nARG JAVA_UPDATE_VERSION=131\nARG JAVA_BUILD_NUMBER=11\nENV JAVA_HOME=\"/usr/jdk1.${JAVA_MAJOR_VERSION}.0_${JAVA_UPDATE_VERSION}\"\nENV PATH=\"$PATH:$JAVA_HOME/bin\"\nRUN curl -sL --retry 3 --insecure --header \"Cookie: oraclelicense=accept-securebackup-cookie;\" \"http://download.oracle.com/otn-pub/java/jdk/${JAVA_MAJOR_VERSION}u${JAVA_UPDATE_VERSION}-b${JAVA_BUILD_NUMBER}/d54c1d3a095b4ff2b6607d096fa80163/server-jre-${JAVA_MAJOR_VERSION}u${JAVA_UPDATE_VERSION}-linux-x64.tar.gz\" | gunzip | tar x -C /usr/ \\\n && ln -s $JAVA_HOME /usr/java \\\n && rm -rf $JAVA_HOME/man\n#  Install HADOOP\nRUN echo \"Installiere Hadoop...\"\nENV HADOOP_VERSION=\"2.7.3\"\nENV HADOOP_HOME=\"/usr/hadoop-$HADOOP_VERSION\"\nENV HADOOP_CONF_DIR=\"$HADOOP_HOME/etc/hadoop\"\nENV PATH=\"$PATH:$HADOOP_HOME/bin\"\nRUN curl -sL --retry 3 \"http://archive.apache.org/dist/hadoop/common/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz\" | gunzip | tar -x -C /usr/ \\\n && rm -rf $HADOOP_HOME/share/doc \\\n && chown -R root:root $HADOOP_HOME\n#  Install SPARK\nRUN echo \"Installiere Spark...\"\nENV SPARK_VERSION=\"2.1.1\"\nENV SPARK_PACKAGE=\"spark-${SPARK_VERSION}-bin-without-hadoop\"\nENV SPARK_HOME=\"/usr/spark-${SPARK_VERSION}\"\nENV SPARK_DIST_CLASSPATH=\"$HADOOP_HOME/etc/hadoop/*:$HADOOP_HOME/share/hadoop/common/lib/*:$HADOOP_HOME/share/hadoop/common/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/hdfs/lib/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/yarn/lib/*:$HADOOP_HOME/share/hadoop/yarn/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*:$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/tools/lib/*\"\nENV PATH=\"$PATH:${SPARK_HOME}/bin\"\nRUN curl -sL --retry 3 \"http://d3kbcqa49mib13.cloudfront.net/${SPARK_PACKAGE}.tgz\" | gunzip | tar x -C /usr/ \\\n && mv /usr/$SPARK_PACKAGE $SPARK_HOME \\\n && chown -R root:root $SPARK_HOME\n#  Install BigDL\nRUN echo \"Installiere BigDL...\"\nRUN pip install BigDL==0.2.0 seaborn wordcloud --no-cache-dir\n#  Install awscli\nRUN echo \"Installiere awscli...\"\nRUN pip install awscli --no-cache-dir\n#  Install google cloud SDK\nRUN echo \"Installiere Google Cloud SDK...\"\nENV CLOUD_SDK_VERSION=\"165.0.0\"\nRUN apt-get update -qqy \\\n && apt-get install curl gcc python-dev python-setuptools apt-transport-https lsb-release openssh-client git -qqy \\\n && easy_install -U pip \\\n && pip install crcmod -U \\\n && export CLOUD_SDK_REPO=\"cloud-sdk-$( lsb_release -c -s ;)\" \\\n && echo \"deb https://packages.cloud.google.com/apt $CLOUD_SDK_REPO main\" > /etc/apt/sources.list.d/google-cloud-sdk.list \\\n && curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - \\\n && apt-get update \\\n && apt-get install google-cloud-sdk-app-engine-python google-cloud-sdk-app-engine-java google-cloud-sdk-app-engine-go google-cloud-sdk-datalab google-cloud-sdk-datastore-emulator google-cloud-sdk-pubsub-emulator google-cloud-sdk-bigtable-emulator google-cloud-sdk-cbt kubectl google-cloud-sdk=${CLOUD_SDK_VERSION}-0 -y \\\n && gcloud config set core/disable_usage_reporting true \\\n && gcloud config set component_manager/disable_update_check true \\\n && gcloud config set metrics/environment github_docker_image\n#  Install SparkNet\nRUN echo \"Installiere SparkNet...\"\nWORKDIR /opt\nRUN git clone https://github.com/amplab/SparkNet.git\nENV SPARKNET_HOME=\"/opt/SparkNet\"\nWORKDIR $SPARKNET_HOME\nRUN echo \"deb https://dl.bintray.com/sbt/debian /\" | tee -a /etc/apt/sources.list.d/sbt.list\nRUN apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 2EE0EA64E40A89B84B2DF73499E82A75642AC823\nRUN apt-get update\nRUN apt-get install sbt\n#  Set up our notebook config.\nCOPY jupyter_notebook_config.py /root/.jupyter/\n#  Copy sample notebooks.\nRUN mkdir /root/notebooks\nRUN mkdir /root/notebooks/kapitel1\nRUN mkdir /root/notebooks/kapitel2\nRUN mkdir /root/notebooks/kapitel3\nRUN mkdir /root/notebooks/kapitel4\nRUN mkdir /root/notebooks/kapitel5\nRUN mkdir /root/notebooks/kapitel6\nRUN mkdir /root/notebooks/kapitel7\nRUN mkdir /root/notebooks/kapitel8\n# COPY notebooks/kapitel1/* /root/notebooks/kapitel1/\n# COPY notebooks/kapitel2/* /root/notebooks/kapitel2/\n# COPY notebooks/kapitel3/* /root/notebooks/kapitel3/\nCOPY notebooks/kapitel4/* /root/notebooks/kapitel4/\nCOPY notebooks/kapitel5/* /root/notebooks/kapitel5/\nCOPY notebooks/kapitel6/* /root/notebooks/kapitel6/\n# COPY notebooks/kapitel7/* /root/notebooks/kapitel7/\n# COPY notebooks/kapitel8/* /root/notebooks/kapitel8/\n#  Jupyter has issues with being run directly:\n#    https://github.com/ipython/ipython/issues/7062\n#  We just add a little wrapper script.\nCOPY run_jupyter.sh /root/\n#  TensorBoard port\nRUN echo \"TensorBoard nutzt Port 6006...\"\nEXPOSE 6006/tcp\nRUN echo \"Jupyter Notebook startet auf Port 8888...\"\nEXPOSE 8888/tcp\nRUN echo \"SparkUI nutzt Port 4040 und 4041...\"\nEXPOSE 4040/tcp\nEXPOSE 4041/tcp\nWORKDIR \"/root\"\nCMD [\"/bin/bash\"]\n","injectedSmells":[],"originalDockerfileHash":"9b1f3516c917b78474f6c2bf1e7251bd","successfullyInjectedSmells":[],"originalDockerfileUglified":"FROM ubuntu:16.04\nMAINTAINER ramon@wartala.de\nRUN apt-get update \\\n && apt-get install --no-install-recommends build-essential curl cmake git wget libatlas-base-dev libboost-all-dev libfreetype6-dev libgflags-dev libgoogle-glog-dev libhdf5-serial-dev libleveldb-dev liblmdb-dev libopencv-dev libpng12-dev libzmq3-dev libprotobuf-dev libsnappy-dev pkg-config protobuf-compiler rsync software-properties-common unzip groff vim zlib1g-dev python-pydot -y \\\n && apt-get clean \\\n && apt-get autoremove \\\n && rm -rf /var/lib/apt/lists/*\nRUN curl -O https://bootstrap.pypa.io/get-pip.py \\\n && python get-pip.py \\\n && rm get-pip.py\n#  RUN pip --no-cache-dir install \\\n#  \t\tpyopenssl \\\n#  \t\tndg-httpsclient \\\n#  \t\tpyasn1\nRUN apt-get update \\\n && apt-get install python-numpy python-scipy python-nose python-h5py python-skimage python-matplotlib python-pandas python-sklearn python-sympy python-skimage python-progressbar -y \\\n && apt-get clean \\\n && apt-get autoremove \\\n && rm -rf /var/lib/apt/lists/*\nRUN pip install ipython --no-cache-dir --upgrade \\\n && pip install Cython ipykernel jupyter path.py Pillow pygments six sphinx wheel zmq protobuf future --no-cache-dir \\\n && python -m ipykernel.kernelspec\n#   Install Caffe\nRUN echo \"Installiere Caffe...\"\nENV CAFFE_ROOT=\"/opt/caffe\"\nWORKDIR $CAFFE_ROOT\n#   FIXME: use ARG instead of ENV once DockerHub supports this\n#   https://github.com/docker/hub-feedback/issues/460\nENV CLONE_TAG=\"1.0\"\nRUN git clone -b ${CLONE_TAG} --depth 1 https://github.com/BVLC/caffe.git . \\\n && cd python \\\n && for req in $( cat requirements.txt ;) pydot; do pip install $req ; done \\\n && cd .. \\\n && mkdir build \\\n && cd build \\\n && cmake -DCPU_ONLY=1 .. \\\n && make -j\"$( nproc ;)\"\n#   Install Caffe2\nRUN echo \"Installiere Caffe2...\"\nENV CAFFE2_ROOT=\"/opt\"\nWORKDIR $CAFFE2_ROOT\nRUN cd $CAFFE2_ROOT\nRUN git clone https://github.com/caffe2/caffe2.git \\\n && cd caffe2 \\\n && git submodule deinit -f third_party/cub \\\n && rm -rf .git/modules/third_party/cub \\\n && git rm -f third_party/cub \\\n && git submodule update --recursive --remote \\\n && git submodule update --init \\\n && mkdir build \\\n && cd build \\\n && cmake .. -DUSE_CUDA=OFF -DUSE_NNPACK=OFF -DUSE_ROCKSDB=OFF \\\n && make -j\"$( nproc ;)\" install \\\n && ldconfig \\\n && cd .. RUN pip install --upgrade pip \\\n && pip install numpy protobuf hypothesis tflearn\n#  ######### INSTALLATION STEPS ###################\n#  RUN cd caffe2 && \\\n#      make && \\\n#      cd build && \\\n#      make install\nENV PYTHONPATH=\"/usr/local\"\nENV LD_LIBRARY_PATH=\"/usr/local/lib:$LD_LIBRARY_PATH\"\nRUN python -c 'from caffe2.python import core' 2> /dev/null \\\n && echo \"Success\" || echo \"Failure\"\n#   Install TensorFlow\nRUN echo \"Installiere TensorFlow...\"\nARG TENSORFLOW_VERSION=1.1.0\nARG TENSORFLOW_ARCH=cpu\n#   Install TensorFlow\nRUN pip install https://storage.googleapis.com/tensorflow/linux/${TENSORFLOW_ARCH}/tensorflow-${TENSORFLOW_VERSION}-cp27-none-linux_x86_64.whl --no-cache-dir\n#   Install Java\nRUN echo \"Installiere Java 8...\"\nARG JAVA_MAJOR_VERSION=8\nARG JAVA_UPDATE_VERSION=131\nARG JAVA_BUILD_NUMBER=11\nENV JAVA_HOME=\"/usr/jdk1.${JAVA_MAJOR_VERSION}.0_${JAVA_UPDATE_VERSION}\"\nENV PATH=\"$PATH:$JAVA_HOME/bin\"\nRUN curl -sL --retry 3 --insecure --header \"Cookie: oraclelicense=accept-securebackup-cookie;\" \"http://download.oracle.com/otn-pub/java/jdk/${JAVA_MAJOR_VERSION}u${JAVA_UPDATE_VERSION}-b${JAVA_BUILD_NUMBER}/d54c1d3a095b4ff2b6607d096fa80163/server-jre-${JAVA_MAJOR_VERSION}u${JAVA_UPDATE_VERSION}-linux-x64.tar.gz\" | gunzip | tar x -C /usr/ \\\n && ln -s $JAVA_HOME /usr/java \\\n && rm -rf $JAVA_HOME/man\n#   Install HADOOP\nRUN echo \"Installiere Hadoop...\"\nENV HADOOP_VERSION=\"2.7.3\"\nENV HADOOP_HOME=\"/usr/hadoop-$HADOOP_VERSION\"\nENV HADOOP_CONF_DIR=\"$HADOOP_HOME/etc/hadoop\"\nENV PATH=\"$PATH:$HADOOP_HOME/bin\"\nRUN curl -sL --retry 3 \"http://archive.apache.org/dist/hadoop/common/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz\" | gunzip | tar -x -C /usr/ \\\n && rm -rf $HADOOP_HOME/share/doc \\\n && chown -R root:root $HADOOP_HOME\n#   Install SPARK\nRUN echo \"Installiere Spark...\"\nENV SPARK_VERSION=\"2.1.1\"\nENV SPARK_PACKAGE=\"spark-${SPARK_VERSION}-bin-without-hadoop\"\nENV SPARK_HOME=\"/usr/spark-${SPARK_VERSION}\"\nENV SPARK_DIST_CLASSPATH=\"$HADOOP_HOME/etc/hadoop/*:$HADOOP_HOME/share/hadoop/common/lib/*:$HADOOP_HOME/share/hadoop/common/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/hdfs/lib/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/yarn/lib/*:$HADOOP_HOME/share/hadoop/yarn/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*:$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/tools/lib/*\"\nENV PATH=\"$PATH:${SPARK_HOME}/bin\"\nRUN curl -sL --retry 3 \"http://d3kbcqa49mib13.cloudfront.net/${SPARK_PACKAGE}.tgz\" | gunzip | tar x -C /usr/ \\\n && mv /usr/$SPARK_PACKAGE $SPARK_HOME \\\n && chown -R root:root $SPARK_HOME\n#   Install BigDL\nRUN echo \"Installiere BigDL...\"\nRUN pip install BigDL==0.2.0 seaborn wordcloud --no-cache-dir\n#   Install awscli\nRUN echo \"Installiere awscli...\"\nRUN pip install awscli --no-cache-dir\n#   Install google cloud SDK\nRUN echo \"Installiere Google Cloud SDK...\"\nENV CLOUD_SDK_VERSION=\"165.0.0\"\nRUN apt-get update -qqy \\\n && apt-get install curl gcc python-dev python-setuptools apt-transport-https lsb-release openssh-client git -qqy \\\n && easy_install -U pip \\\n && pip install crcmod -U \\\n && export CLOUD_SDK_REPO=\"cloud-sdk-$( lsb_release -c -s ;)\" \\\n && echo \"deb https://packages.cloud.google.com/apt $CLOUD_SDK_REPO main\" > /etc/apt/sources.list.d/google-cloud-sdk.list \\\n && curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - \\\n && apt-get update \\\n && apt-get install google-cloud-sdk-app-engine-python google-cloud-sdk-app-engine-java google-cloud-sdk-app-engine-go google-cloud-sdk-datalab google-cloud-sdk-datastore-emulator google-cloud-sdk-pubsub-emulator google-cloud-sdk-bigtable-emulator google-cloud-sdk-cbt kubectl google-cloud-sdk=${CLOUD_SDK_VERSION}-0 -y \\\n && gcloud config set core/disable_usage_reporting true \\\n && gcloud config set component_manager/disable_update_check true \\\n && gcloud config set metrics/environment github_docker_image\n#   Install SparkNet\nRUN echo \"Installiere SparkNet...\"\nWORKDIR /opt\nRUN git clone https://github.com/amplab/SparkNet.git\nENV SPARKNET_HOME=\"/opt/SparkNet\"\nWORKDIR $SPARKNET_HOME\nRUN echo \"deb https://dl.bintray.com/sbt/debian /\" | tee -a /etc/apt/sources.list.d/sbt.list\nRUN apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 2EE0EA64E40A89B84B2DF73499E82A75642AC823\nRUN apt-get update\nRUN apt-get install sbt\n#   Set up our notebook config.\nCOPY jupyter_notebook_config.py /root/.jupyter/\n#   Copy sample notebooks.\nRUN mkdir /root/notebooks\nRUN mkdir /root/notebooks/kapitel1\nRUN mkdir /root/notebooks/kapitel2\nRUN mkdir /root/notebooks/kapitel3\nRUN mkdir /root/notebooks/kapitel4\nRUN mkdir /root/notebooks/kapitel5\nRUN mkdir /root/notebooks/kapitel6\nRUN mkdir /root/notebooks/kapitel7\nRUN mkdir /root/notebooks/kapitel8\n#  COPY notebooks/kapitel1/* /root/notebooks/kapitel1/\n#  COPY notebooks/kapitel2/* /root/notebooks/kapitel2/\n#  COPY notebooks/kapitel3/* /root/notebooks/kapitel3/\nCOPY notebooks/kapitel4/* /root/notebooks/kapitel4/\nCOPY notebooks/kapitel5/* /root/notebooks/kapitel5/\nCOPY notebooks/kapitel6/* /root/notebooks/kapitel6/\n#  COPY notebooks/kapitel7/* /root/notebooks/kapitel7/\n#  COPY notebooks/kapitel8/* /root/notebooks/kapitel8/\n#   Jupyter has issues with being run directly:\n#     https://github.com/ipython/ipython/issues/7062\n#   We just add a little wrapper script.\nCOPY run_jupyter.sh /root/\n#   TensorBoard port\nRUN echo \"TensorBoard nutzt Port 6006...\"\nEXPOSE 6006/tcp\nRUN echo \"Jupyter Notebook startet auf Port 8888...\"\nEXPOSE 8888/tcp\nRUN echo \"SparkUI nutzt Port 4040 und 4041...\"\nEXPOSE 4040/tcp\nEXPOSE 4041/tcp\nWORKDIR \"/root\"\nCMD [\"/bin/bash\"]\n","originalDockerfileUglifiedHash":"273f6a7843f0483e07e4f6365648161c","fileName":"/ICSME-replicationpackage/dataset/smelly_dockerfiles_bianncle/124e7542e909d1b8a513b2b66604ec0d5d60e88d.dockerfile"}