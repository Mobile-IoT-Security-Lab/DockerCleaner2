{"seed":3034358423,"processedDockerfileHash":"312cbb4ee4844caaf55d355eece5c4fc","fixedSmells":["use-no-install-recommends","do-not-use-apt-get-update-alone","pin-package-manager-versions-apt-get","pin-package-manager-versions-pip","pin-package-manager-versions-npm","pin-package-manager-versions-gem","pin-package-manager-versions-apk","use-copy-instead-of-add","use-wget-instead-of-add","do-not-have-secrets","have-a-healthcheck","have-a-user"],"successfullyFixedSmells":["use-no-install-recommends","have-a-healthcheck","have-a-user"],"processedDockerfile":"#   pull base image\nFROM harbor.h2o.ai/opsh2oai/h2o-3/dev-r-3.4.1-jdk-8:10\n#   maintainer details\nMAINTAINER h2oai \"h2o.ai\"\nARG VERSION\nARG PATH_PREFIX='.'\nARG PYTHON_VERSIONS='2.7,3.6'\nARG HIVE_PACKAGE='hive'\nENV DISTRIBUTION=\"hdp\" \\\n    MASTER=\"yarn-client\" \\\n    HADOOP_HOME=\"/usr/hdp/current/hadoop-client/\" \\\n    HADOOP_CONF_DIR=\"/etc/hadoop/conf\" \\\n    MAPRED_USER=\"mapred\" \\\n    YARN_USER=\"yarn\" \\\n    YARN_CONF_DIR=\"/etc/hadoop/conf\" \\\n    HDFS_USER=\"hdfs\" \\\n    HIVE_PACKAGE=\"${HIVE_PACKAGE:-hive}\"\n#   Copy bin and sbin scripts\nCOPY ${PATH_PREFIX}/scripts/sbin ${PATH_PREFIX}/../common/sbin scripts/install_python_version /usr/sbin/\n#   Add HDP repository and install packages\nRUN apt-get update \\\n && apt-get install --no-install-recommends wget curl software-properties-common git -y \\\n && chmod 700 /usr/sbin/add_hdp_repo.sh \\\n && sync \\\n && /usr/sbin/add_hdp_repo.sh $VERSION \\\n && rm /usr/sbin/add_hdp_repo.sh \\\n && add-apt-repository -y ppa:deadsnakes \\\n && apt-get update \\\n && DEBIAN_FRONTEND=noninteractive apt-get install -y hadoop-conf-pseudo python-pip python-dev python-virtualenv mysql-server libmysql-java libmysqlclient-dev sudo unzip html2text slapd ldap-utils libkrb5-dev vim\n#   Create hive user\nRUN adduser --disabled-password --gecos \"\" hive\nARG H2O_BRANCH=master\nENV H2O_BRANCH=\"${H2O_BRANCH}\"\n#   Set required env vars and install Pythons\nRUN chmod 700 /usr/sbin/install_python_version \\\n && sync \\\n && /usr/sbin/install_python_version \\\n && /usr/bin/activate_java_8\n#   Chown folders\nRUN HDP_VERSION=$( ls /usr/hdp/ | grep -e '^2\\|^3' ;) \\\n && chown hdfs:hdfs /usr/hdp/${HDP_VERSION}/hadoop \\\n && chown yarn:yarn /usr/hdp/${HDP_VERSION}/hadoop-yarn \\\n && chown yarn:yarn /usr/hdp/${HDP_VERSION}/hadoop-mapreduce \\\n && chown -R root:hadoop /usr/hdp/current/hadoop-yarn*/bin/container-executor \\\n && chmod -R 6050 /usr/hdp/current/hadoop-yarn*/bin/container-executor \\\n && mkdir -p /usr/hdp/${HDP_VERSION}/hadoop/logs \\\n && chown hdfs:hdfs /usr/hdp/${HDP_VERSION}/hadoop/logs \\\n && chmod a+w /usr/hdp/${HDP_VERSION}/hadoop/logs\n#   Copy conf.pseudo to hadoop conf folder\nRUN rm /usr/hdp/*/hadoop/conf/* \\\n && cp /usr/hdp/*/etc/hadoop/conf.pseudo/* /usr/hdp/*/hadoop/conf/\n#   Copy hadoop configs\nCOPY ${PATH_PREFIX}/conf/ ${HADOOP_CONF_DIR}\n#   Generate mapred-site.xml\nRUN chmod 700 /usr/sbin/generate-mapred-site \\\n && sync \\\n && /usr/sbin/generate-mapred-site \\\n && rm /usr/sbin/generate-mapred-site\n#   Generate yarn-site.xml\nRUN chmod 700 /usr/sbin/generate-yarn-site \\\n && sync \\\n && /usr/sbin/generate-yarn-site \\\n && rm /usr/sbin/generate-yarn-site\n#   Format namenode\nRUN su - hdfs -c \"/usr/hdp/current/hadoop-hdfs-namenode/../hadoop/bin/hdfs namenode -format\"\n#   Copy startup scripts\nCOPY ${PATH_PREFIX}/scripts/startup ${PATH_PREFIX}/../common/startup /etc/startup/\n#   Copy sudoers so we can start hadoop stuff without root access to container\nCOPY ${PATH_PREFIX}/../common/sudoers/jenkins /etc/sudoers.d/jenkins\nCOPY ${PATH_PREFIX}/../common/hive-scripts /opt/hive-scripts/\nCOPY ${PATH_PREFIX}/../common/ldap /opt/ldap-scripts/\nRUN chmod 700 /usr/sbin/startup.sh \\\n && chown -R hive:hive /opt/hive-scripts \\\n && chmod +x /usr/sbin/install_hive.sh \\\n && chmod 700 /usr/sbin/install_ldap.sh \\\n && sync \\\n && /usr/sbin/install_hive.sh\n#   Copy hive configs\nCOPY ${PATH_PREFIX}/../common/conf-hive/ /etc/${HIVE_PACKAGE}/conf/\nCOPY ${PATH_PREFIX}/conf-tez/ /etc/tez/conf/\nRUN /usr/sbin/install_ldap.sh\n#   Expose ports\n#   H2O, Hadoop UI, Hive, LDAP\nEXPOSE 54321/tcp 8088/tcp 10000/tcp 389/tcp\n#   Remove hadoop pids\nRUN rm -f tmp/*.pid /var/run/hadoop-hdfs/*.pid\nRUN groupadd --system docker-user ; useradd --system --gid docker-user docker-user\nUSER docker-user\n# Please add your HEALTHCHECK here!!!\n","originalDockerfile":"#  pull base image\nFROM harbor.h2o.ai/opsh2oai/h2o-3/dev-r-3.4.1-jdk-8:10\n#  maintainer details\nMAINTAINER h2oai \"h2o.ai\"\nARG VERSION\nARG PATH_PREFIX='.'\nARG PYTHON_VERSIONS='2.7,3.6'\nARG HIVE_PACKAGE='hive'\nENV DISTRIBUTION=\"hdp\" \\\n    MASTER=\"yarn-client\" \\\n    HADOOP_HOME=\"/usr/hdp/current/hadoop-client/\" \\\n    HADOOP_CONF_DIR=\"/etc/hadoop/conf\" \\\n    MAPRED_USER=\"mapred\" \\\n    YARN_USER=\"yarn\" \\\n    YARN_CONF_DIR=\"/etc/hadoop/conf\" \\\n    HDFS_USER=\"hdfs\" \\\n    HIVE_PACKAGE=\"${HIVE_PACKAGE:-hive}\"\n#  Copy bin and sbin scripts\nCOPY ${PATH_PREFIX}/scripts/sbin ${PATH_PREFIX}/../common/sbin scripts/install_python_version /usr/sbin/\n#  Add HDP repository and install packages\nRUN apt-get update \\\n && apt-get install wget curl software-properties-common git -y \\\n && chmod 700 /usr/sbin/add_hdp_repo.sh \\\n && sync \\\n && /usr/sbin/add_hdp_repo.sh $VERSION \\\n && rm /usr/sbin/add_hdp_repo.sh \\\n && add-apt-repository -y ppa:deadsnakes \\\n && apt-get update \\\n && DEBIAN_FRONTEND=noninteractive apt-get install -y hadoop-conf-pseudo python-pip python-dev python-virtualenv mysql-server libmysql-java libmysqlclient-dev sudo unzip html2text slapd ldap-utils libkrb5-dev vim\n#  Create hive user\nRUN adduser --disabled-password --gecos \"\" hive\nARG H2O_BRANCH=master\nENV H2O_BRANCH=\"${H2O_BRANCH}\"\n#  Set required env vars and install Pythons\nRUN chmod 700 /usr/sbin/install_python_version \\\n && sync \\\n && /usr/sbin/install_python_version \\\n && /usr/bin/activate_java_8\n#  Chown folders\nRUN HDP_VERSION=$( ls /usr/hdp/ | grep -e '^2\\|^3' ;) \\\n && chown hdfs:hdfs /usr/hdp/${HDP_VERSION}/hadoop \\\n && chown yarn:yarn /usr/hdp/${HDP_VERSION}/hadoop-yarn \\\n && chown yarn:yarn /usr/hdp/${HDP_VERSION}/hadoop-mapreduce \\\n && chown -R root:hadoop /usr/hdp/current/hadoop-yarn*/bin/container-executor \\\n && chmod -R 6050 /usr/hdp/current/hadoop-yarn*/bin/container-executor \\\n && mkdir -p /usr/hdp/${HDP_VERSION}/hadoop/logs \\\n && chown hdfs:hdfs /usr/hdp/${HDP_VERSION}/hadoop/logs \\\n && chmod a+w /usr/hdp/${HDP_VERSION}/hadoop/logs\n#  Copy conf.pseudo to hadoop conf folder\nRUN rm /usr/hdp/*/hadoop/conf/* \\\n && cp /usr/hdp/*/etc/hadoop/conf.pseudo/* /usr/hdp/*/hadoop/conf/\n#  Copy hadoop configs\nCOPY ${PATH_PREFIX}/conf/ ${HADOOP_CONF_DIR}\n#  Generate mapred-site.xml\nRUN chmod 700 /usr/sbin/generate-mapred-site \\\n && sync \\\n && /usr/sbin/generate-mapred-site \\\n && rm /usr/sbin/generate-mapred-site\n#  Generate yarn-site.xml\nRUN chmod 700 /usr/sbin/generate-yarn-site \\\n && sync \\\n && /usr/sbin/generate-yarn-site \\\n && rm /usr/sbin/generate-yarn-site\n#  Format namenode\nRUN su - hdfs -c \"/usr/hdp/current/hadoop-hdfs-namenode/../hadoop/bin/hdfs namenode -format\"\n#  Copy startup scripts\nCOPY ${PATH_PREFIX}/scripts/startup ${PATH_PREFIX}/../common/startup /etc/startup/\n#  Copy sudoers so we can start hadoop stuff without root access to container\nCOPY ${PATH_PREFIX}/../common/sudoers/jenkins /etc/sudoers.d/jenkins\nCOPY ${PATH_PREFIX}/../common/hive-scripts /opt/hive-scripts/\nCOPY ${PATH_PREFIX}/../common/ldap /opt/ldap-scripts/\nRUN chmod 700 /usr/sbin/startup.sh \\\n && chown -R hive:hive /opt/hive-scripts \\\n && chmod +x /usr/sbin/install_hive.sh \\\n && chmod 700 /usr/sbin/install_ldap.sh \\\n && sync \\\n && /usr/sbin/install_hive.sh\n#  Copy hive configs\nCOPY ${PATH_PREFIX}/../common/conf-hive/ /etc/${HIVE_PACKAGE}/conf/\nCOPY ${PATH_PREFIX}/conf-tez/ /etc/tez/conf/\nRUN /usr/sbin/install_ldap.sh\n#  Expose ports\n#  H2O, Hadoop UI, Hive, LDAP\nEXPOSE 54321/tcp 8088/tcp 10000/tcp 389/tcp\n#  Remove hadoop pids\nRUN rm -f tmp/*.pid /var/run/hadoop-hdfs/*.pid\n","injectedSmells":[],"originalDockerfileHash":"1722101ff57b04e708d856b8b4dca297","successfullyInjectedSmells":[],"originalDockerfileUglified":"#   pull base image\nFROM harbor.h2o.ai/opsh2oai/h2o-3/dev-r-3.4.1-jdk-8:10\n#   maintainer details\nMAINTAINER h2oai \"h2o.ai\"\nARG VERSION\nARG PATH_PREFIX='.'\nARG PYTHON_VERSIONS='2.7,3.6'\nARG HIVE_PACKAGE='hive'\nENV DISTRIBUTION=\"hdp\" \\\n    MASTER=\"yarn-client\" \\\n    HADOOP_HOME=\"/usr/hdp/current/hadoop-client/\" \\\n    HADOOP_CONF_DIR=\"/etc/hadoop/conf\" \\\n    MAPRED_USER=\"mapred\" \\\n    YARN_USER=\"yarn\" \\\n    YARN_CONF_DIR=\"/etc/hadoop/conf\" \\\n    HDFS_USER=\"hdfs\" \\\n    HIVE_PACKAGE=\"${HIVE_PACKAGE:-hive}\"\n#   Copy bin and sbin scripts\nCOPY ${PATH_PREFIX}/scripts/sbin ${PATH_PREFIX}/../common/sbin scripts/install_python_version /usr/sbin/\n#   Add HDP repository and install packages\nRUN apt-get update \\\n && apt-get install wget curl software-properties-common git -y \\\n && chmod 700 /usr/sbin/add_hdp_repo.sh \\\n && sync \\\n && /usr/sbin/add_hdp_repo.sh $VERSION \\\n && rm /usr/sbin/add_hdp_repo.sh \\\n && add-apt-repository -y ppa:deadsnakes \\\n && apt-get update \\\n && DEBIAN_FRONTEND=noninteractive apt-get install -y hadoop-conf-pseudo python-pip python-dev python-virtualenv mysql-server libmysql-java libmysqlclient-dev sudo unzip html2text slapd ldap-utils libkrb5-dev vim\n#   Create hive user\nRUN adduser --disabled-password --gecos \"\" hive\nARG H2O_BRANCH=master\nENV H2O_BRANCH=\"${H2O_BRANCH}\"\n#   Set required env vars and install Pythons\nRUN chmod 700 /usr/sbin/install_python_version \\\n && sync \\\n && /usr/sbin/install_python_version \\\n && /usr/bin/activate_java_8\n#   Chown folders\nRUN HDP_VERSION=$( ls /usr/hdp/ | grep -e '^2\\|^3' ;) \\\n && chown hdfs:hdfs /usr/hdp/${HDP_VERSION}/hadoop \\\n && chown yarn:yarn /usr/hdp/${HDP_VERSION}/hadoop-yarn \\\n && chown yarn:yarn /usr/hdp/${HDP_VERSION}/hadoop-mapreduce \\\n && chown -R root:hadoop /usr/hdp/current/hadoop-yarn*/bin/container-executor \\\n && chmod -R 6050 /usr/hdp/current/hadoop-yarn*/bin/container-executor \\\n && mkdir -p /usr/hdp/${HDP_VERSION}/hadoop/logs \\\n && chown hdfs:hdfs /usr/hdp/${HDP_VERSION}/hadoop/logs \\\n && chmod a+w /usr/hdp/${HDP_VERSION}/hadoop/logs\n#   Copy conf.pseudo to hadoop conf folder\nRUN rm /usr/hdp/*/hadoop/conf/* \\\n && cp /usr/hdp/*/etc/hadoop/conf.pseudo/* /usr/hdp/*/hadoop/conf/\n#   Copy hadoop configs\nCOPY ${PATH_PREFIX}/conf/ ${HADOOP_CONF_DIR}\n#   Generate mapred-site.xml\nRUN chmod 700 /usr/sbin/generate-mapred-site \\\n && sync \\\n && /usr/sbin/generate-mapred-site \\\n && rm /usr/sbin/generate-mapred-site\n#   Generate yarn-site.xml\nRUN chmod 700 /usr/sbin/generate-yarn-site \\\n && sync \\\n && /usr/sbin/generate-yarn-site \\\n && rm /usr/sbin/generate-yarn-site\n#   Format namenode\nRUN su - hdfs -c \"/usr/hdp/current/hadoop-hdfs-namenode/../hadoop/bin/hdfs namenode -format\"\n#   Copy startup scripts\nCOPY ${PATH_PREFIX}/scripts/startup ${PATH_PREFIX}/../common/startup /etc/startup/\n#   Copy sudoers so we can start hadoop stuff without root access to container\nCOPY ${PATH_PREFIX}/../common/sudoers/jenkins /etc/sudoers.d/jenkins\nCOPY ${PATH_PREFIX}/../common/hive-scripts /opt/hive-scripts/\nCOPY ${PATH_PREFIX}/../common/ldap /opt/ldap-scripts/\nRUN chmod 700 /usr/sbin/startup.sh \\\n && chown -R hive:hive /opt/hive-scripts \\\n && chmod +x /usr/sbin/install_hive.sh \\\n && chmod 700 /usr/sbin/install_ldap.sh \\\n && sync \\\n && /usr/sbin/install_hive.sh\n#   Copy hive configs\nCOPY ${PATH_PREFIX}/../common/conf-hive/ /etc/${HIVE_PACKAGE}/conf/\nCOPY ${PATH_PREFIX}/conf-tez/ /etc/tez/conf/\nRUN /usr/sbin/install_ldap.sh\n#   Expose ports\n#   H2O, Hadoop UI, Hive, LDAP\nEXPOSE 54321/tcp 8088/tcp 10000/tcp 389/tcp\n#   Remove hadoop pids\nRUN rm -f tmp/*.pid /var/run/hadoop-hdfs/*.pid\n","originalDockerfileUglifiedHash":"273a6857e873fd0e6ac3642443e75eb2","fileName":"/ICSME-replicationpackage/dataset/smelly_dockerfiles_bianncle/43394630c1ea0c2a83ea3aa100936370651bf2fc.dockerfile"}