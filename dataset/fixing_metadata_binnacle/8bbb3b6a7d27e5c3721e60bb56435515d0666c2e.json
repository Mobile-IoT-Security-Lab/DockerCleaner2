{"seed":336721594,"processedDockerfileHash":"2eab541c80e78911279738936e0cffee","fixedSmells":["use-no-install-recommends","do-not-use-apt-get-update-alone","pin-package-manager-versions-apt-get","pin-package-manager-versions-pip","pin-package-manager-versions-npm","pin-package-manager-versions-gem","pin-package-manager-versions-apk","use-copy-instead-of-add","use-wget-instead-of-add","do-not-have-secrets","have-a-healthcheck","have-a-user"],"successfullyFixedSmells":["use-no-install-recommends","pin-package-manager-versions-apt-get","use-copy-instead-of-add","have-a-healthcheck","have-a-user"],"processedDockerfile":"#   Copyright 2016 Yahoo Inc.\n#   Licensed under the terms of the Apache 2.0 license.\n#   Please see LICENSE file in the project root for terms.\n#\n#   This file is the dockerfile to setup caffeonspark cpu standalone version.\nFROM ubuntu:14.04\nRUN apt-get update \\\n && apt-get install --no-install-recommends software-properties-common=0.92.37.8 -y\nRUN add-apt-repository ppa:openjdk-r/ppa\nRUN apt-get update \\\n && apt-get install --no-install-recommends build-essential=11.6ubuntu6 vim=2:7.4.052-1ubuntu3.1 cmake=2.8.12.2-0ubuntu3 git=1:1.9.1-1ubuntu0.10 wget=1.15-1ubuntu1.14.04.5 libatlas-base-dev=3.10.1-4 libboost-all-dev=1.54.0.1ubuntu1 libgflags-dev=2.0-1.1ubuntu1 libgoogle-glog-dev=0.3.3-1 libhdf5-serial-dev=1.8.11-5ubuntu7.1 libleveldb-dev=1.15.0-2 liblmdb-dev=0.9.10-1 libopencv-dev=2.4.8+dfsg1-2ubuntu1.2 libprotobuf-dev=2.5.0-9ubuntu1 libsnappy-dev=1.1.0-1ubuntu1 protobuf-compiler=2.5.0-9ubuntu1 python-dev=2.7.5-5ubuntu3 python-numpy=1:1.8.2-0ubuntu0.1 python-pip=1.5.4-1ubuntu4 python-scipy=0.13.3-1build1 maven=3.0.5-1 unzip=6.0-9ubuntu1.5 zip=3.0-8 unzip=6.0-9ubuntu1.5 libopenblas-dev=0.2.8-6ubuntu1 openssh-server=1:6.6p1-2ubuntu2.13 openssh-client=1:6.6p1-2ubuntu2.13 libopenblas-dev=0.2.8-6ubuntu1 libboost-all-dev=1.54.0.1ubuntu1 openjdk-8-jdk -y\nRUN rm -rf /var/lib/apt/lists/*\n#   Passwordless SSH\nRUN ssh-keygen -y -q -N \"\" -t dsa -f /etc/ssh/ssh_host_dsa_key\nRUN ssh-keygen -y -q -N \"\" -t rsa -f /etc/ssh/ssh_host_rsa_key\nRUN ssh-keygen -q -N \"\" -t rsa -f /root/.ssh/id_rsa\nRUN cp /root/.ssh/id_rsa.pub ~/.ssh/authorized_keys\n#   Apache Hadoop and Spark section\nRUN wget http://apache.mirrors.tds.net/hadoop/common/hadoop-2.6.4/hadoop-2.6.4.tar.gz\nRUN wget http://archive.apache.org/dist/spark/spark-1.6.0/spark-1.6.0-bin-hadoop2.6.tgz\nRUN gunzip hadoop-2.6.4.tar.gz\nRUN gunzip spark-1.6.0-bin-hadoop2.6.tgz\nRUN tar -xf hadoop-2.6.4.tar\nRUN tar -xf spark-1.6.0-bin-hadoop2.6.tar\nRUN sudo cp -r hadoop-2.6.4 /usr/local/hadoop\nRUN sudo cp -r spark-1.6.0-bin-hadoop2.6 /usr/local/spark\nRUN rm hadoop-2.6.4.tar spark-1.6.0-bin-hadoop2.6.tar\nRUN rm -rf hadoop-2.6.4/ spark-1.6.0-bin-hadoop2.6/\nRUN sudo mkdir -p /usr/local/hadoop/hadoop_data/hdfs/namenode\nRUN sudo mkdir -p /usr/local/hadoop/hadoop_data/hdfs/datanode\n#   Environment variables\nENV JAVA_HOME=\"/usr/lib/jvm/java-1.8.0-openjdk-amd64\"\nENV HADOOP_HOME=\"/usr/local/hadoop\"\nENV SPARK_HOME=\"/usr/local/spark\"\nENV PATH=\"$PATH:$JAVA_HOME/bin\"\nENV PATH=\"$PATH:$HADOOP_HOME/bin\"\nENV PATH=\"$PATH:$HADOOP_HOME/sbin\"\nENV PATH=\"$PATH:$SPARK_HOME/bin\"\nENV PATH=\"$PATH:$SPARK_HOME/sbin\"\nENV HADOOP_MAPRED_HOME=\"/usr/local/hadoop\"\nENV HADOOP_COMMON_HOME=\"/usr/local/hadoop\"\nENV HADOOP_HDFS_HOME=\"/usr/local/hadoop\"\nENV HADOOP_CONF_DIR=\"/usr/local/hadoop/etc/hadoop\"\nENV YARN_HOME=\"/usr/local/hadoop\"\nENV HADOOP_COMMON_LIB_NATIVE_DIR=\"/usr/local/hadoop/lib/native\"\nENV HADOOP_OPTS=\"\\\"-Djava.library.path=$HADOOP_HOME/lib\\\"\"\n#   Clone CaffeOnSpark\nENV CAFFE_ON_SPARK=\"/opt/CaffeOnSpark\"\nWORKDIR $CAFFE_ON_SPARK\nRUN git clone https://github.com/yahoo/CaffeOnSpark.git . --recursive\n#   Some of the Hadoop part extracted from \"https://hub.docker.com/r/sequenceiq/hadoop-docker/~/dockerfile/\"\nRUN mkdir $HADOOP_HOME/input\nRUN cp $HADOOP_HOME/etc/hadoop/*.xml $HADOOP_HOME/input\nRUN cd /usr/local/hadoop/input\n#   Copy .xml files.\nRUN cp ${CAFFE_ON_SPARK}/scripts/*.xml ${HADOOP_HOME}/etc/hadoop\n#   Format namenode and finish hadoop, spark installations.\nRUN $HADOOP_HOME/bin/hdfs namenode -format\nRUN ls /root/.ssh/\nCOPY config/ssh_config /root/.ssh/config\nRUN chmod 600 /root/.ssh/config\nRUN chown root:root /root/.ssh/config\nCOPY config/bootstrap.sh /etc/bootstrap.sh\nRUN chown root:root /etc/bootstrap.sh\nRUN chmod 700 /etc/bootstrap.sh\nENV BOOTSTRAP=\"/etc/bootstrap.sh\"\nRUN sed -i '/^export JAVA_HOME/ s:.*:export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64\\nexport HADOOP_HOME=/usr/local/hadoop\\n:' $HADOOP_HOME/etc/hadoop/hadoop-env.sh\nRUN sed -i '/^export HADOOP_CONF_DIR/ s:.*:export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop/:' $HADOOP_HOME/etc/hadoop/hadoop-env.sh\n#   workingaround docker.io build error\nRUN ls -la /usr/local/hadoop/etc/hadoop/*-env.sh\nRUN chmod +x /usr/local/hadoop/etc/hadoop/*-env.sh\nRUN ls -la /usr/local/hadoop/etc/hadoop/*-env.sh\n#   fix the 254 error code\nRUN sed -i \"/^[^#]*UsePAM/ s/.*/#&/\" /etc/ssh/sshd_config\nRUN echo \"UsePAM no\" >> /etc/ssh/sshd_config\nRUN echo \"Port 2122\" >> /etc/ssh/sshd_config\nRUN service ssh start \\\n && $HADOOP_HOME/etc/hadoop/hadoop-env.sh \\\n && $HADOOP_HOME/sbin/start-dfs.sh \\\n && $HADOOP_HOME/bin/hdfs dfs -mkdir -p /user/root\nRUN service ssh start \\\n && $HADOOP_HOME/etc/hadoop/hadoop-env.sh \\\n && $HADOOP_HOME/sbin/start-dfs.sh \\\n && $HADOOP_HOME/bin/hdfs dfs -put $HADOOP_HOME/etc/hadoop/ input\nCMD [\"/etc/bootstrap.sh\", \"-bash\"]\n#   Hdfs ports\nEXPOSE 50010/tcp 50020/tcp 50070/tcp 50075/tcp 50090/tcp 8020/tcp 9000/tcp\n#   Mapred ports\nEXPOSE 10020/tcp 19888/tcp\n#  Yarn ports\nEXPOSE 8030/tcp 8031/tcp 8032/tcp 8033/tcp 8040/tcp 8042/tcp 8088/tcp\n#  Other ports\nEXPOSE 49707/tcp 2122/tcp\n#   Continue with CaffeOnSpark build.\n#   ENV CAFFE_ON_SPARK=/opt/CaffeOnSpark\nWORKDIR $CAFFE_ON_SPARK\n#   RUN git clone https://github.com/yahoo/CaffeOnSpark.git . --recursive\nRUN cp caffe-public/Makefile.config.example caffe-public/Makefile.config\nRUN echo \"INCLUDE_DIRS += ${JAVA_HOME}/include\" >> caffe-public/Makefile.config\nRUN sed -i \"s/# CPU_ONLY := 1/CPU_ONLY := 1/g\" caffe-public/Makefile.config\nRUN sed -i \"s|CUDA_DIR := /usr/local/cuda|# CUDA_DIR := /usr/local/cuda|g\" caffe-public/Makefile.config\nRUN sed -i \"s|CUDA_ARCH :=|# CUDA_ARCH :=|g\" caffe-public/Makefile.config\nRUN sed -i \"s|BLAS := atlas|BLAS := open|g\" caffe-public/Makefile.config\nRUN sed -i \"s|TEST_GPUID := 0|# TEST_GPUID := 0|g\" caffe-public/Makefile.config\nRUN make build\nENV LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:$CAFFE_ON_SPARK/caffe-public/distribute/lib:$CAFFE_ON_SPARK/caffe-distri/distribute/lib\"\nWORKDIR /root\nRUN groupadd --system docker-user ; useradd --system --gid docker-user docker-user\nUSER docker-user\n# Please add your HEALTHCHECK here!!!\n","originalDockerfile":"#  Copyright 2016 Yahoo Inc.\n#  Licensed under the terms of the Apache 2.0 license.\n#  Please see LICENSE file in the project root for terms.\n#\n#  This file is the dockerfile to setup caffeonspark cpu standalone version.\nFROM ubuntu:14.04\nRUN apt-get update \\\n && apt-get install software-properties-common -y\nRUN add-apt-repository ppa:openjdk-r/ppa\nRUN apt-get update \\\n && apt-get install --no-install-recommends build-essential vim cmake git wget libatlas-base-dev libboost-all-dev libgflags-dev libgoogle-glog-dev libhdf5-serial-dev libleveldb-dev liblmdb-dev libopencv-dev libprotobuf-dev libsnappy-dev protobuf-compiler python-dev python-numpy python-pip python-scipy maven unzip zip unzip libopenblas-dev openssh-server openssh-client libopenblas-dev libboost-all-dev openjdk-8-jdk -y\nRUN rm -rf /var/lib/apt/lists/*\n#  Passwordless SSH\nRUN ssh-keygen -y -q -N \"\" -t dsa -f /etc/ssh/ssh_host_dsa_key\nRUN ssh-keygen -y -q -N \"\" -t rsa -f /etc/ssh/ssh_host_rsa_key\nRUN ssh-keygen -q -N \"\" -t rsa -f /root/.ssh/id_rsa\nRUN cp /root/.ssh/id_rsa.pub ~/.ssh/authorized_keys\n#  Apache Hadoop and Spark section\nRUN wget http://apache.mirrors.tds.net/hadoop/common/hadoop-2.6.4/hadoop-2.6.4.tar.gz\nRUN wget http://archive.apache.org/dist/spark/spark-1.6.0/spark-1.6.0-bin-hadoop2.6.tgz\nRUN gunzip hadoop-2.6.4.tar.gz\nRUN gunzip spark-1.6.0-bin-hadoop2.6.tgz\nRUN tar -xf hadoop-2.6.4.tar\nRUN tar -xf spark-1.6.0-bin-hadoop2.6.tar\nRUN sudo cp -r hadoop-2.6.4 /usr/local/hadoop\nRUN sudo cp -r spark-1.6.0-bin-hadoop2.6 /usr/local/spark\nRUN rm hadoop-2.6.4.tar spark-1.6.0-bin-hadoop2.6.tar\nRUN rm -rf hadoop-2.6.4/ spark-1.6.0-bin-hadoop2.6/\nRUN sudo mkdir -p /usr/local/hadoop/hadoop_data/hdfs/namenode\nRUN sudo mkdir -p /usr/local/hadoop/hadoop_data/hdfs/datanode\n#  Environment variables\nENV JAVA_HOME=\"/usr/lib/jvm/java-1.8.0-openjdk-amd64\"\nENV HADOOP_HOME=\"/usr/local/hadoop\"\nENV SPARK_HOME=\"/usr/local/spark\"\nENV PATH=\"$PATH:$JAVA_HOME/bin\"\nENV PATH=\"$PATH:$HADOOP_HOME/bin\"\nENV PATH=\"$PATH:$HADOOP_HOME/sbin\"\nENV PATH=\"$PATH:$SPARK_HOME/bin\"\nENV PATH=\"$PATH:$SPARK_HOME/sbin\"\nENV HADOOP_MAPRED_HOME=\"/usr/local/hadoop\"\nENV HADOOP_COMMON_HOME=\"/usr/local/hadoop\"\nENV HADOOP_HDFS_HOME=\"/usr/local/hadoop\"\nENV HADOOP_CONF_DIR=\"/usr/local/hadoop/etc/hadoop\"\nENV YARN_HOME=\"/usr/local/hadoop\"\nENV HADOOP_COMMON_LIB_NATIVE_DIR=\"/usr/local/hadoop/lib/native\"\nENV HADOOP_OPTS=\"\\\"-Djava.library.path=$HADOOP_HOME/lib\\\"\"\n#  Clone CaffeOnSpark\nENV CAFFE_ON_SPARK=\"/opt/CaffeOnSpark\"\nWORKDIR $CAFFE_ON_SPARK\nRUN git clone https://github.com/yahoo/CaffeOnSpark.git . --recursive\n#  Some of the Hadoop part extracted from \"https://hub.docker.com/r/sequenceiq/hadoop-docker/~/dockerfile/\"\nRUN mkdir $HADOOP_HOME/input\nRUN cp $HADOOP_HOME/etc/hadoop/*.xml $HADOOP_HOME/input\nRUN cd /usr/local/hadoop/input\n#  Copy .xml files.\nRUN cp ${CAFFE_ON_SPARK}/scripts/*.xml ${HADOOP_HOME}/etc/hadoop\n#  Format namenode and finish hadoop, spark installations.\nRUN $HADOOP_HOME/bin/hdfs namenode -format\nRUN ls /root/.ssh/\nADD config/ssh_config /root/.ssh/config\nRUN chmod 600 /root/.ssh/config\nRUN chown root:root /root/.ssh/config\nADD config/bootstrap.sh /etc/bootstrap.sh\nRUN chown root:root /etc/bootstrap.sh\nRUN chmod 700 /etc/bootstrap.sh\nENV BOOTSTRAP=\"/etc/bootstrap.sh\"\nRUN sed -i '/^export JAVA_HOME/ s:.*:export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64\\nexport HADOOP_HOME=/usr/local/hadoop\\n:' $HADOOP_HOME/etc/hadoop/hadoop-env.sh\nRUN sed -i '/^export HADOOP_CONF_DIR/ s:.*:export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop/:' $HADOOP_HOME/etc/hadoop/hadoop-env.sh\n#  workingaround docker.io build error\nRUN ls -la /usr/local/hadoop/etc/hadoop/*-env.sh\nRUN chmod +x /usr/local/hadoop/etc/hadoop/*-env.sh\nRUN ls -la /usr/local/hadoop/etc/hadoop/*-env.sh\n#  fix the 254 error code\nRUN sed -i \"/^[^#]*UsePAM/ s/.*/#&/\" /etc/ssh/sshd_config\nRUN echo \"UsePAM no\" >> /etc/ssh/sshd_config\nRUN echo \"Port 2122\" >> /etc/ssh/sshd_config\nRUN service ssh start \\\n && $HADOOP_HOME/etc/hadoop/hadoop-env.sh \\\n && $HADOOP_HOME/sbin/start-dfs.sh \\\n && $HADOOP_HOME/bin/hdfs dfs -mkdir -p /user/root\nRUN service ssh start \\\n && $HADOOP_HOME/etc/hadoop/hadoop-env.sh \\\n && $HADOOP_HOME/sbin/start-dfs.sh \\\n && $HADOOP_HOME/bin/hdfs dfs -put $HADOOP_HOME/etc/hadoop/ input\nCMD [\"/etc/bootstrap.sh\", \"-bash\"]\n#  Hdfs ports\nEXPOSE 50010/tcp 50020/tcp 50070/tcp 50075/tcp 50090/tcp 8020/tcp 9000/tcp\n#  Mapred ports\nEXPOSE 10020/tcp 19888/tcp\n# Yarn ports\nEXPOSE 8030/tcp 8031/tcp 8032/tcp 8033/tcp 8040/tcp 8042/tcp 8088/tcp\n# Other ports\nEXPOSE 49707/tcp 2122/tcp\n#  Continue with CaffeOnSpark build.\n#  ENV CAFFE_ON_SPARK=/opt/CaffeOnSpark\nWORKDIR $CAFFE_ON_SPARK\n#  RUN git clone https://github.com/yahoo/CaffeOnSpark.git . --recursive\nRUN cp caffe-public/Makefile.config.example caffe-public/Makefile.config\nRUN echo \"INCLUDE_DIRS += ${JAVA_HOME}/include\" >> caffe-public/Makefile.config\nRUN sed -i \"s/# CPU_ONLY := 1/CPU_ONLY := 1/g\" caffe-public/Makefile.config\nRUN sed -i \"s|CUDA_DIR := /usr/local/cuda|# CUDA_DIR := /usr/local/cuda|g\" caffe-public/Makefile.config\nRUN sed -i \"s|CUDA_ARCH :=|# CUDA_ARCH :=|g\" caffe-public/Makefile.config\nRUN sed -i \"s|BLAS := atlas|BLAS := open|g\" caffe-public/Makefile.config\nRUN sed -i \"s|TEST_GPUID := 0|# TEST_GPUID := 0|g\" caffe-public/Makefile.config\nRUN make build\nENV LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:$CAFFE_ON_SPARK/caffe-public/distribute/lib:$CAFFE_ON_SPARK/caffe-distri/distribute/lib\"\nWORKDIR /root\n","injectedSmells":[],"originalDockerfileHash":"3689c1faf3cbfee5492193c71411f6ba","successfullyInjectedSmells":[],"originalDockerfileUglified":"#   Copyright 2016 Yahoo Inc.\n#   Licensed under the terms of the Apache 2.0 license.\n#   Please see LICENSE file in the project root for terms.\n#\n#   This file is the dockerfile to setup caffeonspark cpu standalone version.\nFROM ubuntu:14.04\nRUN apt-get update \\\n && apt-get install software-properties-common -y\nRUN add-apt-repository ppa:openjdk-r/ppa\nRUN apt-get update \\\n && apt-get install --no-install-recommends build-essential vim cmake git wget libatlas-base-dev libboost-all-dev libgflags-dev libgoogle-glog-dev libhdf5-serial-dev libleveldb-dev liblmdb-dev libopencv-dev libprotobuf-dev libsnappy-dev protobuf-compiler python-dev python-numpy python-pip python-scipy maven unzip zip unzip libopenblas-dev openssh-server openssh-client libopenblas-dev libboost-all-dev openjdk-8-jdk -y\nRUN rm -rf /var/lib/apt/lists/*\n#   Passwordless SSH\nRUN ssh-keygen -y -q -N \"\" -t dsa -f /etc/ssh/ssh_host_dsa_key\nRUN ssh-keygen -y -q -N \"\" -t rsa -f /etc/ssh/ssh_host_rsa_key\nRUN ssh-keygen -q -N \"\" -t rsa -f /root/.ssh/id_rsa\nRUN cp /root/.ssh/id_rsa.pub ~/.ssh/authorized_keys\n#   Apache Hadoop and Spark section\nRUN wget http://apache.mirrors.tds.net/hadoop/common/hadoop-2.6.4/hadoop-2.6.4.tar.gz\nRUN wget http://archive.apache.org/dist/spark/spark-1.6.0/spark-1.6.0-bin-hadoop2.6.tgz\nRUN gunzip hadoop-2.6.4.tar.gz\nRUN gunzip spark-1.6.0-bin-hadoop2.6.tgz\nRUN tar -xf hadoop-2.6.4.tar\nRUN tar -xf spark-1.6.0-bin-hadoop2.6.tar\nRUN sudo cp -r hadoop-2.6.4 /usr/local/hadoop\nRUN sudo cp -r spark-1.6.0-bin-hadoop2.6 /usr/local/spark\nRUN rm hadoop-2.6.4.tar spark-1.6.0-bin-hadoop2.6.tar\nRUN rm -rf hadoop-2.6.4/ spark-1.6.0-bin-hadoop2.6/\nRUN sudo mkdir -p /usr/local/hadoop/hadoop_data/hdfs/namenode\nRUN sudo mkdir -p /usr/local/hadoop/hadoop_data/hdfs/datanode\n#   Environment variables\nENV JAVA_HOME=\"/usr/lib/jvm/java-1.8.0-openjdk-amd64\"\nENV HADOOP_HOME=\"/usr/local/hadoop\"\nENV SPARK_HOME=\"/usr/local/spark\"\nENV PATH=\"$PATH:$JAVA_HOME/bin\"\nENV PATH=\"$PATH:$HADOOP_HOME/bin\"\nENV PATH=\"$PATH:$HADOOP_HOME/sbin\"\nENV PATH=\"$PATH:$SPARK_HOME/bin\"\nENV PATH=\"$PATH:$SPARK_HOME/sbin\"\nENV HADOOP_MAPRED_HOME=\"/usr/local/hadoop\"\nENV HADOOP_COMMON_HOME=\"/usr/local/hadoop\"\nENV HADOOP_HDFS_HOME=\"/usr/local/hadoop\"\nENV HADOOP_CONF_DIR=\"/usr/local/hadoop/etc/hadoop\"\nENV YARN_HOME=\"/usr/local/hadoop\"\nENV HADOOP_COMMON_LIB_NATIVE_DIR=\"/usr/local/hadoop/lib/native\"\nENV HADOOP_OPTS=\"\\\"-Djava.library.path=$HADOOP_HOME/lib\\\"\"\n#   Clone CaffeOnSpark\nENV CAFFE_ON_SPARK=\"/opt/CaffeOnSpark\"\nWORKDIR $CAFFE_ON_SPARK\nRUN git clone https://github.com/yahoo/CaffeOnSpark.git . --recursive\n#   Some of the Hadoop part extracted from \"https://hub.docker.com/r/sequenceiq/hadoop-docker/~/dockerfile/\"\nRUN mkdir $HADOOP_HOME/input\nRUN cp $HADOOP_HOME/etc/hadoop/*.xml $HADOOP_HOME/input\nRUN cd /usr/local/hadoop/input\n#   Copy .xml files.\nRUN cp ${CAFFE_ON_SPARK}/scripts/*.xml ${HADOOP_HOME}/etc/hadoop\n#   Format namenode and finish hadoop, spark installations.\nRUN $HADOOP_HOME/bin/hdfs namenode -format\nRUN ls /root/.ssh/\nADD config/ssh_config /root/.ssh/config\nRUN chmod 600 /root/.ssh/config\nRUN chown root:root /root/.ssh/config\nADD config/bootstrap.sh /etc/bootstrap.sh\nRUN chown root:root /etc/bootstrap.sh\nRUN chmod 700 /etc/bootstrap.sh\nENV BOOTSTRAP=\"/etc/bootstrap.sh\"\nRUN sed -i '/^export JAVA_HOME/ s:.*:export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64\\nexport HADOOP_HOME=/usr/local/hadoop\\n:' $HADOOP_HOME/etc/hadoop/hadoop-env.sh\nRUN sed -i '/^export HADOOP_CONF_DIR/ s:.*:export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop/:' $HADOOP_HOME/etc/hadoop/hadoop-env.sh\n#   workingaround docker.io build error\nRUN ls -la /usr/local/hadoop/etc/hadoop/*-env.sh\nRUN chmod +x /usr/local/hadoop/etc/hadoop/*-env.sh\nRUN ls -la /usr/local/hadoop/etc/hadoop/*-env.sh\n#   fix the 254 error code\nRUN sed -i \"/^[^#]*UsePAM/ s/.*/#&/\" /etc/ssh/sshd_config\nRUN echo \"UsePAM no\" >> /etc/ssh/sshd_config\nRUN echo \"Port 2122\" >> /etc/ssh/sshd_config\nRUN service ssh start \\\n && $HADOOP_HOME/etc/hadoop/hadoop-env.sh \\\n && $HADOOP_HOME/sbin/start-dfs.sh \\\n && $HADOOP_HOME/bin/hdfs dfs -mkdir -p /user/root\nRUN service ssh start \\\n && $HADOOP_HOME/etc/hadoop/hadoop-env.sh \\\n && $HADOOP_HOME/sbin/start-dfs.sh \\\n && $HADOOP_HOME/bin/hdfs dfs -put $HADOOP_HOME/etc/hadoop/ input\nCMD [\"/etc/bootstrap.sh\", \"-bash\"]\n#   Hdfs ports\nEXPOSE 50010/tcp 50020/tcp 50070/tcp 50075/tcp 50090/tcp 8020/tcp 9000/tcp\n#   Mapred ports\nEXPOSE 10020/tcp 19888/tcp\n#  Yarn ports\nEXPOSE 8030/tcp 8031/tcp 8032/tcp 8033/tcp 8040/tcp 8042/tcp 8088/tcp\n#  Other ports\nEXPOSE 49707/tcp 2122/tcp\n#   Continue with CaffeOnSpark build.\n#   ENV CAFFE_ON_SPARK=/opt/CaffeOnSpark\nWORKDIR $CAFFE_ON_SPARK\n#   RUN git clone https://github.com/yahoo/CaffeOnSpark.git . --recursive\nRUN cp caffe-public/Makefile.config.example caffe-public/Makefile.config\nRUN echo \"INCLUDE_DIRS += ${JAVA_HOME}/include\" >> caffe-public/Makefile.config\nRUN sed -i \"s/# CPU_ONLY := 1/CPU_ONLY := 1/g\" caffe-public/Makefile.config\nRUN sed -i \"s|CUDA_DIR := /usr/local/cuda|# CUDA_DIR := /usr/local/cuda|g\" caffe-public/Makefile.config\nRUN sed -i \"s|CUDA_ARCH :=|# CUDA_ARCH :=|g\" caffe-public/Makefile.config\nRUN sed -i \"s|BLAS := atlas|BLAS := open|g\" caffe-public/Makefile.config\nRUN sed -i \"s|TEST_GPUID := 0|# TEST_GPUID := 0|g\" caffe-public/Makefile.config\nRUN make build\nENV LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:$CAFFE_ON_SPARK/caffe-public/distribute/lib:$CAFFE_ON_SPARK/caffe-distri/distribute/lib\"\nWORKDIR /root\n","originalDockerfileUglifiedHash":"6099ef21fe4a9813c596968656382358","fileName":"/ICSME-replicationpackage/dataset/smelly_dockerfiles_bianncle/8bbb3b6a7d27e5c3721e60bb56435515d0666c2e.dockerfile"}