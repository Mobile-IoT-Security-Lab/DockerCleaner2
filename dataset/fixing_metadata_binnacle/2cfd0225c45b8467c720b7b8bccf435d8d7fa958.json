{"seed":3708935032,"processedDockerfileHash":"084c7f3e41077eddd1221b302a2a6224","fixedSmells":["use-no-install-recommends","do-not-use-apt-get-update-alone","pin-package-manager-versions-apt-get","pin-package-manager-versions-pip","pin-package-manager-versions-npm","pin-package-manager-versions-gem","pin-package-manager-versions-apk","use-copy-instead-of-add","use-wget-instead-of-add","do-not-have-secrets","have-a-healthcheck","have-a-user"],"successfullyFixedSmells":["use-no-install-recommends","pin-package-manager-versions-apt-get","do-not-have-secrets","have-a-user"],"processedDockerfile":"FROM ubuntu:16.04\nRUN apt-get update \\\n && apt-get install --no-install-recommends curl=7.47.0-1ubuntu2.19 wget=1.17.1-1ubuntu1.5 python3.5=3.5.2-2ubuntu0~16.04.13 python3-pip=8.1.1-2ubuntu0.6 git=1:2.7.4-0ubuntu1.10 vim=2:7.4.1689-3ubuntu1.5 openjdk-8-jdk=8u292-b10-0ubuntu1~16.04.1 net-tools=1.60-26ubuntu1 -y\n#   Install Hadoop.\nENV JAVA_HOME=\"/usr/lib/jvm/java-8-openjdk-amd64/\"\nENV HADOOP_VERSION=\"3.2.0 \"\nENV HADOOP_URL=\"https://archive.apache.org/dist/hadoop/common/stable/hadoop-$HADOOP_VERSION.tar.gz\"\nRUN set -x \\\n && curl -fsSL \"$HADOOP_URL\" -o /tmp/hadoop.tar.gz \\\n && tar -xzf /tmp/hadoop.tar.gz -C /opt/ \\\n && rm /tmp/hadoop.tar.gz*\n#   Configure Hadoop\nRUN ln -s /opt/hadoop-$HADOOP_VERSION/etc/hadoop /etc/hadoop\nRUN mkdir /opt/hadoop-$HADOOP_VERSION/logs\nRUN mkdir /hadoop-data\nENV HADOOP_PREFIX=\"/opt/hadoop-$HADOOP_VERSION\"\nENV HADOOP_CONF_DIR=\"/etc/hadoop\"\nENV MULTIHOMED_NETWORK=\"1\"\nENV PATH=\"$HADOOP_PREFIX/bin/:$PATH\"\nENV HIVE_SITE_CONF_javax_jdo_option_ConnectionURL=\"jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true\"\nENV HIVE_SITE_CONF_javax_jdo_option_ConnectionDriverName=\"com.mysql.jdbc.Driver\"\nENV HIVE_SITE_CONF_javax_jdo_option_ConnectionUserName=\"root\"\n# A secret has been removed here. Please do not provide secrets from the Dockerfile as these will leak into the metadata of the resulting docker image. To provide secrets the --secret flag of the docker build command can be used (https://docs.docker.com/develop/develop-images/build_enhancements/#new-docker-build-secret-information).\nENV HIVE_SITE_CONF_datanucleus_autoCreateSchema=\"false\"\n#  ENV HIVE_SITE_CONF_hive_metastore_uris=thrift://hive-metastore:9083\nENV HIVE_SITE_CONF_hive_server2_transport_mode=\"binary\"\n#  ENV HIVE_SITE_CONF_hive_server2_use_SSL=false\nENV HIVE_SITE_CONF_hive_server2_authentication=\"NOSASL\"\nENV HIVE_SITE_CONF_hive_server2_enable_doAs=\"false\"\nENV HIVE_SITE_CONF_hive_metastore_schema_verification=\"false\"\nENV HIVE_SITE_CONF_datanucleus_schema_autoCreateTables=\"true\"\nENV HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check=\"false\"\nENV CORE_CONF_fs_defaultFS=\"hdfs://localhost:8020\"\nENV CORE_CONF_hadoop_http_staticuser_user=\"root\"\nENV CORE_CONF_hadoop_proxyuser_hue_hosts=\"*\"\nENV CORE_CONF_hadoop_proxyuser_hue_groups=\"*\"\nENV CORE_CONF_hadoop.proxyuser.root.hosts=\"*\"\nENV CORE_CONF_hadoop.proxyuser.root.groups=\"*\"\nENV HDFS_CONF_dfs_webhdfs_enabled=\"true\"\nENV HDFS_CONF_dfs_permissions_enabled=\"false\"\nENV YARN_CONF_yarn_log___aggregation___enable=\"true\"\nENV YARN_CONF_yarn_resourcemanager_recovery_enabled=\"true\"\nENV YARN_CONF_yarn_resourcemanager_store_class=\"org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore\"\nENV YARN_CONF_yarn_resourcemanager_fs_state___store_uri=\"/rmstate\"\nENV YARN_CONF_yarn_nodemanager_remote___app___log___dir=\"/app-logs\"\nENV YARN_CONF_yarn_log_server_url=\"http://historyserver:8188/applicationhistory/logs/\"\nENV YARN_CONF_yarn_timeline___service_enabled=\"true\"\nENV YARN_CONF_yarn_timeline___service_generic___application___history_enabled=\"true\"\nENV YARN_CONF_yarn_resourcemanager_system___metrics___publisher_enabled=\"true\"\nENV YARN_CONF_yarn_resourcemanager_hostname=\"resourcemanager\"\nENV YARN_CONF_yarn_timeline___service_hostname=\"historyserver\"\nENV YARN_CONF_yarn_resourcemanager_address=\"resourcemanager:8032\"\nENV YARN_CONF_yarn_resourcemanager_scheduler_address=\"resourcemanager:8030\"\nENV YARN_CONF_yarn_resourcemanager_resource__tracker_address=\"resourcemanager:8031\"\nCOPY entrypoint.sh /entrypoint.sh\nRUN chmod a+x /entrypoint.sh\nENTRYPOINT [\"/entrypoint.sh\"]\nRUN mkdir /cmd\n#   For HDFS NameNode\nHEALTHCHECK CMD curl -f http://localhost:50070/ || exit 1\nENV HDFS_CONF_dfs_namenode_name_dir=\"file:///hadoop/dfs/name\"\nRUN mkdir -p /hadoop/dfs/name\nVOLUME /hadoop/dfs/name\nCOPY cmd/start_namenode.sh /cmd/start_namenode.sh\nRUN chmod a+x /cmd/start_namenode.sh\n#   For HDFS DataNode\nHEALTHCHECK CMD curl -f http://localhost:50075/ || exit 1\nENV HDFS_CONF_dfs_datanode_data_dir=\"file:///hadoop/dfs/data\"\nRUN mkdir -p /hadoop/dfs/data\nVOLUME /hadoop/dfs/data\nCOPY cmd/start_datanode.sh /cmd/start_datanode.sh\nRUN chmod a+x /cmd/start_datanode.sh\n#   For ResourceManager\nHEALTHCHECK CMD curl -f http://localhost:8088/ || exit 1\nCOPY cmd/start_resourcemanager.sh /cmd/start_resourcemanager.sh\nRUN chmod a+x /cmd/start_resourcemanager.sh\n#   For NodeManager\nHEALTHCHECK CMD curl -f http://localhost:8042/ || exit 1\nCOPY cmd/start_nodemanager.sh /cmd/start_nodemanager.sh\nRUN chmod a+x /cmd/start_nodemanager.sh\n#   For Hive\nARG HIVE_VERSION\nENV HIVE_VERSION=\"${HIVE_VERSION:-2.3.2}\"\nENV HIVE_HOME=\"/opt/hive\"\nENV PATH=\"$HIVE_HOME/bin:$PATH\"\nENV HADOOP_HOME=\"/opt/hadoop-$HADOOP_VERSION\"\nRUN apt-get install --no-install-recommends wget=1.17.1-1ubuntu1.5 procps=2:3.3.10-4ubuntu2.5 -y\nWORKDIR /opt\nRUN wget --quiet http://archive.apache.org/dist/hive/hive-$HIVE_VERSION/apache-hive-$HIVE_VERSION-bin.tar.gz\nRUN tar -xzf apache-hive-$HIVE_VERSION-bin.tar.gz \\\n && mv apache-hive-$HIVE_VERSION-bin hive \\\n && rm apache-hive-$HIVE_VERSION-bin.tar.gz \\\n && apt-get clean \\\n && rm -rf /var/lib/apt/lists/*\nCOPY conf/ $HIVE_HOME/conf\nCOPY cmd/start_hiveserver2.sh /cmd/start_hiveserver2.sh\nRUN chmod +x /cmd/start_hiveserver2.sh\n#   Start all hadoop components\nCOPY cmd/start_all.sh /cmd/start_all.sh\nRUN chmod +x /cmd/start_all.sh\n#   install mysql without a password prompt\nRUN [\"/bin/bash\", \"-c\", \"debconf-set-selections\", \"<<<\", \"'mysql-server\", \"mysql-server/root_password\", \"password\", \"root'\"]\nRUN [\"/bin/bash\", \"-c\", \"debconf-set-selections\", \"<<<\", \"'mysql-server\", \"mysql-server/root_password_again\", \"password\", \"root'\"]\nRUN apt-get update \\\n && apt-get install --no-install-recommends mysql-server=5.7.33-0ubuntu0.16.04.1 -y\nVOLUME /var/lib/mysql\nWORKDIR /tmp\nRUN wget --quiet https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.47.tar.gz\nRUN tar -xzf mysql-connector-java-5.1.47.tar.gz\nRUN cp mysql-connector-java-5.1.47/mysql-connector-java-5.1.47.jar /opt/hive/lib/\nRUN rm -rf /tmp/mysql-connector-java-5.1.47.tar.gz\nRUN rm -rf /tmp/mysql-connector-java-5.1.47\nRUN mkdir /dataset\nCOPY dataset/popularize_churn.sql /dataset/popularize_churn.sql\nCOPY dataset/popularize_iris.sql /dataset/popularize_iris.sql\nCOPY dataset/create_model_db.sql /dataset/create_model_db.sql\n#   Install the Go compiler.\nRUN wget --quiet https://dl.google.com/go/go1.11.5.linux-amd64.tar.gz\nRUN tar -C /usr/local -xzf go1.11.5.linux-amd64.tar.gz\nRUN rm go1.11.5.linux-amd64.tar.gz\nRUN apt-get install --no-install-recommends build-essential=12.1ubuntu2 -y\nENV PATH=\"$PATH:/usr/local/go/bin\"\n#   Setup Go source workspace.\nRUN mkdir -p /go/bin\nENV GOPATH=\"/go\"\nENV PATH=\"$PATH:$GOPATH/bin\"\n#   Install python and tensorflow env for run test\nARG CONDA_OS=Linux\nRUN cd / \\\n && curl -sL https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -o mconda-install.sh \\\n && bash -x mconda-install.sh -b -p miniconda \\\n && rm mconda-install.sh\nENV PATH=\"/miniconda/bin:$PATH\"\nRUN ls /miniconda/bin \\\n && /miniconda/bin/conda create -y -q -n sqlflow-dev python=3.6 \\\n && echo \". /miniconda/etc/profile.d/conda.sh\" >> ~/.bashrc \\\n && echo \"source activate sqlflow-dev\" >> ~/.bashrc \\\n && bash -c \"source activate sqlflow-dev \\\n && python -m pip install tensorflow==2.0.0-alpha0 mysql-connector-python impyla jupyter\"\n#   Install protobuf\nRUN wget --quiet https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protoc-3.6.1-linux-x86_64.zip \\\n && apt-get install --no-install-recommends unzip=6.0-20ubuntu1.1 -y \\\n && unzip -qq protoc-3.6.1-linux-x86_64.zip -d /usr/local \\\n && rm protoc-3.6.1-linux-x86_64.zip \\\n && go get github.com/golang/protobuf/protoc-gen-go \\\n && mv /go/bin/protoc-gen-go /usr/local/bin/\nRUN echo \"go get -t sqlflow.org/gohive \\\n && go test -v sqlflow.org/gohive\" > /build_and_test.bash\nRUN chmod +x /build_and_test.bash\nRUN groupadd --system docker-user ; useradd --system --gid docker-user docker-user\nUSER docker-user\n","originalDockerfile":"FROM ubuntu:16.04\nRUN apt-get update \\\n && apt-get install --no-install-recommends curl wget python3.5 python3-pip git vim openjdk-8-jdk net-tools -y\n#  Install Hadoop.\nENV JAVA_HOME=\"/usr/lib/jvm/java-8-openjdk-amd64/\"\nENV HADOOP_VERSION=\"3.2.0 \"\nENV HADOOP_URL=\"https://archive.apache.org/dist/hadoop/common/stable/hadoop-$HADOOP_VERSION.tar.gz\"\nRUN set -x \\\n && curl -fsSL \"$HADOOP_URL\" -o /tmp/hadoop.tar.gz \\\n && tar -xzf /tmp/hadoop.tar.gz -C /opt/ \\\n && rm /tmp/hadoop.tar.gz*\n#  Configure Hadoop\nRUN ln -s /opt/hadoop-$HADOOP_VERSION/etc/hadoop /etc/hadoop\nRUN mkdir /opt/hadoop-$HADOOP_VERSION/logs\nRUN mkdir /hadoop-data\nENV HADOOP_PREFIX=\"/opt/hadoop-$HADOOP_VERSION\"\nENV HADOOP_CONF_DIR=\"/etc/hadoop\"\nENV MULTIHOMED_NETWORK=\"1\"\nENV PATH=\"$HADOOP_PREFIX/bin/:$PATH\"\nENV HIVE_SITE_CONF_javax_jdo_option_ConnectionURL=\"jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true\"\nENV HIVE_SITE_CONF_javax_jdo_option_ConnectionDriverName=\"com.mysql.jdbc.Driver\"\nENV HIVE_SITE_CONF_javax_jdo_option_ConnectionUserName=\"root\"\nENV HIVE_SITE_CONF_javax_jdo_option_ConnectionPassword=\"root\"\nENV HIVE_SITE_CONF_datanucleus_autoCreateSchema=\"false\"\n# ENV HIVE_SITE_CONF_hive_metastore_uris=thrift://hive-metastore:9083\nENV HIVE_SITE_CONF_hive_server2_transport_mode=\"binary\"\n# ENV HIVE_SITE_CONF_hive_server2_use_SSL=false\nENV HIVE_SITE_CONF_hive_server2_authentication=\"NOSASL\"\nENV HIVE_SITE_CONF_hive_server2_enable_doAs=\"false\"\nENV HIVE_SITE_CONF_hive_metastore_schema_verification=\"false\"\nENV HIVE_SITE_CONF_datanucleus_schema_autoCreateTables=\"true\"\nENV HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check=\"false\"\nENV CORE_CONF_fs_defaultFS=\"hdfs://localhost:8020\"\nENV CORE_CONF_hadoop_http_staticuser_user=\"root\"\nENV CORE_CONF_hadoop_proxyuser_hue_hosts=\"*\"\nENV CORE_CONF_hadoop_proxyuser_hue_groups=\"*\"\nENV CORE_CONF_hadoop.proxyuser.root.hosts=\"*\"\nENV CORE_CONF_hadoop.proxyuser.root.groups=\"*\"\nENV HDFS_CONF_dfs_webhdfs_enabled=\"true\"\nENV HDFS_CONF_dfs_permissions_enabled=\"false\"\nENV YARN_CONF_yarn_log___aggregation___enable=\"true\"\nENV YARN_CONF_yarn_resourcemanager_recovery_enabled=\"true\"\nENV YARN_CONF_yarn_resourcemanager_store_class=\"org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore\"\nENV YARN_CONF_yarn_resourcemanager_fs_state___store_uri=\"/rmstate\"\nENV YARN_CONF_yarn_nodemanager_remote___app___log___dir=\"/app-logs\"\nENV YARN_CONF_yarn_log_server_url=\"http://historyserver:8188/applicationhistory/logs/\"\nENV YARN_CONF_yarn_timeline___service_enabled=\"true\"\nENV YARN_CONF_yarn_timeline___service_generic___application___history_enabled=\"true\"\nENV YARN_CONF_yarn_resourcemanager_system___metrics___publisher_enabled=\"true\"\nENV YARN_CONF_yarn_resourcemanager_hostname=\"resourcemanager\"\nENV YARN_CONF_yarn_timeline___service_hostname=\"historyserver\"\nENV YARN_CONF_yarn_resourcemanager_address=\"resourcemanager:8032\"\nENV YARN_CONF_yarn_resourcemanager_scheduler_address=\"resourcemanager:8030\"\nENV YARN_CONF_yarn_resourcemanager_resource__tracker_address=\"resourcemanager:8031\"\nCOPY entrypoint.sh /entrypoint.sh\nRUN chmod a+x /entrypoint.sh\nENTRYPOINT [\"/entrypoint.sh\"]\nRUN mkdir /cmd\n#  For HDFS NameNode\nHEALTHCHECK CMD curl -f http://localhost:50070/ || exit 1\nENV HDFS_CONF_dfs_namenode_name_dir=\"file:///hadoop/dfs/name\"\nRUN mkdir -p /hadoop/dfs/name\nVOLUME /hadoop/dfs/name\nCOPY cmd/start_namenode.sh /cmd/start_namenode.sh\nRUN chmod a+x /cmd/start_namenode.sh\n#  For HDFS DataNode\nHEALTHCHECK CMD curl -f http://localhost:50075/ || exit 1\nENV HDFS_CONF_dfs_datanode_data_dir=\"file:///hadoop/dfs/data\"\nRUN mkdir -p /hadoop/dfs/data\nVOLUME /hadoop/dfs/data\nCOPY cmd/start_datanode.sh /cmd/start_datanode.sh\nRUN chmod a+x /cmd/start_datanode.sh\n#  For ResourceManager\nHEALTHCHECK CMD curl -f http://localhost:8088/ || exit 1\nCOPY cmd/start_resourcemanager.sh /cmd/start_resourcemanager.sh\nRUN chmod a+x /cmd/start_resourcemanager.sh\n#  For NodeManager\nHEALTHCHECK CMD curl -f http://localhost:8042/ || exit 1\nCOPY cmd/start_nodemanager.sh /cmd/start_nodemanager.sh\nRUN chmod a+x /cmd/start_nodemanager.sh\n#  For Hive\nARG HIVE_VERSION\nENV HIVE_VERSION=\"${HIVE_VERSION:-2.3.2}\"\nENV HIVE_HOME=\"/opt/hive\"\nENV PATH=\"$HIVE_HOME/bin:$PATH\"\nENV HADOOP_HOME=\"/opt/hadoop-$HADOOP_VERSION\"\nRUN apt-get install wget procps -y\nWORKDIR /opt\nRUN wget --quiet http://archive.apache.org/dist/hive/hive-$HIVE_VERSION/apache-hive-$HIVE_VERSION-bin.tar.gz\nRUN tar -xzf apache-hive-$HIVE_VERSION-bin.tar.gz \\\n && mv apache-hive-$HIVE_VERSION-bin hive \\\n && rm apache-hive-$HIVE_VERSION-bin.tar.gz \\\n && apt-get clean \\\n && rm -rf /var/lib/apt/lists/*\nCOPY conf/ $HIVE_HOME/conf\nCOPY cmd/start_hiveserver2.sh /cmd/start_hiveserver2.sh\nRUN chmod +x /cmd/start_hiveserver2.sh\n#  Start all hadoop components\nCOPY cmd/start_all.sh /cmd/start_all.sh\nRUN chmod +x /cmd/start_all.sh\n#  install mysql without a password prompt\nRUN [\"/bin/bash\", \"-c\", \"debconf-set-selections\", \"<<<\", \"'mysql-server\", \"mysql-server/root_password\", \"password\", \"root'\"]\nRUN [\"/bin/bash\", \"-c\", \"debconf-set-selections\", \"<<<\", \"'mysql-server\", \"mysql-server/root_password_again\", \"password\", \"root'\"]\nRUN apt-get update \\\n && apt-get install --no-install-recommends mysql-server -y\nVOLUME /var/lib/mysql\nWORKDIR /tmp\nRUN wget --quiet https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.47.tar.gz\nRUN tar -xzf mysql-connector-java-5.1.47.tar.gz\nRUN cp mysql-connector-java-5.1.47/mysql-connector-java-5.1.47.jar /opt/hive/lib/\nRUN rm -rf /tmp/mysql-connector-java-5.1.47.tar.gz\nRUN rm -rf /tmp/mysql-connector-java-5.1.47\nRUN mkdir /dataset\nCOPY dataset/popularize_churn.sql /dataset/popularize_churn.sql\nCOPY dataset/popularize_iris.sql /dataset/popularize_iris.sql\nCOPY dataset/create_model_db.sql /dataset/create_model_db.sql\n#  Install the Go compiler.\nRUN wget --quiet https://dl.google.com/go/go1.11.5.linux-amd64.tar.gz\nRUN tar -C /usr/local -xzf go1.11.5.linux-amd64.tar.gz\nRUN rm go1.11.5.linux-amd64.tar.gz\nRUN apt-get install --no-install-recommends build-essential -y\nENV PATH=\"$PATH:/usr/local/go/bin\"\n#  Setup Go source workspace.\nRUN mkdir -p /go/bin\nENV GOPATH=\"/go\"\nENV PATH=\"$PATH:$GOPATH/bin\"\n#  Install python and tensorflow env for run test\nARG CONDA_OS=Linux\nRUN cd / \\\n && curl -sL https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -o mconda-install.sh \\\n && bash -x mconda-install.sh -b -p miniconda \\\n && rm mconda-install.sh\nENV PATH=\"/miniconda/bin:$PATH\"\nRUN ls /miniconda/bin \\\n && /miniconda/bin/conda create -y -q -n sqlflow-dev python=3.6 \\\n && echo \". /miniconda/etc/profile.d/conda.sh\" >> ~/.bashrc \\\n && echo \"source activate sqlflow-dev\" >> ~/.bashrc \\\n && bash -c \"source activate sqlflow-dev \\\n && python -m pip install tensorflow==2.0.0-alpha0 mysql-connector-python impyla jupyter\"\n#  Install protobuf\nRUN wget --quiet https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protoc-3.6.1-linux-x86_64.zip \\\n && apt-get install unzip -y \\\n && unzip -qq protoc-3.6.1-linux-x86_64.zip -d /usr/local \\\n && rm protoc-3.6.1-linux-x86_64.zip \\\n && go get github.com/golang/protobuf/protoc-gen-go \\\n && mv /go/bin/protoc-gen-go /usr/local/bin/\nRUN echo \"go get -t sqlflow.org/gohive \\\n && go test -v sqlflow.org/gohive\" > /build_and_test.bash\nRUN chmod +x /build_and_test.bash\n","injectedSmells":[],"originalDockerfileHash":"da0f1e2b904557857f213f11dd11db78","successfullyInjectedSmells":[],"originalDockerfileUglified":"FROM ubuntu:16.04\nRUN apt-get update \\\n && apt-get install --no-install-recommends curl wget python3.5 python3-pip git vim openjdk-8-jdk net-tools -y\n#   Install Hadoop.\nENV JAVA_HOME=\"/usr/lib/jvm/java-8-openjdk-amd64/\"\nENV HADOOP_VERSION=\"3.2.0 \"\nENV HADOOP_URL=\"https://archive.apache.org/dist/hadoop/common/stable/hadoop-$HADOOP_VERSION.tar.gz\"\nRUN set -x \\\n && curl -fsSL \"$HADOOP_URL\" -o /tmp/hadoop.tar.gz \\\n && tar -xzf /tmp/hadoop.tar.gz -C /opt/ \\\n && rm /tmp/hadoop.tar.gz*\n#   Configure Hadoop\nRUN ln -s /opt/hadoop-$HADOOP_VERSION/etc/hadoop /etc/hadoop\nRUN mkdir /opt/hadoop-$HADOOP_VERSION/logs\nRUN mkdir /hadoop-data\nENV HADOOP_PREFIX=\"/opt/hadoop-$HADOOP_VERSION\"\nENV HADOOP_CONF_DIR=\"/etc/hadoop\"\nENV MULTIHOMED_NETWORK=\"1\"\nENV PATH=\"$HADOOP_PREFIX/bin/:$PATH\"\nENV HIVE_SITE_CONF_javax_jdo_option_ConnectionURL=\"jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true\"\nENV HIVE_SITE_CONF_javax_jdo_option_ConnectionDriverName=\"com.mysql.jdbc.Driver\"\nENV HIVE_SITE_CONF_javax_jdo_option_ConnectionUserName=\"root\"\nENV HIVE_SITE_CONF_javax_jdo_option_ConnectionPassword=\"root\"\nENV HIVE_SITE_CONF_datanucleus_autoCreateSchema=\"false\"\n#  ENV HIVE_SITE_CONF_hive_metastore_uris=thrift://hive-metastore:9083\nENV HIVE_SITE_CONF_hive_server2_transport_mode=\"binary\"\n#  ENV HIVE_SITE_CONF_hive_server2_use_SSL=false\nENV HIVE_SITE_CONF_hive_server2_authentication=\"NOSASL\"\nENV HIVE_SITE_CONF_hive_server2_enable_doAs=\"false\"\nENV HIVE_SITE_CONF_hive_metastore_schema_verification=\"false\"\nENV HIVE_SITE_CONF_datanucleus_schema_autoCreateTables=\"true\"\nENV HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check=\"false\"\nENV CORE_CONF_fs_defaultFS=\"hdfs://localhost:8020\"\nENV CORE_CONF_hadoop_http_staticuser_user=\"root\"\nENV CORE_CONF_hadoop_proxyuser_hue_hosts=\"*\"\nENV CORE_CONF_hadoop_proxyuser_hue_groups=\"*\"\nENV CORE_CONF_hadoop.proxyuser.root.hosts=\"*\"\nENV CORE_CONF_hadoop.proxyuser.root.groups=\"*\"\nENV HDFS_CONF_dfs_webhdfs_enabled=\"true\"\nENV HDFS_CONF_dfs_permissions_enabled=\"false\"\nENV YARN_CONF_yarn_log___aggregation___enable=\"true\"\nENV YARN_CONF_yarn_resourcemanager_recovery_enabled=\"true\"\nENV YARN_CONF_yarn_resourcemanager_store_class=\"org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore\"\nENV YARN_CONF_yarn_resourcemanager_fs_state___store_uri=\"/rmstate\"\nENV YARN_CONF_yarn_nodemanager_remote___app___log___dir=\"/app-logs\"\nENV YARN_CONF_yarn_log_server_url=\"http://historyserver:8188/applicationhistory/logs/\"\nENV YARN_CONF_yarn_timeline___service_enabled=\"true\"\nENV YARN_CONF_yarn_timeline___service_generic___application___history_enabled=\"true\"\nENV YARN_CONF_yarn_resourcemanager_system___metrics___publisher_enabled=\"true\"\nENV YARN_CONF_yarn_resourcemanager_hostname=\"resourcemanager\"\nENV YARN_CONF_yarn_timeline___service_hostname=\"historyserver\"\nENV YARN_CONF_yarn_resourcemanager_address=\"resourcemanager:8032\"\nENV YARN_CONF_yarn_resourcemanager_scheduler_address=\"resourcemanager:8030\"\nENV YARN_CONF_yarn_resourcemanager_resource__tracker_address=\"resourcemanager:8031\"\nCOPY entrypoint.sh /entrypoint.sh\nRUN chmod a+x /entrypoint.sh\nENTRYPOINT [\"/entrypoint.sh\"]\nRUN mkdir /cmd\n#   For HDFS NameNode\nHEALTHCHECK CMD curl -f http://localhost:50070/ || exit 1\nENV HDFS_CONF_dfs_namenode_name_dir=\"file:///hadoop/dfs/name\"\nRUN mkdir -p /hadoop/dfs/name\nVOLUME /hadoop/dfs/name\nCOPY cmd/start_namenode.sh /cmd/start_namenode.sh\nRUN chmod a+x /cmd/start_namenode.sh\n#   For HDFS DataNode\nHEALTHCHECK CMD curl -f http://localhost:50075/ || exit 1\nENV HDFS_CONF_dfs_datanode_data_dir=\"file:///hadoop/dfs/data\"\nRUN mkdir -p /hadoop/dfs/data\nVOLUME /hadoop/dfs/data\nCOPY cmd/start_datanode.sh /cmd/start_datanode.sh\nRUN chmod a+x /cmd/start_datanode.sh\n#   For ResourceManager\nHEALTHCHECK CMD curl -f http://localhost:8088/ || exit 1\nCOPY cmd/start_resourcemanager.sh /cmd/start_resourcemanager.sh\nRUN chmod a+x /cmd/start_resourcemanager.sh\n#   For NodeManager\nHEALTHCHECK CMD curl -f http://localhost:8042/ || exit 1\nCOPY cmd/start_nodemanager.sh /cmd/start_nodemanager.sh\nRUN chmod a+x /cmd/start_nodemanager.sh\n#   For Hive\nARG HIVE_VERSION\nENV HIVE_VERSION=\"${HIVE_VERSION:-2.3.2}\"\nENV HIVE_HOME=\"/opt/hive\"\nENV PATH=\"$HIVE_HOME/bin:$PATH\"\nENV HADOOP_HOME=\"/opt/hadoop-$HADOOP_VERSION\"\nRUN apt-get install wget procps -y\nWORKDIR /opt\nRUN wget --quiet http://archive.apache.org/dist/hive/hive-$HIVE_VERSION/apache-hive-$HIVE_VERSION-bin.tar.gz\nRUN tar -xzf apache-hive-$HIVE_VERSION-bin.tar.gz \\\n && mv apache-hive-$HIVE_VERSION-bin hive \\\n && rm apache-hive-$HIVE_VERSION-bin.tar.gz \\\n && apt-get clean \\\n && rm -rf /var/lib/apt/lists/*\nCOPY conf/ $HIVE_HOME/conf\nCOPY cmd/start_hiveserver2.sh /cmd/start_hiveserver2.sh\nRUN chmod +x /cmd/start_hiveserver2.sh\n#   Start all hadoop components\nCOPY cmd/start_all.sh /cmd/start_all.sh\nRUN chmod +x /cmd/start_all.sh\n#   install mysql without a password prompt\nRUN [\"/bin/bash\", \"-c\", \"debconf-set-selections\", \"<<<\", \"'mysql-server\", \"mysql-server/root_password\", \"password\", \"root'\"]\nRUN [\"/bin/bash\", \"-c\", \"debconf-set-selections\", \"<<<\", \"'mysql-server\", \"mysql-server/root_password_again\", \"password\", \"root'\"]\nRUN apt-get update \\\n && apt-get install --no-install-recommends mysql-server -y\nVOLUME /var/lib/mysql\nWORKDIR /tmp\nRUN wget --quiet https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.47.tar.gz\nRUN tar -xzf mysql-connector-java-5.1.47.tar.gz\nRUN cp mysql-connector-java-5.1.47/mysql-connector-java-5.1.47.jar /opt/hive/lib/\nRUN rm -rf /tmp/mysql-connector-java-5.1.47.tar.gz\nRUN rm -rf /tmp/mysql-connector-java-5.1.47\nRUN mkdir /dataset\nCOPY dataset/popularize_churn.sql /dataset/popularize_churn.sql\nCOPY dataset/popularize_iris.sql /dataset/popularize_iris.sql\nCOPY dataset/create_model_db.sql /dataset/create_model_db.sql\n#   Install the Go compiler.\nRUN wget --quiet https://dl.google.com/go/go1.11.5.linux-amd64.tar.gz\nRUN tar -C /usr/local -xzf go1.11.5.linux-amd64.tar.gz\nRUN rm go1.11.5.linux-amd64.tar.gz\nRUN apt-get install --no-install-recommends build-essential -y\nENV PATH=\"$PATH:/usr/local/go/bin\"\n#   Setup Go source workspace.\nRUN mkdir -p /go/bin\nENV GOPATH=\"/go\"\nENV PATH=\"$PATH:$GOPATH/bin\"\n#   Install python and tensorflow env for run test\nARG CONDA_OS=Linux\nRUN cd / \\\n && curl -sL https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -o mconda-install.sh \\\n && bash -x mconda-install.sh -b -p miniconda \\\n && rm mconda-install.sh\nENV PATH=\"/miniconda/bin:$PATH\"\nRUN ls /miniconda/bin \\\n && /miniconda/bin/conda create -y -q -n sqlflow-dev python=3.6 \\\n && echo \". /miniconda/etc/profile.d/conda.sh\" >> ~/.bashrc \\\n && echo \"source activate sqlflow-dev\" >> ~/.bashrc \\\n && bash -c \"source activate sqlflow-dev \\\n && python -m pip install tensorflow==2.0.0-alpha0 mysql-connector-python impyla jupyter\"\n#   Install protobuf\nRUN wget --quiet https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protoc-3.6.1-linux-x86_64.zip \\\n && apt-get install unzip -y \\\n && unzip -qq protoc-3.6.1-linux-x86_64.zip -d /usr/local \\\n && rm protoc-3.6.1-linux-x86_64.zip \\\n && go get github.com/golang/protobuf/protoc-gen-go \\\n && mv /go/bin/protoc-gen-go /usr/local/bin/\nRUN echo \"go get -t sqlflow.org/gohive \\\n && go test -v sqlflow.org/gohive\" > /build_and_test.bash\nRUN chmod +x /build_and_test.bash\n","originalDockerfileUglifiedHash":"9f90766060466ec7497b0737017658b0","fileName":"/ICSME-replicationpackage/dataset/smelly_dockerfiles_bianncle/2cfd0225c45b8467c720b7b8bccf435d8d7fa958.dockerfile"}