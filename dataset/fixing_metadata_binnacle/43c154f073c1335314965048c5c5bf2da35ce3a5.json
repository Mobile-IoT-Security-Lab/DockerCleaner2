{"seed":2449251539,"processedDockerfileHash":"869756396b81b0c31efbe4f35a3af6f5","fixedSmells":["use-no-install-recommends","do-not-use-apt-get-update-alone","pin-package-manager-versions-apt-get","pin-package-manager-versions-pip","pin-package-manager-versions-npm","pin-package-manager-versions-gem","pin-package-manager-versions-apk","use-copy-instead-of-add","use-wget-instead-of-add","do-not-have-secrets","have-a-healthcheck","have-a-user"],"successfullyFixedSmells":["use-no-install-recommends","use-copy-instead-of-add","have-a-healthcheck","have-a-user"],"processedDockerfile":"#   Copyright 2016 Yahoo Inc.\n#   Licensed under the terms of the Apache 2.0 license.\n#   Please see LICENSE file in the project root for terms.\n#\n#   This file is the dockerfile to setup caffeonspark cpu standalone version.\nFROM nvidia/cuda:7.5-cudnn5-devel-ubuntu14.04\nRUN apt-get update \\\n && apt-get install --no-install-recommends software-properties-common -y\nRUN add-apt-repository ppa:openjdk-r/ppa\nRUN apt-get update \\\n && apt-get install --no-install-recommends build-essential vim cmake git wget libatlas-base-dev libboost-all-dev libgflags-dev libgoogle-glog-dev libhdf5-serial-dev libleveldb-dev liblmdb-dev libopencv-dev libprotobuf-dev libsnappy-dev protobuf-compiler python-dev python-numpy python-pip python-scipy maven unzip zip unzip libopenblas-dev openssh-server openssh-client libopenblas-dev libboost-all-dev openjdk-8-jdk -y\nRUN rm -rf /var/lib/apt/lists/*\n#   Passwordless SSH\nRUN ssh-keygen -y -q -N \"\" -t dsa -f /etc/ssh/ssh_host_dsa_key\nRUN ssh-keygen -y -q -N \"\" -t rsa -f /etc/ssh/ssh_host_rsa_key\nRUN ssh-keygen -q -N \"\" -t rsa -f /root/.ssh/id_rsa\nRUN cp /root/.ssh/id_rsa.pub ~/.ssh/authorized_keys\n#   Apache Hadoop and Spark section\nRUN wget http://apache.mirrors.tds.net/hadoop/common/hadoop-2.6.4/hadoop-2.6.4.tar.gz\nRUN wget http://archive.apache.org/dist/spark/spark-1.6.0/spark-1.6.0-bin-hadoop2.6.tgz\nRUN gunzip hadoop-2.6.4.tar.gz\nRUN gunzip spark-1.6.0-bin-hadoop2.6.tgz\nRUN tar -xf hadoop-2.6.4.tar\nRUN tar -xf spark-1.6.0-bin-hadoop2.6.tar\nRUN sudo cp -r hadoop-2.6.4 /usr/local/hadoop\nRUN sudo cp -r spark-1.6.0-bin-hadoop2.6 /usr/local/spark\nRUN rm hadoop-2.6.4.tar spark-1.6.0-bin-hadoop2.6.tar\nRUN rm -rf hadoop-2.6.4/ spark-1.6.0-bin-hadoop2.6/\nRUN sudo mkdir -p /usr/local/hadoop/hadoop_data/hdfs/namenode\nRUN sudo mkdir -p /usr/local/hadoop/hadoop_data/hdfs/datanode\n#   Environment variables\nENV JAVA_HOME=\"/usr/lib/jvm/java-1.8.0-openjdk-amd64\"\nENV HADOOP_HOME=\"/usr/local/hadoop\"\nENV SPARK_HOME=\"/usr/local/spark\"\nENV PATH=\"$PATH:$JAVA_HOME/bin\"\nENV PATH=\"$PATH:$HADOOP_HOME/bin\"\nENV PATH=\"$PATH:$HADOOP_HOME/sbin\"\nENV PATH=\"$PATH:$SPARK_HOME/bin\"\nENV PATH=\"$PATH:$SPARK_HOME/sbin\"\nENV HADOOP_MAPRED_HOME=\"/usr/local/hadoop\"\nENV HADOOP_COMMON_HOME=\"/usr/local/hadoop\"\nENV HADOOP_HDFS_HOME=\"/usr/local/hadoop\"\nENV HADOOP_CONF_DIR=\"/usr/local/hadoop/etc/hadoop\"\nENV YARN_CONF_DIR=\"/usr/local/hadoop/etc/hadoop\"\nENV YARN_HOME=\"/usr/local/hadoop\"\nENV HADOOP_COMMON_LIB_NATIVE_DIR=\"/usr/local/hadoop/lib/native\"\nENV HADOOP_OPTS=\"\\\"-Djava.library.path=$HADOOP_HOME/lib\\\"\"\n#   Clone CaffeOnSpark\nENV CAFFE_ON_SPARK=\"/opt/CaffeOnSpark\"\nWORKDIR $CAFFE_ON_SPARK\nRUN git clone https://github.com/yahoo/CaffeOnSpark.git . --recursive\n#   Some of the Hadoop part extracted from \"https://hub.docker.com/r/sequenceiq/hadoop-docker/~/dockerfile/\"\nRUN mkdir $HADOOP_HOME/input\nRUN cp $HADOOP_HOME/etc/hadoop/*.xml $HADOOP_HOME/input\nRUN cd /usr/local/hadoop/input\n#   Copy .xml files.\nRUN cp ${CAFFE_ON_SPARK}/scripts/*.xml ${HADOOP_HOME}/etc/hadoop\n#   Format namenode and finish hadoop, spark installations.\nRUN $HADOOP_HOME/bin/hdfs namenode -format\nRUN ls /root/.ssh/\nCOPY config/ssh_config /root/.ssh/config\nRUN chmod 600 /root/.ssh/config\nRUN chown root:root /root/.ssh/config\nCOPY config/bootstrap.sh /etc/bootstrap.sh\nRUN chown root:root /etc/bootstrap.sh\nRUN chmod 700 /etc/bootstrap.sh\nENV BOOTSTRAP=\"/etc/bootstrap.sh\"\nRUN sed -i '/^export JAVA_HOME/ s:.*:export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64\\nexport HADOOP_HOME=/usr/local/hadoop\\n:' $HADOOP_HOME/etc/hadoop/hadoop-env.sh\nRUN sed -i '/^export HADOOP_CONF_DIR/ s:.*:export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop/:' $HADOOP_HOME/etc/hadoop/hadoop-env.sh\n#   workingaround docker.io build error\nRUN ls -la /usr/local/hadoop/etc/hadoop/*-env.sh\nRUN chmod +x /usr/local/hadoop/etc/hadoop/*-env.sh\nRUN ls -la /usr/local/hadoop/etc/hadoop/*-env.sh\n#   fix the 254 error code\nRUN sed -i \"/^[^#]*UsePAM/ s/.*/#&/\" /etc/ssh/sshd_config\nRUN echo \"UsePAM no\" >> /etc/ssh/sshd_config\nRUN echo \"Port 2122\" >> /etc/ssh/sshd_config\nRUN service ssh start \\\n && $HADOOP_HOME/etc/hadoop/hadoop-env.sh \\\n && $HADOOP_HOME/sbin/start-dfs.sh \\\n && $HADOOP_HOME/bin/hdfs dfs -mkdir -p /user/root\nRUN service ssh start \\\n && $HADOOP_HOME/etc/hadoop/hadoop-env.sh \\\n && $HADOOP_HOME/sbin/start-dfs.sh \\\n && $HADOOP_HOME/bin/hdfs dfs -put $HADOOP_HOME/etc/hadoop/ input\nCMD [\"/etc/bootstrap.sh\", \"-bash\"]\n#   Hdfs ports\nEXPOSE 50010/tcp 50020/tcp 50070/tcp 50075/tcp 50090/tcp 8020/tcp 9000/tcp\n#   Mapred ports\nEXPOSE 10020/tcp 19888/tcp\n#  Yarn ports\nEXPOSE 8030/tcp 8031/tcp 8032/tcp 8033/tcp 8040/tcp 8042/tcp 8088/tcp\n#  Other ports\nEXPOSE 49707/tcp 2122/tcp\n#   Continue with CaffeOnSpark build.\n#   ENV CAFFE_ON_SPARK=/opt/CaffeOnSpark\nWORKDIR $CAFFE_ON_SPARK\n#   RUN git clone https://github.com/yahoo/CaffeOnSpark.git . --recursive\nRUN cp caffe-public/Makefile.config.example caffe-public/Makefile.config\nRUN echo \"INCLUDE_DIRS += ${JAVA_HOME}/include\" >> caffe-public/Makefile.config\n#  RUN sed -i \"s/# USE_CUDNN := 1/USE_CUDNN := 1/g\" caffe-public/Makefile.config\nRUN sed -i \"s|BLAS := atlas|BLAS := open|g\" caffe-public/Makefile.config\nRUN make build\nENV LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:$CAFFE_ON_SPARK/caffe-public/distribute/lib:$CAFFE_ON_SPARK/caffe-distri/distribute/lib\"\nWORKDIR /root\nRUN groupadd --system docker-user ; useradd --system --gid docker-user docker-user\nUSER docker-user\n# Please add your HEALTHCHECK here!!!\n","originalDockerfile":"#  Copyright 2016 Yahoo Inc.\n#  Licensed under the terms of the Apache 2.0 license.\n#  Please see LICENSE file in the project root for terms.\n#\n#  This file is the dockerfile to setup caffeonspark cpu standalone version.\nFROM nvidia/cuda:7.5-cudnn5-devel-ubuntu14.04\nRUN apt-get update \\\n && apt-get install software-properties-common -y\nRUN add-apt-repository ppa:openjdk-r/ppa\nRUN apt-get update \\\n && apt-get install --no-install-recommends build-essential vim cmake git wget libatlas-base-dev libboost-all-dev libgflags-dev libgoogle-glog-dev libhdf5-serial-dev libleveldb-dev liblmdb-dev libopencv-dev libprotobuf-dev libsnappy-dev protobuf-compiler python-dev python-numpy python-pip python-scipy maven unzip zip unzip libopenblas-dev openssh-server openssh-client libopenblas-dev libboost-all-dev openjdk-8-jdk -y\nRUN rm -rf /var/lib/apt/lists/*\n#  Passwordless SSH\nRUN ssh-keygen -y -q -N \"\" -t dsa -f /etc/ssh/ssh_host_dsa_key\nRUN ssh-keygen -y -q -N \"\" -t rsa -f /etc/ssh/ssh_host_rsa_key\nRUN ssh-keygen -q -N \"\" -t rsa -f /root/.ssh/id_rsa\nRUN cp /root/.ssh/id_rsa.pub ~/.ssh/authorized_keys\n#  Apache Hadoop and Spark section\nRUN wget http://apache.mirrors.tds.net/hadoop/common/hadoop-2.6.4/hadoop-2.6.4.tar.gz\nRUN wget http://archive.apache.org/dist/spark/spark-1.6.0/spark-1.6.0-bin-hadoop2.6.tgz\nRUN gunzip hadoop-2.6.4.tar.gz\nRUN gunzip spark-1.6.0-bin-hadoop2.6.tgz\nRUN tar -xf hadoop-2.6.4.tar\nRUN tar -xf spark-1.6.0-bin-hadoop2.6.tar\nRUN sudo cp -r hadoop-2.6.4 /usr/local/hadoop\nRUN sudo cp -r spark-1.6.0-bin-hadoop2.6 /usr/local/spark\nRUN rm hadoop-2.6.4.tar spark-1.6.0-bin-hadoop2.6.tar\nRUN rm -rf hadoop-2.6.4/ spark-1.6.0-bin-hadoop2.6/\nRUN sudo mkdir -p /usr/local/hadoop/hadoop_data/hdfs/namenode\nRUN sudo mkdir -p /usr/local/hadoop/hadoop_data/hdfs/datanode\n#  Environment variables\nENV JAVA_HOME=\"/usr/lib/jvm/java-1.8.0-openjdk-amd64\"\nENV HADOOP_HOME=\"/usr/local/hadoop\"\nENV SPARK_HOME=\"/usr/local/spark\"\nENV PATH=\"$PATH:$JAVA_HOME/bin\"\nENV PATH=\"$PATH:$HADOOP_HOME/bin\"\nENV PATH=\"$PATH:$HADOOP_HOME/sbin\"\nENV PATH=\"$PATH:$SPARK_HOME/bin\"\nENV PATH=\"$PATH:$SPARK_HOME/sbin\"\nENV HADOOP_MAPRED_HOME=\"/usr/local/hadoop\"\nENV HADOOP_COMMON_HOME=\"/usr/local/hadoop\"\nENV HADOOP_HDFS_HOME=\"/usr/local/hadoop\"\nENV HADOOP_CONF_DIR=\"/usr/local/hadoop/etc/hadoop\"\nENV YARN_CONF_DIR=\"/usr/local/hadoop/etc/hadoop\"\nENV YARN_HOME=\"/usr/local/hadoop\"\nENV HADOOP_COMMON_LIB_NATIVE_DIR=\"/usr/local/hadoop/lib/native\"\nENV HADOOP_OPTS=\"\\\"-Djava.library.path=$HADOOP_HOME/lib\\\"\"\n#  Clone CaffeOnSpark\nENV CAFFE_ON_SPARK=\"/opt/CaffeOnSpark\"\nWORKDIR $CAFFE_ON_SPARK\nRUN git clone https://github.com/yahoo/CaffeOnSpark.git . --recursive\n#  Some of the Hadoop part extracted from \"https://hub.docker.com/r/sequenceiq/hadoop-docker/~/dockerfile/\"\nRUN mkdir $HADOOP_HOME/input\nRUN cp $HADOOP_HOME/etc/hadoop/*.xml $HADOOP_HOME/input\nRUN cd /usr/local/hadoop/input\n#  Copy .xml files.\nRUN cp ${CAFFE_ON_SPARK}/scripts/*.xml ${HADOOP_HOME}/etc/hadoop\n#  Format namenode and finish hadoop, spark installations.\nRUN $HADOOP_HOME/bin/hdfs namenode -format\nRUN ls /root/.ssh/\nADD config/ssh_config /root/.ssh/config\nRUN chmod 600 /root/.ssh/config\nRUN chown root:root /root/.ssh/config\nADD config/bootstrap.sh /etc/bootstrap.sh\nRUN chown root:root /etc/bootstrap.sh\nRUN chmod 700 /etc/bootstrap.sh\nENV BOOTSTRAP=\"/etc/bootstrap.sh\"\nRUN sed -i '/^export JAVA_HOME/ s:.*:export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64\\nexport HADOOP_HOME=/usr/local/hadoop\\n:' $HADOOP_HOME/etc/hadoop/hadoop-env.sh\nRUN sed -i '/^export HADOOP_CONF_DIR/ s:.*:export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop/:' $HADOOP_HOME/etc/hadoop/hadoop-env.sh\n#  workingaround docker.io build error\nRUN ls -la /usr/local/hadoop/etc/hadoop/*-env.sh\nRUN chmod +x /usr/local/hadoop/etc/hadoop/*-env.sh\nRUN ls -la /usr/local/hadoop/etc/hadoop/*-env.sh\n#  fix the 254 error code\nRUN sed -i \"/^[^#]*UsePAM/ s/.*/#&/\" /etc/ssh/sshd_config\nRUN echo \"UsePAM no\" >> /etc/ssh/sshd_config\nRUN echo \"Port 2122\" >> /etc/ssh/sshd_config\nRUN service ssh start \\\n && $HADOOP_HOME/etc/hadoop/hadoop-env.sh \\\n && $HADOOP_HOME/sbin/start-dfs.sh \\\n && $HADOOP_HOME/bin/hdfs dfs -mkdir -p /user/root\nRUN service ssh start \\\n && $HADOOP_HOME/etc/hadoop/hadoop-env.sh \\\n && $HADOOP_HOME/sbin/start-dfs.sh \\\n && $HADOOP_HOME/bin/hdfs dfs -put $HADOOP_HOME/etc/hadoop/ input\nCMD [\"/etc/bootstrap.sh\", \"-bash\"]\n#  Hdfs ports\nEXPOSE 50010/tcp 50020/tcp 50070/tcp 50075/tcp 50090/tcp 8020/tcp 9000/tcp\n#  Mapred ports\nEXPOSE 10020/tcp 19888/tcp\n# Yarn ports\nEXPOSE 8030/tcp 8031/tcp 8032/tcp 8033/tcp 8040/tcp 8042/tcp 8088/tcp\n# Other ports\nEXPOSE 49707/tcp 2122/tcp\n#  Continue with CaffeOnSpark build.\n#  ENV CAFFE_ON_SPARK=/opt/CaffeOnSpark\nWORKDIR $CAFFE_ON_SPARK\n#  RUN git clone https://github.com/yahoo/CaffeOnSpark.git . --recursive\nRUN cp caffe-public/Makefile.config.example caffe-public/Makefile.config\nRUN echo \"INCLUDE_DIRS += ${JAVA_HOME}/include\" >> caffe-public/Makefile.config\n# RUN sed -i \"s/# USE_CUDNN := 1/USE_CUDNN := 1/g\" caffe-public/Makefile.config\nRUN sed -i \"s|BLAS := atlas|BLAS := open|g\" caffe-public/Makefile.config\nRUN make build\nENV LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:$CAFFE_ON_SPARK/caffe-public/distribute/lib:$CAFFE_ON_SPARK/caffe-distri/distribute/lib\"\nWORKDIR /root\n","injectedSmells":[],"originalDockerfileHash":"5f2c0176a02fd6f7105e7bc291fc9cd3","successfullyInjectedSmells":[],"originalDockerfileUglified":"#   Copyright 2016 Yahoo Inc.\n#   Licensed under the terms of the Apache 2.0 license.\n#   Please see LICENSE file in the project root for terms.\n#\n#   This file is the dockerfile to setup caffeonspark cpu standalone version.\nFROM nvidia/cuda:7.5-cudnn5-devel-ubuntu14.04\nRUN apt-get update \\\n && apt-get install software-properties-common -y\nRUN add-apt-repository ppa:openjdk-r/ppa\nRUN apt-get update \\\n && apt-get install --no-install-recommends build-essential vim cmake git wget libatlas-base-dev libboost-all-dev libgflags-dev libgoogle-glog-dev libhdf5-serial-dev libleveldb-dev liblmdb-dev libopencv-dev libprotobuf-dev libsnappy-dev protobuf-compiler python-dev python-numpy python-pip python-scipy maven unzip zip unzip libopenblas-dev openssh-server openssh-client libopenblas-dev libboost-all-dev openjdk-8-jdk -y\nRUN rm -rf /var/lib/apt/lists/*\n#   Passwordless SSH\nRUN ssh-keygen -y -q -N \"\" -t dsa -f /etc/ssh/ssh_host_dsa_key\nRUN ssh-keygen -y -q -N \"\" -t rsa -f /etc/ssh/ssh_host_rsa_key\nRUN ssh-keygen -q -N \"\" -t rsa -f /root/.ssh/id_rsa\nRUN cp /root/.ssh/id_rsa.pub ~/.ssh/authorized_keys\n#   Apache Hadoop and Spark section\nRUN wget http://apache.mirrors.tds.net/hadoop/common/hadoop-2.6.4/hadoop-2.6.4.tar.gz\nRUN wget http://archive.apache.org/dist/spark/spark-1.6.0/spark-1.6.0-bin-hadoop2.6.tgz\nRUN gunzip hadoop-2.6.4.tar.gz\nRUN gunzip spark-1.6.0-bin-hadoop2.6.tgz\nRUN tar -xf hadoop-2.6.4.tar\nRUN tar -xf spark-1.6.0-bin-hadoop2.6.tar\nRUN sudo cp -r hadoop-2.6.4 /usr/local/hadoop\nRUN sudo cp -r spark-1.6.0-bin-hadoop2.6 /usr/local/spark\nRUN rm hadoop-2.6.4.tar spark-1.6.0-bin-hadoop2.6.tar\nRUN rm -rf hadoop-2.6.4/ spark-1.6.0-bin-hadoop2.6/\nRUN sudo mkdir -p /usr/local/hadoop/hadoop_data/hdfs/namenode\nRUN sudo mkdir -p /usr/local/hadoop/hadoop_data/hdfs/datanode\n#   Environment variables\nENV JAVA_HOME=\"/usr/lib/jvm/java-1.8.0-openjdk-amd64\"\nENV HADOOP_HOME=\"/usr/local/hadoop\"\nENV SPARK_HOME=\"/usr/local/spark\"\nENV PATH=\"$PATH:$JAVA_HOME/bin\"\nENV PATH=\"$PATH:$HADOOP_HOME/bin\"\nENV PATH=\"$PATH:$HADOOP_HOME/sbin\"\nENV PATH=\"$PATH:$SPARK_HOME/bin\"\nENV PATH=\"$PATH:$SPARK_HOME/sbin\"\nENV HADOOP_MAPRED_HOME=\"/usr/local/hadoop\"\nENV HADOOP_COMMON_HOME=\"/usr/local/hadoop\"\nENV HADOOP_HDFS_HOME=\"/usr/local/hadoop\"\nENV HADOOP_CONF_DIR=\"/usr/local/hadoop/etc/hadoop\"\nENV YARN_CONF_DIR=\"/usr/local/hadoop/etc/hadoop\"\nENV YARN_HOME=\"/usr/local/hadoop\"\nENV HADOOP_COMMON_LIB_NATIVE_DIR=\"/usr/local/hadoop/lib/native\"\nENV HADOOP_OPTS=\"\\\"-Djava.library.path=$HADOOP_HOME/lib\\\"\"\n#   Clone CaffeOnSpark\nENV CAFFE_ON_SPARK=\"/opt/CaffeOnSpark\"\nWORKDIR $CAFFE_ON_SPARK\nRUN git clone https://github.com/yahoo/CaffeOnSpark.git . --recursive\n#   Some of the Hadoop part extracted from \"https://hub.docker.com/r/sequenceiq/hadoop-docker/~/dockerfile/\"\nRUN mkdir $HADOOP_HOME/input\nRUN cp $HADOOP_HOME/etc/hadoop/*.xml $HADOOP_HOME/input\nRUN cd /usr/local/hadoop/input\n#   Copy .xml files.\nRUN cp ${CAFFE_ON_SPARK}/scripts/*.xml ${HADOOP_HOME}/etc/hadoop\n#   Format namenode and finish hadoop, spark installations.\nRUN $HADOOP_HOME/bin/hdfs namenode -format\nRUN ls /root/.ssh/\nADD config/ssh_config /root/.ssh/config\nRUN chmod 600 /root/.ssh/config\nRUN chown root:root /root/.ssh/config\nADD config/bootstrap.sh /etc/bootstrap.sh\nRUN chown root:root /etc/bootstrap.sh\nRUN chmod 700 /etc/bootstrap.sh\nENV BOOTSTRAP=\"/etc/bootstrap.sh\"\nRUN sed -i '/^export JAVA_HOME/ s:.*:export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64\\nexport HADOOP_HOME=/usr/local/hadoop\\n:' $HADOOP_HOME/etc/hadoop/hadoop-env.sh\nRUN sed -i '/^export HADOOP_CONF_DIR/ s:.*:export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop/:' $HADOOP_HOME/etc/hadoop/hadoop-env.sh\n#   workingaround docker.io build error\nRUN ls -la /usr/local/hadoop/etc/hadoop/*-env.sh\nRUN chmod +x /usr/local/hadoop/etc/hadoop/*-env.sh\nRUN ls -la /usr/local/hadoop/etc/hadoop/*-env.sh\n#   fix the 254 error code\nRUN sed -i \"/^[^#]*UsePAM/ s/.*/#&/\" /etc/ssh/sshd_config\nRUN echo \"UsePAM no\" >> /etc/ssh/sshd_config\nRUN echo \"Port 2122\" >> /etc/ssh/sshd_config\nRUN service ssh start \\\n && $HADOOP_HOME/etc/hadoop/hadoop-env.sh \\\n && $HADOOP_HOME/sbin/start-dfs.sh \\\n && $HADOOP_HOME/bin/hdfs dfs -mkdir -p /user/root\nRUN service ssh start \\\n && $HADOOP_HOME/etc/hadoop/hadoop-env.sh \\\n && $HADOOP_HOME/sbin/start-dfs.sh \\\n && $HADOOP_HOME/bin/hdfs dfs -put $HADOOP_HOME/etc/hadoop/ input\nCMD [\"/etc/bootstrap.sh\", \"-bash\"]\n#   Hdfs ports\nEXPOSE 50010/tcp 50020/tcp 50070/tcp 50075/tcp 50090/tcp 8020/tcp 9000/tcp\n#   Mapred ports\nEXPOSE 10020/tcp 19888/tcp\n#  Yarn ports\nEXPOSE 8030/tcp 8031/tcp 8032/tcp 8033/tcp 8040/tcp 8042/tcp 8088/tcp\n#  Other ports\nEXPOSE 49707/tcp 2122/tcp\n#   Continue with CaffeOnSpark build.\n#   ENV CAFFE_ON_SPARK=/opt/CaffeOnSpark\nWORKDIR $CAFFE_ON_SPARK\n#   RUN git clone https://github.com/yahoo/CaffeOnSpark.git . --recursive\nRUN cp caffe-public/Makefile.config.example caffe-public/Makefile.config\nRUN echo \"INCLUDE_DIRS += ${JAVA_HOME}/include\" >> caffe-public/Makefile.config\n#  RUN sed -i \"s/# USE_CUDNN := 1/USE_CUDNN := 1/g\" caffe-public/Makefile.config\nRUN sed -i \"s|BLAS := atlas|BLAS := open|g\" caffe-public/Makefile.config\nRUN make build\nENV LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:$CAFFE_ON_SPARK/caffe-public/distribute/lib:$CAFFE_ON_SPARK/caffe-distri/distribute/lib\"\nWORKDIR /root\n","originalDockerfileUglifiedHash":"0edb380f3d81f0f1a5b410255e67141e","fileName":"/ICSME-replicationpackage/dataset/smelly_dockerfiles_bianncle/43c154f073c1335314965048c5c5bf2da35ce3a5.dockerfile"}