{"seed":2196631671,"processedDockerfileHash":"8c4c1177285f3195079e3de2fa658da3","fixedSmells":["use-no-install-recommends","do-not-use-apt-get-update-alone","pin-package-manager-versions-apt-get","pin-package-manager-versions-pip","pin-package-manager-versions-npm","pin-package-manager-versions-gem","pin-package-manager-versions-apk","use-copy-instead-of-add","use-wget-instead-of-add","do-not-have-secrets","have-a-healthcheck","have-a-user"],"successfullyFixedSmells":["use-no-install-recommends","do-not-use-apt-get-update-alone","use-copy-instead-of-add","do-not-have-secrets","have-a-healthcheck"],"processedDockerfile":"ARG BASE_IMAGE_TAG=latest\nFROM edxops/xenial-common:${BASE_IMAGE_TAG}\nLABEL maintainer=\"edxops\"\nUSER root\n# A secret has been removed here. Please do not provide secrets from the Dockerfile as these will leak into the metadata of the resulting docker image. To provide secrets the --secret flag of the docker build command can be used (https://docs.docker.com/develop/develop-images/build_enhancements/#new-docker-build-secret-information).\nENV BOTO_CONFIG=\"/dev/null\" \\\n    JDK_URL=\"http://download.oracle.com/otn-pub/java/jdk/8u131-b11/d54c1d3a095b4ff2b6607d096fa80163/jdk-8u131-linux-x64.tar.gz\" \\\n    JDK_DIST_FILE=\"jdk-8u131-linux-x64.tar.gz\" \\\n    JAVA_HOME=\"/usr/lib/jvm/java-8-oracle\" \\\n    HADOOP_URL=\"https://archive.apache.org/dist/hadoop/common/hadoop-2.7.2/hadoop-2.7.2.tar.gz\" \\\n    HADOOP_DIST_FILE=\"hadoop-2.7.2.tar.gz\" \\\n    HADOOP_HOME=\"/edx/app/hadoop/hadoop\" \\\n    HADOOP_PREFIX=\"/edx/app/hadoop/hadoop\" \\\n    HIVE_URL=\"https://archive.apache.org/dist/hive/hive-2.1.1/apache-hive-2.1.1-bin.tar.gz\" \\\n    HIVE_DIST_FILE=\"apache-hive-2.1.1-bin.tar.gz\" \\\n    HIVE_HOME=\"/edx/app/hadoop/hive\" \\\n    SQOOP_URL=\"http://archive.apache.org/dist/sqoop/1.4.6/sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz\" \\\n    SQOOP_DIST_FILE=\"sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz\" \\\n    SQOOP_MYSQL_CONNECTOR_URL=\"http://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.29.tar.gz\" \\\n    SQOOP_MYSQL_CONNECTOR_FILE=\"mysql-connector-java-5.1.29\" \\\n    SQOOP_HOME=\"/edx/app/hadoop/sqoop\" \\\n    SQOOP_LIB=\"/edx/app/hadoop/sqoop/lib\" \\\n    SPARK_URL=\"https://archive.apache.org/dist/spark/spark-2.1.0/spark-2.1.0-bin-hadoop2.7.tgz\" \\\n    SPARK_DIST_FILE=\"spark-2.1.0-bin-hadoop2.7.tgz\" \\\n    SPARK_HOME=\"/edx/app/hadoop/spark\" \\\n    LUIGI_CONFIG_PATH=\"/edx/app/analytics_pipeline/analytics_pipeline/config/luigi_docker.cfg\" \\\n    ANALYTICS_PIPELINE_VENV=\"/edx/app/analytics_pipeline/venvs\" \\\n    BOOTSTRAP=\"/etc/bootstrap.sh\" \\\n    COMMON_BASE_DIR=\"/edx\" \\\n    COMMON_PIP_PACKAGES_PIP=\"pip==9.0.3\" \\\n    COMMON_PIP_PACKAGES_SETUPTOOLS=\"setuptools==39.0.1\" \\\n    COMMON_PIP_PACKAGES_VIRTUALENV=\"virtualenv==15.2.0\" \\\n    COMMON_PIP_PACKAGES_VIRTUALENVWRAPPER=\"virtualenvwrapper==4.8.2\" \\\n    COMMON_MYSQL_READ_ONLY_USER=\"read_only\" \\\n    COMMON_MYSQL_READ_ONLY_PASS=\"password\" \\\n    ANALYTICS_PIPELINE_OUTPUT_DATABASE_USER=\"pipeline001\" \\\n    EDX_PPA_KEY_SERVER=\"keyserver.ubuntu.com\" \\\n    EDX_PPA_KEY_ID=\"69464050\"\nENV PATH=\"/edx/app/analytics_pipeline/venvs/analytics_pipeline/bin:${JAVA_HOME}/bin:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:${HIVE_HOME}/bin:${SPARK_HOME}/bin:${SPARK_HOME}/sbin:${SQOOP_HOME}/bin:$PATH\" \\\n    COMMON_DATA_DIR=\"$COMMON_BASE_DIR/var\" \\\n    COMMON_APP_DIR=\"$COMMON_BASE_DIR/app\" \\\n    COMMON_LOG_DIR=\"$COMMON_BASE_DIR/var/log\" \\\n    COMMON_BIN_DIR=\"$COMMON_BASE_DIR/bin\" \\\n    COMMON_CFG_DIR=\"$COMMON_BASE_DIR/etc\"\n#   add custom PPAs & install packages\nRUN apt-get update -y \\\n && (apt-get update ;apt-get install --no-install-recommends software-properties-common -y ) \\\n && apt-key adv --keyserver $EDX_PPA_KEY_SERVER --recv-keys $EDX_PPA_KEY_ID \\\n && add-apt-repository -y 'deb http://ppa.edx.org xenial main' \\\n && apt-get update -y \\\n && (apt-get update ;apt-get install --no-install-recommends python2.7 python2.7-dev python-pip python-apt python-yaml python-jinja2 libmysqlclient-dev libffi-dev libssl-dev libatlas-base-dev libblas-dev liblapack-dev libpq-dev sudo make build-essential git-core openssh-server openssh-client rsync software-properties-common vim net-tools curl netcat mysql-client-5.6 apt-transport-https ntp acl lynx-cur logrotate rsyslog unzip ack-grep mosh tree screen tmux dnsutils inetutils-telnet -y ) \\\n && rm -rf /var/lib/apt/lists/*\n#   creating directory structure\nRUN mkdir -p $HADOOP_HOME $JAVA_HOME $ANALYTICS_PIPELINE_VENV /edx/app/hadoop/lib $HIVE_HOME /etc/luigi $SPARK_HOME $SQOOP_HOME $COMMON_DATA_DIR $COMMON_APP_DIR $COMMON_LOG_DIR $COMMON_BIN_DIR $COMMON_CFG_DIR/edx-analytics-pipeline\n#   create user & group for hadoop\nRUN groupadd hadoop\nRUN useradd -ms /bin/bash hadoop -g hadoop -d /edx/app/hadoop\nRUN echo '%hadoop ALL=(ALL) NOPASSWD:ALL' >> /etc/sudoers\n#   JAVA\nRUN curl -fSL --header \"Cookie:oraclelicense=accept-securebackup-cookie\" \"$JDK_URL\" -o /var/tmp/$JDK_DIST_FILE \\\n && tar -xzf /var/tmp/$JDK_DIST_FILE -C $JAVA_HOME --strip-components=1 \\\n && rm -f /var/tmp/$JDK_DIST_FILE\n#   HADOOP\nRUN curl -fSL \"$HADOOP_URL\" -o /var/tmp/$HADOOP_DIST_FILE \\\n && tar -xzf /var/tmp/$HADOOP_DIST_FILE -C $HADOOP_HOME --strip-components=1 \\\n && sed -i '/^export JAVA_HOME/ s:.*:export JAVA_HOME=/usr/lib/jvm/java-8-oracle\\nexport HADOOP_PREFIX=/edx/app/hadoop/hadoop\\nexport HADOOP_HOME=/edx/app/hadoop/hadoop\\n:' $HADOOP_HOME/etc/hadoop/hadoop-env.sh \\\n && sed -i '/^export HADOOP_CONF_DIR/ s:.*:export HADOOP_CONF_DIR=/edx/app/hadoop/hadoop/etc/hadoop/:' $HADOOP_HOME/etc/hadoop/hadoop-env.sh \\\n && sed -i 's#<configuration>#<configuration><property><name>fs.defaultFS</name><value>hdfs://namenode:8020</value></property>#' $HADOOP_HOME/etc/hadoop/core-site.xml \\\n && sed 's#<configuration>#<configuration><property><name>mapreduce.framework.name</name><value>yarn</value></property>#' $HADOOP_HOME/etc/hadoop/mapred-site.xml.template > $HADOOP_HOME/etc/hadoop/mapred-site.xml \\\n && sed -i 's#<configuration>#<configuration><property><name>yarn.resourcemanager.hostname</name><value>resourcemanager</value></property>#' $HADOOP_HOME/etc/hadoop/yarn-site.xml \\\n && rm -f /var/tmp/$HADOOP_DIST_FILE\n#   HIVE\nRUN curl -fSL \"$HIVE_URL\" -o /var/tmp/$HIVE_DIST_FILE \\\n && tar -xzf /var/tmp/$HIVE_DIST_FILE -C $HIVE_HOME --strip-components=1 \\\n && rm -f /var/tmp/$HIVE_DIST_FILE\nCOPY docker/build/analytics_pipeline/hive-site.xml.template $HIVE_HOME/conf/hive-site.xml\n#   SPARK\nRUN curl -fSL \"$SPARK_URL\" -o /var/tmp/$SPARK_DIST_FILE \\\n && tar -xzf /var/tmp/$SPARK_DIST_FILE -C $SPARK_HOME --strip-components=1 \\\n && echo 'spark.master spark://sparkmaster:7077\\nspark.eventLog.enabled true\\nspark.eventLog.dir hdfs://namenode:8020/tmp/spark-events\\nspark.history.fs.logDirectory hdfs://namenode:8020/tmp/spark-events\\nspark.sql.warehouse.dir hdfs://namenode:8020/spark-warehouse' > $SPARK_HOME/conf/spark-defaults.conf \\\n && rm -f /var/tmp/$SPARK_DIST_FILE\n#   SQOOP\nRUN curl -fSL \"$SQOOP_URL\" -o /var/tmp/$SQOOP_DIST_FILE \\\n && curl -fSL \"$SQOOP_MYSQL_CONNECTOR_URL\" -o /var/tmp/$SQOOP_MYSQL_CONNECTOR_FILE.tar.gz \\\n && tar -xzf /var/tmp/$SQOOP_DIST_FILE -C $SQOOP_HOME --strip-components=1 \\\n && tar -xzf /var/tmp/$SQOOP_MYSQL_CONNECTOR_FILE.tar.gz -C /var/tmp/ \\\n && cp /var/tmp/$SQOOP_MYSQL_CONNECTOR_FILE/$SQOOP_MYSQL_CONNECTOR_FILE-bin.jar $SQOOP_LIB \\\n && cp /var/tmp/$SQOOP_MYSQL_CONNECTOR_FILE/$SQOOP_MYSQL_CONNECTOR_FILE-bin.jar $HIVE_HOME/lib/ \\\n && rm -rf /var/tmp/$SQOOP_DIST_FILE /var/tmp/$SQOOP_MYSQL_CONNECTOR_FILE*\nWORKDIR /var/tmp\n#   Edx Hadoop Util Library\nRUN git clone https://github.com/edx/edx-analytics-hadoop-util \\\n && cd /var/tmp/edx-analytics-hadoop-util \\\n && $JAVA_HOME/bin/javac -cp `/edx/app/hadoop/hadoop/bin/hadoop classpath ` org/edx/hadoop/input/ManifestTextInputFormat.java \\\n && $JAVA_HOME/bin/jar cf /edx/app/hadoop/lib/edx-analytics-hadoop-util.jar org/edx/hadoop/input/ManifestTextInputFormat.class\n#   configure bootstrap scripts for container\nCOPY docker/build/analytics_pipeline/bootstrap.sh /etc/bootstrap.sh\nRUN chown hadoop:hadoop /etc/bootstrap.sh \\\n && chmod 700 /etc/bootstrap.sh \\\n && chown -R hadoop:hadoop /edx/app/hadoop\n#   Analytics pipeline\nARG OPENEDX_RELEASE=master\nENV OPENEDX_RELEASE=\"${OPENEDX_RELEASE}\"\nRUN git clone https://github.com/edx/edx-analytics-pipeline \\\n && cd edx-analytics-pipeline \\\n && git checkout ${OPENEDX_RELEASE} \\\n && cd .. \\\n && cp /var/tmp/edx-analytics-pipeline/Makefile /var/tmp/Makefile \\\n && cp -r /var/tmp/edx-analytics-pipeline/requirements /var/tmp/requirements \\\n && rm -rf /var/tmp/edx-analytics-pipeline\nRUN pip install $COMMON_PIP_PACKAGES_PIP $COMMON_PIP_PACKAGES_SETUPTOOLS $COMMON_PIP_PACKAGES_VIRTUALENV $COMMON_PIP_PACKAGES_VIRTUALENVWRAPPER \\\n && virtualenv $ANALYTICS_PIPELINE_VENV/analytics_pipeline/ \\\n && chown -R hadoop:hadoop $ANALYTICS_PIPELINE_VENV/analytics_pipeline/ \\\n && echo '[hadoop]\\nversion: cdh4\\ncommand: /edx/app/hadoop/hadoop/bin/hadoop\\nstreaming-jar: /edx/app/hadoop/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar' > /etc/luigi/client.cfg\nRUN : \\\n && make system-requirements\nCOPY docker/build/analytics_pipeline/devstack.sh /edx/app/analytics_pipeline/devstack.sh\nRUN chown hadoop:hadoop /edx/app/analytics_pipeline/devstack.sh \\\n && chmod a+x /edx/app/analytics_pipeline/devstack.sh\nUSER hadoop\nRUN touch /edx/app/hadoop/.bashrc \\\n && echo 'export JAVA_HOME=/usr/lib/jvm/java-8-oracle\\nexport HADOOP_HOME=/edx/app/hadoop/hadoop\\nexport HIVE_HOME=/edx/app/hadoop/hive\\nexport SQOOP_HOME=/edx/app/hadoop/sqoop\\nexport SPARK_HOME=/edx/app/hadoop/spark\\nexport PATH=\"/edx/app/analytics_pipeline/venvs/analytics_pipeline/bin:${JAVA_HOME}/bin:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:${HIVE_HOME}/bin:${SPARK_HOME}/bin:${SPARK_HOME}/sbin:${SQOOP_HOME}/bin:$PATH\"' > /edx/app/hadoop/.bashrc \\\n && . $ANALYTICS_PIPELINE_VENV/analytics_pipeline/bin/activate \\\n && make test-requirements requirements\nRUN sudo chown hadoop:hadoop $COMMON_CFG_DIR/edx-analytics-pipeline/ \\\n && echo \"{\\\"username\\\": \\\"$COMMON_MYSQL_READ_ONLY_USER\\\", \\\"host\\\": \\\"mysql\\\", \\\"password\\\": \\\"$COMMON_MYSQL_READ_ONLY_PASS\\\", \\\"port\\\": 3306}\" > $COMMON_CFG_DIR/edx-analytics-pipeline/input.json \\\n && echo \"{\\\"username\\\": \\\"$ANALYTICS_PIPELINE_OUTPUT_DATABASE_USER\\\", \\\"host\\\": \\\"mysql\\\", \\\"password\\\": \\\"$ANALYTICS_PIPELINE_OUTPUT_DATABASE_PASSWORD\\\", \\\"port\\\": 3306}\" > $COMMON_CFG_DIR/edx-analytics-pipeline/output.json \\\n && echo \"{\\\"username\\\": \\\"dbadmin\\\", \\\"host\\\": \\\"vertica\\\", \\\"password\\\": \\\"\\\", \\\"port\\\": 5433}\" > $COMMON_CFG_DIR/edx-analytics-pipeline/warehouse.json\nCOPY docker/build/analytics_pipeline/acceptance.json $COMMON_CFG_DIR/edx-analytics-pipeline/acceptance.json\nWORKDIR /edx/app/analytics_pipeline/analytics_pipeline\nCMD [\"/etc/bootstrap.sh\", \"-d\"]\n# Please add your HEALTHCHECK here!!!\n","originalDockerfile":"ARG BASE_IMAGE_TAG=latest\nFROM edxops/xenial-common:${BASE_IMAGE_TAG}\nLABEL maintainer=\"edxops\"\nUSER root\nENV BOTO_CONFIG=\"/dev/null\" \\\n    JDK_URL=\"http://download.oracle.com/otn-pub/java/jdk/8u131-b11/d54c1d3a095b4ff2b6607d096fa80163/jdk-8u131-linux-x64.tar.gz\" \\\n    JDK_DIST_FILE=\"jdk-8u131-linux-x64.tar.gz\" \\\n    JAVA_HOME=\"/usr/lib/jvm/java-8-oracle\" \\\n    HADOOP_URL=\"https://archive.apache.org/dist/hadoop/common/hadoop-2.7.2/hadoop-2.7.2.tar.gz\" \\\n    HADOOP_DIST_FILE=\"hadoop-2.7.2.tar.gz\" \\\n    HADOOP_HOME=\"/edx/app/hadoop/hadoop\" \\\n    HADOOP_PREFIX=\"/edx/app/hadoop/hadoop\" \\\n    HIVE_URL=\"https://archive.apache.org/dist/hive/hive-2.1.1/apache-hive-2.1.1-bin.tar.gz\" \\\n    HIVE_DIST_FILE=\"apache-hive-2.1.1-bin.tar.gz\" \\\n    HIVE_HOME=\"/edx/app/hadoop/hive\" \\\n    SQOOP_URL=\"http://archive.apache.org/dist/sqoop/1.4.6/sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz\" \\\n    SQOOP_DIST_FILE=\"sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz\" \\\n    SQOOP_MYSQL_CONNECTOR_URL=\"http://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.29.tar.gz\" \\\n    SQOOP_MYSQL_CONNECTOR_FILE=\"mysql-connector-java-5.1.29\" \\\n    SQOOP_HOME=\"/edx/app/hadoop/sqoop\" \\\n    SQOOP_LIB=\"/edx/app/hadoop/sqoop/lib\" \\\n    SPARK_URL=\"https://archive.apache.org/dist/spark/spark-2.1.0/spark-2.1.0-bin-hadoop2.7.tgz\" \\\n    SPARK_DIST_FILE=\"spark-2.1.0-bin-hadoop2.7.tgz\" \\\n    SPARK_HOME=\"/edx/app/hadoop/spark\" \\\n    LUIGI_CONFIG_PATH=\"/edx/app/analytics_pipeline/analytics_pipeline/config/luigi_docker.cfg\" \\\n    ANALYTICS_PIPELINE_VENV=\"/edx/app/analytics_pipeline/venvs\" \\\n    BOOTSTRAP=\"/etc/bootstrap.sh\" \\\n    COMMON_BASE_DIR=\"/edx\" \\\n    COMMON_PIP_PACKAGES_PIP=\"pip==9.0.3\" \\\n    COMMON_PIP_PACKAGES_SETUPTOOLS=\"setuptools==39.0.1\" \\\n    COMMON_PIP_PACKAGES_VIRTUALENV=\"virtualenv==15.2.0\" \\\n    COMMON_PIP_PACKAGES_VIRTUALENVWRAPPER=\"virtualenvwrapper==4.8.2\" \\\n    COMMON_MYSQL_READ_ONLY_USER=\"read_only\" \\\n    COMMON_MYSQL_READ_ONLY_PASS=\"password\" \\\n    ANALYTICS_PIPELINE_OUTPUT_DATABASE_USER=\"pipeline001\" \\\n    ANALYTICS_PIPELINE_OUTPUT_DATABASE_PASSWORD=\"password\" \\\n    EDX_PPA_KEY_SERVER=\"keyserver.ubuntu.com\" \\\n    EDX_PPA_KEY_ID=\"69464050\"\nENV PATH=\"/edx/app/analytics_pipeline/venvs/analytics_pipeline/bin:${JAVA_HOME}/bin:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:${HIVE_HOME}/bin:${SPARK_HOME}/bin:${SPARK_HOME}/sbin:${SQOOP_HOME}/bin:$PATH\" \\\n    COMMON_DATA_DIR=\"$COMMON_BASE_DIR/var\" \\\n    COMMON_APP_DIR=\"$COMMON_BASE_DIR/app\" \\\n    COMMON_LOG_DIR=\"$COMMON_BASE_DIR/var/log\" \\\n    COMMON_BIN_DIR=\"$COMMON_BASE_DIR/bin\" \\\n    COMMON_CFG_DIR=\"$COMMON_BASE_DIR/etc\"\n#  add custom PPAs & install packages\nRUN apt-get update -y \\\n && apt-get install software-properties-common -y \\\n && apt-key adv --keyserver $EDX_PPA_KEY_SERVER --recv-keys $EDX_PPA_KEY_ID \\\n && add-apt-repository -y 'deb http://ppa.edx.org xenial main' \\\n && apt-get update -y \\\n && apt-get install --no-install-recommends python2.7 python2.7-dev python-pip python-apt python-yaml python-jinja2 libmysqlclient-dev libffi-dev libssl-dev libatlas-base-dev libblas-dev liblapack-dev libpq-dev sudo make build-essential git-core openssh-server openssh-client rsync software-properties-common vim net-tools curl netcat mysql-client-5.6 apt-transport-https ntp acl lynx-cur logrotate rsyslog unzip ack-grep mosh tree screen tmux dnsutils inetutils-telnet -y \\\n && rm -rf /var/lib/apt/lists/*\n#  creating directory structure\nRUN mkdir -p $HADOOP_HOME $JAVA_HOME $ANALYTICS_PIPELINE_VENV /edx/app/hadoop/lib $HIVE_HOME /etc/luigi $SPARK_HOME $SQOOP_HOME $COMMON_DATA_DIR $COMMON_APP_DIR $COMMON_LOG_DIR $COMMON_BIN_DIR $COMMON_CFG_DIR/edx-analytics-pipeline\n#  create user & group for hadoop\nRUN groupadd hadoop\nRUN useradd -ms /bin/bash hadoop -g hadoop -d /edx/app/hadoop\nRUN echo '%hadoop ALL=(ALL) NOPASSWD:ALL' >> /etc/sudoers\n#  JAVA\nRUN curl -fSL --header \"Cookie:oraclelicense=accept-securebackup-cookie\" \"$JDK_URL\" -o /var/tmp/$JDK_DIST_FILE \\\n && tar -xzf /var/tmp/$JDK_DIST_FILE -C $JAVA_HOME --strip-components=1 \\\n && rm -f /var/tmp/$JDK_DIST_FILE\n#  HADOOP\nRUN curl -fSL \"$HADOOP_URL\" -o /var/tmp/$HADOOP_DIST_FILE \\\n && tar -xzf /var/tmp/$HADOOP_DIST_FILE -C $HADOOP_HOME --strip-components=1 \\\n && sed -i '/^export JAVA_HOME/ s:.*:export JAVA_HOME=/usr/lib/jvm/java-8-oracle\\nexport HADOOP_PREFIX=/edx/app/hadoop/hadoop\\nexport HADOOP_HOME=/edx/app/hadoop/hadoop\\n:' $HADOOP_HOME/etc/hadoop/hadoop-env.sh \\\n && sed -i '/^export HADOOP_CONF_DIR/ s:.*:export HADOOP_CONF_DIR=/edx/app/hadoop/hadoop/etc/hadoop/:' $HADOOP_HOME/etc/hadoop/hadoop-env.sh \\\n && sed -i 's#<configuration>#<configuration><property><name>fs.defaultFS</name><value>hdfs://namenode:8020</value></property>#' $HADOOP_HOME/etc/hadoop/core-site.xml \\\n && sed 's#<configuration>#<configuration><property><name>mapreduce.framework.name</name><value>yarn</value></property>#' $HADOOP_HOME/etc/hadoop/mapred-site.xml.template > $HADOOP_HOME/etc/hadoop/mapred-site.xml \\\n && sed -i 's#<configuration>#<configuration><property><name>yarn.resourcemanager.hostname</name><value>resourcemanager</value></property>#' $HADOOP_HOME/etc/hadoop/yarn-site.xml \\\n && rm -f /var/tmp/$HADOOP_DIST_FILE\n#  HIVE\nRUN curl -fSL \"$HIVE_URL\" -o /var/tmp/$HIVE_DIST_FILE \\\n && tar -xzf /var/tmp/$HIVE_DIST_FILE -C $HIVE_HOME --strip-components=1 \\\n && rm -f /var/tmp/$HIVE_DIST_FILE\nADD docker/build/analytics_pipeline/hive-site.xml.template $HIVE_HOME/conf/hive-site.xml\n#  SPARK\nRUN curl -fSL \"$SPARK_URL\" -o /var/tmp/$SPARK_DIST_FILE \\\n && tar -xzf /var/tmp/$SPARK_DIST_FILE -C $SPARK_HOME --strip-components=1 \\\n && echo 'spark.master spark://sparkmaster:7077\\nspark.eventLog.enabled true\\nspark.eventLog.dir hdfs://namenode:8020/tmp/spark-events\\nspark.history.fs.logDirectory hdfs://namenode:8020/tmp/spark-events\\nspark.sql.warehouse.dir hdfs://namenode:8020/spark-warehouse' > $SPARK_HOME/conf/spark-defaults.conf \\\n && rm -f /var/tmp/$SPARK_DIST_FILE\n#  SQOOP\nRUN curl -fSL \"$SQOOP_URL\" -o /var/tmp/$SQOOP_DIST_FILE \\\n && curl -fSL \"$SQOOP_MYSQL_CONNECTOR_URL\" -o /var/tmp/$SQOOP_MYSQL_CONNECTOR_FILE.tar.gz \\\n && tar -xzf /var/tmp/$SQOOP_DIST_FILE -C $SQOOP_HOME --strip-components=1 \\\n && tar -xzf /var/tmp/$SQOOP_MYSQL_CONNECTOR_FILE.tar.gz -C /var/tmp/ \\\n && cp /var/tmp/$SQOOP_MYSQL_CONNECTOR_FILE/$SQOOP_MYSQL_CONNECTOR_FILE-bin.jar $SQOOP_LIB \\\n && cp /var/tmp/$SQOOP_MYSQL_CONNECTOR_FILE/$SQOOP_MYSQL_CONNECTOR_FILE-bin.jar $HIVE_HOME/lib/ \\\n && rm -rf /var/tmp/$SQOOP_DIST_FILE /var/tmp/$SQOOP_MYSQL_CONNECTOR_FILE*\nWORKDIR /var/tmp\n#  Edx Hadoop Util Library\nRUN git clone https://github.com/edx/edx-analytics-hadoop-util \\\n && cd /var/tmp/edx-analytics-hadoop-util \\\n && $JAVA_HOME/bin/javac -cp `/edx/app/hadoop/hadoop/bin/hadoop classpath ` org/edx/hadoop/input/ManifestTextInputFormat.java \\\n && $JAVA_HOME/bin/jar cf /edx/app/hadoop/lib/edx-analytics-hadoop-util.jar org/edx/hadoop/input/ManifestTextInputFormat.class\n#  configure bootstrap scripts for container\nADD docker/build/analytics_pipeline/bootstrap.sh /etc/bootstrap.sh\nRUN chown hadoop:hadoop /etc/bootstrap.sh \\\n && chmod 700 /etc/bootstrap.sh \\\n && chown -R hadoop:hadoop /edx/app/hadoop\n#  Analytics pipeline\nARG OPENEDX_RELEASE=master\nENV OPENEDX_RELEASE=\"${OPENEDX_RELEASE}\"\nRUN git clone https://github.com/edx/edx-analytics-pipeline \\\n && cd edx-analytics-pipeline \\\n && git checkout ${OPENEDX_RELEASE} \\\n && cd .. \\\n && cp /var/tmp/edx-analytics-pipeline/Makefile /var/tmp/Makefile \\\n && cp -r /var/tmp/edx-analytics-pipeline/requirements /var/tmp/requirements \\\n && rm -rf /var/tmp/edx-analytics-pipeline\nRUN pip install $COMMON_PIP_PACKAGES_PIP $COMMON_PIP_PACKAGES_SETUPTOOLS $COMMON_PIP_PACKAGES_VIRTUALENV $COMMON_PIP_PACKAGES_VIRTUALENVWRAPPER \\\n && virtualenv $ANALYTICS_PIPELINE_VENV/analytics_pipeline/ \\\n && chown -R hadoop:hadoop $ANALYTICS_PIPELINE_VENV/analytics_pipeline/ \\\n && echo '[hadoop]\\nversion: cdh4\\ncommand: /edx/app/hadoop/hadoop/bin/hadoop\\nstreaming-jar: /edx/app/hadoop/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar' > /etc/luigi/client.cfg\nRUN apt-get update \\\n && make system-requirements\nADD docker/build/analytics_pipeline/devstack.sh /edx/app/analytics_pipeline/devstack.sh\nRUN chown hadoop:hadoop /edx/app/analytics_pipeline/devstack.sh \\\n && chmod a+x /edx/app/analytics_pipeline/devstack.sh\nUSER hadoop\nRUN touch /edx/app/hadoop/.bashrc \\\n && echo 'export JAVA_HOME=/usr/lib/jvm/java-8-oracle\\nexport HADOOP_HOME=/edx/app/hadoop/hadoop\\nexport HIVE_HOME=/edx/app/hadoop/hive\\nexport SQOOP_HOME=/edx/app/hadoop/sqoop\\nexport SPARK_HOME=/edx/app/hadoop/spark\\nexport PATH=\"/edx/app/analytics_pipeline/venvs/analytics_pipeline/bin:${JAVA_HOME}/bin:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:${HIVE_HOME}/bin:${SPARK_HOME}/bin:${SPARK_HOME}/sbin:${SQOOP_HOME}/bin:$PATH\"' > /edx/app/hadoop/.bashrc \\\n && . $ANALYTICS_PIPELINE_VENV/analytics_pipeline/bin/activate \\\n && make test-requirements requirements\nRUN sudo chown hadoop:hadoop $COMMON_CFG_DIR/edx-analytics-pipeline/ \\\n && echo \"{\\\"username\\\": \\\"$COMMON_MYSQL_READ_ONLY_USER\\\", \\\"host\\\": \\\"mysql\\\", \\\"password\\\": \\\"$COMMON_MYSQL_READ_ONLY_PASS\\\", \\\"port\\\": 3306}\" > $COMMON_CFG_DIR/edx-analytics-pipeline/input.json \\\n && echo \"{\\\"username\\\": \\\"$ANALYTICS_PIPELINE_OUTPUT_DATABASE_USER\\\", \\\"host\\\": \\\"mysql\\\", \\\"password\\\": \\\"$ANALYTICS_PIPELINE_OUTPUT_DATABASE_PASSWORD\\\", \\\"port\\\": 3306}\" > $COMMON_CFG_DIR/edx-analytics-pipeline/output.json \\\n && echo \"{\\\"username\\\": \\\"dbadmin\\\", \\\"host\\\": \\\"vertica\\\", \\\"password\\\": \\\"\\\", \\\"port\\\": 5433}\" > $COMMON_CFG_DIR/edx-analytics-pipeline/warehouse.json\nADD docker/build/analytics_pipeline/acceptance.json $COMMON_CFG_DIR/edx-analytics-pipeline/acceptance.json\nWORKDIR /edx/app/analytics_pipeline/analytics_pipeline\nCMD [\"/etc/bootstrap.sh\", \"-d\"]\n","injectedSmells":[],"originalDockerfileHash":"120a2dd2df21d5d6bf667e4b82edf077","successfullyInjectedSmells":[],"originalDockerfileUglified":"ARG BASE_IMAGE_TAG=latest\nFROM edxops/xenial-common:${BASE_IMAGE_TAG}\nLABEL maintainer=\"edxops\"\nUSER root\nENV BOTO_CONFIG=\"/dev/null\" \\\n    JDK_URL=\"http://download.oracle.com/otn-pub/java/jdk/8u131-b11/d54c1d3a095b4ff2b6607d096fa80163/jdk-8u131-linux-x64.tar.gz\" \\\n    JDK_DIST_FILE=\"jdk-8u131-linux-x64.tar.gz\" \\\n    JAVA_HOME=\"/usr/lib/jvm/java-8-oracle\" \\\n    HADOOP_URL=\"https://archive.apache.org/dist/hadoop/common/hadoop-2.7.2/hadoop-2.7.2.tar.gz\" \\\n    HADOOP_DIST_FILE=\"hadoop-2.7.2.tar.gz\" \\\n    HADOOP_HOME=\"/edx/app/hadoop/hadoop\" \\\n    HADOOP_PREFIX=\"/edx/app/hadoop/hadoop\" \\\n    HIVE_URL=\"https://archive.apache.org/dist/hive/hive-2.1.1/apache-hive-2.1.1-bin.tar.gz\" \\\n    HIVE_DIST_FILE=\"apache-hive-2.1.1-bin.tar.gz\" \\\n    HIVE_HOME=\"/edx/app/hadoop/hive\" \\\n    SQOOP_URL=\"http://archive.apache.org/dist/sqoop/1.4.6/sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz\" \\\n    SQOOP_DIST_FILE=\"sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz\" \\\n    SQOOP_MYSQL_CONNECTOR_URL=\"http://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.29.tar.gz\" \\\n    SQOOP_MYSQL_CONNECTOR_FILE=\"mysql-connector-java-5.1.29\" \\\n    SQOOP_HOME=\"/edx/app/hadoop/sqoop\" \\\n    SQOOP_LIB=\"/edx/app/hadoop/sqoop/lib\" \\\n    SPARK_URL=\"https://archive.apache.org/dist/spark/spark-2.1.0/spark-2.1.0-bin-hadoop2.7.tgz\" \\\n    SPARK_DIST_FILE=\"spark-2.1.0-bin-hadoop2.7.tgz\" \\\n    SPARK_HOME=\"/edx/app/hadoop/spark\" \\\n    LUIGI_CONFIG_PATH=\"/edx/app/analytics_pipeline/analytics_pipeline/config/luigi_docker.cfg\" \\\n    ANALYTICS_PIPELINE_VENV=\"/edx/app/analytics_pipeline/venvs\" \\\n    BOOTSTRAP=\"/etc/bootstrap.sh\" \\\n    COMMON_BASE_DIR=\"/edx\" \\\n    COMMON_PIP_PACKAGES_PIP=\"pip==9.0.3\" \\\n    COMMON_PIP_PACKAGES_SETUPTOOLS=\"setuptools==39.0.1\" \\\n    COMMON_PIP_PACKAGES_VIRTUALENV=\"virtualenv==15.2.0\" \\\n    COMMON_PIP_PACKAGES_VIRTUALENVWRAPPER=\"virtualenvwrapper==4.8.2\" \\\n    COMMON_MYSQL_READ_ONLY_USER=\"read_only\" \\\n    COMMON_MYSQL_READ_ONLY_PASS=\"password\" \\\n    ANALYTICS_PIPELINE_OUTPUT_DATABASE_USER=\"pipeline001\" \\\n    ANALYTICS_PIPELINE_OUTPUT_DATABASE_PASSWORD=\"password\" \\\n    EDX_PPA_KEY_SERVER=\"keyserver.ubuntu.com\" \\\n    EDX_PPA_KEY_ID=\"69464050\"\nENV PATH=\"/edx/app/analytics_pipeline/venvs/analytics_pipeline/bin:${JAVA_HOME}/bin:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:${HIVE_HOME}/bin:${SPARK_HOME}/bin:${SPARK_HOME}/sbin:${SQOOP_HOME}/bin:$PATH\" \\\n    COMMON_DATA_DIR=\"$COMMON_BASE_DIR/var\" \\\n    COMMON_APP_DIR=\"$COMMON_BASE_DIR/app\" \\\n    COMMON_LOG_DIR=\"$COMMON_BASE_DIR/var/log\" \\\n    COMMON_BIN_DIR=\"$COMMON_BASE_DIR/bin\" \\\n    COMMON_CFG_DIR=\"$COMMON_BASE_DIR/etc\"\n#   add custom PPAs & install packages\nRUN apt-get update -y \\\n && apt-get install software-properties-common -y \\\n && apt-key adv --keyserver $EDX_PPA_KEY_SERVER --recv-keys $EDX_PPA_KEY_ID \\\n && add-apt-repository -y 'deb http://ppa.edx.org xenial main' \\\n && apt-get update -y \\\n && apt-get install --no-install-recommends python2.7 python2.7-dev python-pip python-apt python-yaml python-jinja2 libmysqlclient-dev libffi-dev libssl-dev libatlas-base-dev libblas-dev liblapack-dev libpq-dev sudo make build-essential git-core openssh-server openssh-client rsync software-properties-common vim net-tools curl netcat mysql-client-5.6 apt-transport-https ntp acl lynx-cur logrotate rsyslog unzip ack-grep mosh tree screen tmux dnsutils inetutils-telnet -y \\\n && rm -rf /var/lib/apt/lists/*\n#   creating directory structure\nRUN mkdir -p $HADOOP_HOME $JAVA_HOME $ANALYTICS_PIPELINE_VENV /edx/app/hadoop/lib $HIVE_HOME /etc/luigi $SPARK_HOME $SQOOP_HOME $COMMON_DATA_DIR $COMMON_APP_DIR $COMMON_LOG_DIR $COMMON_BIN_DIR $COMMON_CFG_DIR/edx-analytics-pipeline\n#   create user & group for hadoop\nRUN groupadd hadoop\nRUN useradd -ms /bin/bash hadoop -g hadoop -d /edx/app/hadoop\nRUN echo '%hadoop ALL=(ALL) NOPASSWD:ALL' >> /etc/sudoers\n#   JAVA\nRUN curl -fSL --header \"Cookie:oraclelicense=accept-securebackup-cookie\" \"$JDK_URL\" -o /var/tmp/$JDK_DIST_FILE \\\n && tar -xzf /var/tmp/$JDK_DIST_FILE -C $JAVA_HOME --strip-components=1 \\\n && rm -f /var/tmp/$JDK_DIST_FILE\n#   HADOOP\nRUN curl -fSL \"$HADOOP_URL\" -o /var/tmp/$HADOOP_DIST_FILE \\\n && tar -xzf /var/tmp/$HADOOP_DIST_FILE -C $HADOOP_HOME --strip-components=1 \\\n && sed -i '/^export JAVA_HOME/ s:.*:export JAVA_HOME=/usr/lib/jvm/java-8-oracle\\nexport HADOOP_PREFIX=/edx/app/hadoop/hadoop\\nexport HADOOP_HOME=/edx/app/hadoop/hadoop\\n:' $HADOOP_HOME/etc/hadoop/hadoop-env.sh \\\n && sed -i '/^export HADOOP_CONF_DIR/ s:.*:export HADOOP_CONF_DIR=/edx/app/hadoop/hadoop/etc/hadoop/:' $HADOOP_HOME/etc/hadoop/hadoop-env.sh \\\n && sed -i 's#<configuration>#<configuration><property><name>fs.defaultFS</name><value>hdfs://namenode:8020</value></property>#' $HADOOP_HOME/etc/hadoop/core-site.xml \\\n && sed 's#<configuration>#<configuration><property><name>mapreduce.framework.name</name><value>yarn</value></property>#' $HADOOP_HOME/etc/hadoop/mapred-site.xml.template > $HADOOP_HOME/etc/hadoop/mapred-site.xml \\\n && sed -i 's#<configuration>#<configuration><property><name>yarn.resourcemanager.hostname</name><value>resourcemanager</value></property>#' $HADOOP_HOME/etc/hadoop/yarn-site.xml \\\n && rm -f /var/tmp/$HADOOP_DIST_FILE\n#   HIVE\nRUN curl -fSL \"$HIVE_URL\" -o /var/tmp/$HIVE_DIST_FILE \\\n && tar -xzf /var/tmp/$HIVE_DIST_FILE -C $HIVE_HOME --strip-components=1 \\\n && rm -f /var/tmp/$HIVE_DIST_FILE\nADD docker/build/analytics_pipeline/hive-site.xml.template $HIVE_HOME/conf/hive-site.xml\n#   SPARK\nRUN curl -fSL \"$SPARK_URL\" -o /var/tmp/$SPARK_DIST_FILE \\\n && tar -xzf /var/tmp/$SPARK_DIST_FILE -C $SPARK_HOME --strip-components=1 \\\n && echo 'spark.master spark://sparkmaster:7077\\nspark.eventLog.enabled true\\nspark.eventLog.dir hdfs://namenode:8020/tmp/spark-events\\nspark.history.fs.logDirectory hdfs://namenode:8020/tmp/spark-events\\nspark.sql.warehouse.dir hdfs://namenode:8020/spark-warehouse' > $SPARK_HOME/conf/spark-defaults.conf \\\n && rm -f /var/tmp/$SPARK_DIST_FILE\n#   SQOOP\nRUN curl -fSL \"$SQOOP_URL\" -o /var/tmp/$SQOOP_DIST_FILE \\\n && curl -fSL \"$SQOOP_MYSQL_CONNECTOR_URL\" -o /var/tmp/$SQOOP_MYSQL_CONNECTOR_FILE.tar.gz \\\n && tar -xzf /var/tmp/$SQOOP_DIST_FILE -C $SQOOP_HOME --strip-components=1 \\\n && tar -xzf /var/tmp/$SQOOP_MYSQL_CONNECTOR_FILE.tar.gz -C /var/tmp/ \\\n && cp /var/tmp/$SQOOP_MYSQL_CONNECTOR_FILE/$SQOOP_MYSQL_CONNECTOR_FILE-bin.jar $SQOOP_LIB \\\n && cp /var/tmp/$SQOOP_MYSQL_CONNECTOR_FILE/$SQOOP_MYSQL_CONNECTOR_FILE-bin.jar $HIVE_HOME/lib/ \\\n && rm -rf /var/tmp/$SQOOP_DIST_FILE /var/tmp/$SQOOP_MYSQL_CONNECTOR_FILE*\nWORKDIR /var/tmp\n#   Edx Hadoop Util Library\nRUN git clone https://github.com/edx/edx-analytics-hadoop-util \\\n && cd /var/tmp/edx-analytics-hadoop-util \\\n && $JAVA_HOME/bin/javac -cp `/edx/app/hadoop/hadoop/bin/hadoop classpath ` org/edx/hadoop/input/ManifestTextInputFormat.java \\\n && $JAVA_HOME/bin/jar cf /edx/app/hadoop/lib/edx-analytics-hadoop-util.jar org/edx/hadoop/input/ManifestTextInputFormat.class\n#   configure bootstrap scripts for container\nADD docker/build/analytics_pipeline/bootstrap.sh /etc/bootstrap.sh\nRUN chown hadoop:hadoop /etc/bootstrap.sh \\\n && chmod 700 /etc/bootstrap.sh \\\n && chown -R hadoop:hadoop /edx/app/hadoop\n#   Analytics pipeline\nARG OPENEDX_RELEASE=master\nENV OPENEDX_RELEASE=\"${OPENEDX_RELEASE}\"\nRUN git clone https://github.com/edx/edx-analytics-pipeline \\\n && cd edx-analytics-pipeline \\\n && git checkout ${OPENEDX_RELEASE} \\\n && cd .. \\\n && cp /var/tmp/edx-analytics-pipeline/Makefile /var/tmp/Makefile \\\n && cp -r /var/tmp/edx-analytics-pipeline/requirements /var/tmp/requirements \\\n && rm -rf /var/tmp/edx-analytics-pipeline\nRUN pip install $COMMON_PIP_PACKAGES_PIP $COMMON_PIP_PACKAGES_SETUPTOOLS $COMMON_PIP_PACKAGES_VIRTUALENV $COMMON_PIP_PACKAGES_VIRTUALENVWRAPPER \\\n && virtualenv $ANALYTICS_PIPELINE_VENV/analytics_pipeline/ \\\n && chown -R hadoop:hadoop $ANALYTICS_PIPELINE_VENV/analytics_pipeline/ \\\n && echo '[hadoop]\\nversion: cdh4\\ncommand: /edx/app/hadoop/hadoop/bin/hadoop\\nstreaming-jar: /edx/app/hadoop/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar' > /etc/luigi/client.cfg\nRUN apt-get update \\\n && make system-requirements\nADD docker/build/analytics_pipeline/devstack.sh /edx/app/analytics_pipeline/devstack.sh\nRUN chown hadoop:hadoop /edx/app/analytics_pipeline/devstack.sh \\\n && chmod a+x /edx/app/analytics_pipeline/devstack.sh\nUSER hadoop\nRUN touch /edx/app/hadoop/.bashrc \\\n && echo 'export JAVA_HOME=/usr/lib/jvm/java-8-oracle\\nexport HADOOP_HOME=/edx/app/hadoop/hadoop\\nexport HIVE_HOME=/edx/app/hadoop/hive\\nexport SQOOP_HOME=/edx/app/hadoop/sqoop\\nexport SPARK_HOME=/edx/app/hadoop/spark\\nexport PATH=\"/edx/app/analytics_pipeline/venvs/analytics_pipeline/bin:${JAVA_HOME}/bin:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:${HIVE_HOME}/bin:${SPARK_HOME}/bin:${SPARK_HOME}/sbin:${SQOOP_HOME}/bin:$PATH\"' > /edx/app/hadoop/.bashrc \\\n && . $ANALYTICS_PIPELINE_VENV/analytics_pipeline/bin/activate \\\n && make test-requirements requirements\nRUN sudo chown hadoop:hadoop $COMMON_CFG_DIR/edx-analytics-pipeline/ \\\n && echo \"{\\\"username\\\": \\\"$COMMON_MYSQL_READ_ONLY_USER\\\", \\\"host\\\": \\\"mysql\\\", \\\"password\\\": \\\"$COMMON_MYSQL_READ_ONLY_PASS\\\", \\\"port\\\": 3306}\" > $COMMON_CFG_DIR/edx-analytics-pipeline/input.json \\\n && echo \"{\\\"username\\\": \\\"$ANALYTICS_PIPELINE_OUTPUT_DATABASE_USER\\\", \\\"host\\\": \\\"mysql\\\", \\\"password\\\": \\\"$ANALYTICS_PIPELINE_OUTPUT_DATABASE_PASSWORD\\\", \\\"port\\\": 3306}\" > $COMMON_CFG_DIR/edx-analytics-pipeline/output.json \\\n && echo \"{\\\"username\\\": \\\"dbadmin\\\", \\\"host\\\": \\\"vertica\\\", \\\"password\\\": \\\"\\\", \\\"port\\\": 5433}\" > $COMMON_CFG_DIR/edx-analytics-pipeline/warehouse.json\nADD docker/build/analytics_pipeline/acceptance.json $COMMON_CFG_DIR/edx-analytics-pipeline/acceptance.json\nWORKDIR /edx/app/analytics_pipeline/analytics_pipeline\nCMD [\"/etc/bootstrap.sh\", \"-d\"]\n","originalDockerfileUglifiedHash":"03296365b20ef80fd76f47a2f927df47","fileName":"/ICSME-replicationpackage/dataset/smelly_dockerfiles_bianncle/7dfd9d589cc97fa5c385de10231dbba4e26893b1.dockerfile"}