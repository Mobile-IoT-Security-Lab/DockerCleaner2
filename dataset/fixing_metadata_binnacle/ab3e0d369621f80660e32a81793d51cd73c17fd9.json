{"seed":4076818820,"processedDockerfileHash":"64625931eb0046b3f881c73eb53f0063","fixedSmells":["use-no-install-recommends","do-not-use-apt-get-update-alone","pin-package-manager-versions-apt-get","pin-package-manager-versions-pip","pin-package-manager-versions-npm","pin-package-manager-versions-gem","pin-package-manager-versions-apk","use-copy-instead-of-add","use-wget-instead-of-add","do-not-have-secrets","have-a-healthcheck","have-a-user"],"successfullyFixedSmells":["use-no-install-recommends","pin-package-manager-versions-apt-get","pin-package-manager-versions-pip","use-copy-instead-of-add","have-a-healthcheck","have-a-user"],"processedDockerfile":"FROM lablup/kernel-base:jail AS jail-builder\nFROM lablup/kernel-base:hook AS hook-builder\nFROM lablup/kernel-base:python3.6 AS python-binary\nFROM lablup/common-tensorflow:1.12-py36-srv AS tf-serving\nFROM nvidia/cuda:9.0-base-ubuntu16.04\nMAINTAINER Mario Cho \"m.cho@lablup.com\"\nENV LANG=\"C.UTF-8\"\nENV PYTHONUNBUFFERED=\"1\"\nENV NCCL_VERSION=\"2.2.13\"\nENV CUDNN_VERSION=\"7.2.1.38\"\nENV TF_TENSORRT_VERSION=\"4.1.2\"\nARG TF_SERVING_VERSION_GIT_BRANCH=master\nARG TF_SERVING_VERSION_GIT_COMMIT=head\nLABEL tensorflow_serving_github_branchtag=\"${TF_SERVING_VERSION_GIT_BRANCH}\"\nLABEL tensorflow_serving_github_commit=\"${TF_SERVING_VERSION_GIT_COMMIT}\"\nRUN apt-get update \\\n && apt-get install --no-install-recommends ca-certificates cuda-command-line-tools-9-0 cuda-command-line-tools-9-0 cuda-cublas-9-0 cuda-cufft-9-0 cuda-curand-9-0 cuda-cusolver-9-0 cuda-cusparse-9-0 libgomp1 wget libexpat1 libgdbm3 libbz2-dev libffi6 libsqlite3-0 liblzma5 zlib1g libmpdec2 libssl1.0.0 libssl-dev libncursesw5 libtinfo5 libreadline6 proj-bin libgeos-dev mime-support gcc g++ libproj-dev libgeos-dev libzmq3-dev libuv1 libcudnn7=${CUDNN_VERSION}-1+cuda9.0 libnccl2=${NCCL_VERSION}-1+cuda9.0 -y \\\n && apt-get clean \\\n && rm -rf /var/lib/apt/lists/*\nRUN apt-get update \\\n && apt-get install --no-install-recommends nvinfer-runtime-trt-repo-ubuntu1604-4.0.1-ga-cuda9.0 \\\n && apt-get update \\\n && apt-get install --no-install-recommends libnvinfer4=${TF_TENSORRT_VERSION}-1+cuda9.0 \\\n && apt-get clean \\\n && rm -rf /var/lib/apt/lists/* \\\n && rm /usr/lib/x86_64-linux-gnu/libnvinfer_plugin* \\\n && rm /usr/lib/x86_64-linux-gnu/libnvcaffe_parser* \\\n && rm /usr/lib/x86_64-linux-gnu/libnvparsers*\n#   Install TensorFlow-serving\nCOPY --from=tf-serving /usr/local/bin/tensorflow_model_server /usr/bin/tensorflow_model_server\n#   Expose ports\n#   gRPC\nEXPOSE 8500/tcp\n#   REST\nEXPOSE 8501/tcp\n#   Copy the whole Python from the docker library image\nCOPY --from=python-binary /python.tar.gz /\nRUN cd / ; tar xzpf python.tar.gz ; rm python.tar.gz ; ldconfig\nRUN export LD_LIBRARY_PATH=/usr/local/ssl/lib:$LD_LIBRARY_PATH\n#   Test if Python is working\nRUN python -c 'import sys; print(sys.version_info); import ssl'\n#   As we mostly have \"manylinux\" glibc-compatible binary packaes,\n#   we don't have to rebuild these!\nRUN pip install pyzmq==25.0.2 simplejson==3.19.1 msgpack-python==0.5.6 uvloop==0.17.0 --no-cache-dir \\\n && pip install aiozmq==1.0.0 dataclasses==0.8 tabulate==0.9.0 namedlist==1.8 six==1.16.0 \"python-dateutil>=2\" --no-cache-dir\n#   Install CUDA-9.0 + cuDNN 7.2\nRUN ln -s /usr/local/cuda-9.0 /usr/local/cuda \\\n && ln -s /usr/lib/x86_64-linux-gnu/libcudnn.so.7.2.1 /usr/local/cuda/lib64/libcudnn.so \\\n && ldconfig\nENV LD_LIBRARY_PATH=\"/usr/local/cuda/lib64:/usr/local/nvidia/lib64\" \\\n    PATH=\"/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n#   python package install\nRUN pip install wheel==0.40.0 --no-cache-dir \\\n && pip install pyzmq==25.0.2 simplejson==3.19.1 msgpack-python==0.5.6 uvloop==0.17.0 --no-cache-dir \\\n && pip install aiozmq==1.0.0 dataclasses==0.8 tabulate==0.9.0 namedlist==1.8 six==1.16.0 \"python-dateutil>=2\" --no-cache-dir \\\n && pip install keras==2.12.0 --no-cache-dir \\\n && pip install h5py==3.8.0 --no-cache-dir \\\n && pip install Cython==0.29.34 --no-cache-dir \\\n && pip install matplotlib==3.7.1 bokeh==3.1.0 --no-cache-dir \\\n && pip install pyproj==3.5.0 --no-cache-dir \\\n && pip install Cartopy==0.21.1 --no-cache-dir \\\n && pip install ipython==8.12.0 --no-cache-dir \\\n && pip install pandas==2.0.0 --no-cache-dir \\\n && pip install seaborn==0.12.2 --no-cache-dir \\\n && pip install pillow==9.5.0 --no-cache-dir \\\n && pip install networkx==3.1 cvxpy==1.3.1 --no-cache-dir \\\n && pip install scikit-learn==1.2.2 scikit-image==0.20.0 --no-cache-dir \\\n && pip install scikit-image==0.20.0 --no-cache-dir \\\n && pip install pygments==2.15.0 --no-cache-dir \\\n && pip install requests==2.28.2 --no-cache-dir \\\n && rm -f /tmp/*.whl\n#   Set where models should be stored in the container\n#  ENV MODEL_BASE_PATH=/home/work/models\n#  RUN mkdir -p ${MODEL_BASE_PATH}\n#   The only required piece is the model name in order to differentiate endpoints\n#  ENV MODEL_NAME=model\nRUN apt-get update \\\n && apt-get install --no-install-recommends libseccomp2 gosu -y \\\n && apt-get clean \\\n && rm -r /var/lib/apt/lists /var/cache/apt/archives \\\n && ln -s /usr/sbin/gosu /usr/sbin/su-exec \\\n && mkdir /home/work \\\n && chmod 755 /home/work ; mkdir /home/backend.ai \\\n && chmod 755 /home/backend.ai\nCOPY entrypoint.sh /usr/local/bin/entrypoint.sh\nRUN chmod +x /usr/local/bin/entrypoint.sh\n#   Create a script that runs the model server so we can use environment variables\n#   while also passing in arguments from the docker command line\n#  RUN echo '#!/bin/bash \\n\\n\\\n#      tensorflow_model_server --port=8500 --rest_api_port=8501 \\\n#          --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \\\n#          \"$@\"' >> /usr/local/bin/entrypoint.sh && \\\n#      chmod +x /usr/local/bin/entrypoint.sh\nENTRYPOINT [\"/usr/local/bin/entrypoint.sh\"]\nCOPY policy.yml /home/backend.ai/policy.yml\n#   Install jail\nCOPY --from=jail-builder /go/src/github.com/lablup/backend.ai-jail/backend.ai-jail /home/backend.ai/jail\nCOPY --from=hook-builder /root/backend.ai-hook/libbaihook.so /home/backend.ai/libbaihook.so\nENV LD_PRELOAD=\"/home/backend.ai/libbaihook.so\"\n#   Install kernel-runner scripts package\nRUN pip install \"backend.ai-kernel-runner[python]~=1.4.0\" --no-cache-dir\n#   Matplotlib configuration and pre-heating\nENV MPLCONFIGDIR=\"/home/backend.ai/.matplotlib\"\nRUN mkdir /home/backend.ai/.matplotlib\nCOPY matplotlibrc /home/backend.ai/.matplotlib/\nRUN echo 'import matplotlib.pyplot' > /tmp/matplotlib-fontcache.py \\\n && python /tmp/matplotlib-fontcache.py \\\n && rm /tmp/matplotlib-fontcache.py\nWORKDIR /home/work\nVOLUME [\"/home/work\"]\nEXPOSE 2000/tcp 2001/tcp 2002/tcp 2003/tcp\nLABEL ai.backend.nvidia.enabled=\"yes\" \\\n      com.nvidia.cuda.version=\"9.0.176\" \\\n      com.nvidia.volumes.needed=\"nvidia_driver\" \\\n      ai.backend.port=\"8500, 8501\" \\\n      ai.backend.timeout=\"0\" \\\n      ai.backend.maxmem=\"8g\" \\\n      ai.backend.maxcores=\"4\" \\\n      ai.backend.envs.corecount=\"OPENBLAS_NUM_THREADS,OMP_NUM_THREADS,NPROC\" \\\n      ai.backend.features=\"batch query uid-match user-input\"\nCMD [\"/home/backend.ai/jail\", \"-policy\", \"/home/backend.ai/policy.yml\", \"/usr/local/bin/python\", \"-m\", \"ai.backend.kernel\", \"python\"]\n#   vim: ft=dockerfile sts=4 sw=4 et tw=0\nRUN groupadd --system docker-user ; useradd --system --gid docker-user docker-user\nUSER docker-user\n# Please add your HEALTHCHECK here!!!\n","originalDockerfile":"FROM lablup/kernel-base:jail AS jail-builder\nFROM lablup/kernel-base:hook AS hook-builder\nFROM lablup/kernel-base:python3.6 AS python-binary\nFROM lablup/common-tensorflow:1.12-py36-srv AS tf-serving\nFROM nvidia/cuda:9.0-base-ubuntu16.04\nMAINTAINER Mario Cho \"m.cho@lablup.com\"\nENV LANG=\"C.UTF-8\"\nENV PYTHONUNBUFFERED=\"1\"\nENV NCCL_VERSION=\"2.2.13\"\nENV CUDNN_VERSION=\"7.2.1.38\"\nENV TF_TENSORRT_VERSION=\"4.1.2\"\nARG TF_SERVING_VERSION_GIT_BRANCH=master\nARG TF_SERVING_VERSION_GIT_COMMIT=head\nLABEL tensorflow_serving_github_branchtag=\"${TF_SERVING_VERSION_GIT_BRANCH}\"\nLABEL tensorflow_serving_github_commit=\"${TF_SERVING_VERSION_GIT_COMMIT}\"\nRUN apt-get update \\\n && apt-get install --no-install-recommends ca-certificates cuda-command-line-tools-9-0 cuda-command-line-tools-9-0 cuda-cublas-9-0 cuda-cufft-9-0 cuda-curand-9-0 cuda-cusolver-9-0 cuda-cusparse-9-0 libgomp1 wget libexpat1 libgdbm3 libbz2-dev libffi6 libsqlite3-0 liblzma5 zlib1g libmpdec2 libssl1.0.0 libssl-dev libncursesw5 libtinfo5 libreadline6 proj-bin libgeos-dev mime-support gcc g++ libproj-dev libgeos-dev libzmq3-dev libuv1 libcudnn7=${CUDNN_VERSION}-1+cuda9.0 libnccl2=${NCCL_VERSION}-1+cuda9.0 -y \\\n && apt-get clean \\\n && rm -rf /var/lib/apt/lists/*\nRUN apt-get update \\\n && apt-get install --no-install-recommends nvinfer-runtime-trt-repo-ubuntu1604-4.0.1-ga-cuda9.0 \\\n && apt-get update \\\n && apt-get install --no-install-recommends libnvinfer4=${TF_TENSORRT_VERSION}-1+cuda9.0 \\\n && apt-get clean \\\n && rm -rf /var/lib/apt/lists/* \\\n && rm /usr/lib/x86_64-linux-gnu/libnvinfer_plugin* \\\n && rm /usr/lib/x86_64-linux-gnu/libnvcaffe_parser* \\\n && rm /usr/lib/x86_64-linux-gnu/libnvparsers*\n#  Install TensorFlow-serving\nCOPY --from=tf-serving /usr/local/bin/tensorflow_model_server /usr/bin/tensorflow_model_server\n#  Expose ports\n#  gRPC\nEXPOSE 8500/tcp\n#  REST\nEXPOSE 8501/tcp\n#  Copy the whole Python from the docker library image\nCOPY --from=python-binary /python.tar.gz /\nRUN cd / ; tar xzpf python.tar.gz ; rm python.tar.gz ; ldconfig\nRUN export LD_LIBRARY_PATH=/usr/local/ssl/lib:$LD_LIBRARY_PATH\n#  Test if Python is working\nRUN python -c 'import sys; print(sys.version_info); import ssl'\n#  As we mostly have \"manylinux\" glibc-compatible binary packaes,\n#  we don't have to rebuild these!\nRUN pip install pyzmq simplejson msgpack-python uvloop --no-cache-dir \\\n && pip install aiozmq dataclasses tabulate namedlist six \"python-dateutil>=2\" --no-cache-dir\n#  Install CUDA-9.0 + cuDNN 7.2\nRUN ln -s /usr/local/cuda-9.0 /usr/local/cuda \\\n && ln -s /usr/lib/x86_64-linux-gnu/libcudnn.so.7.2.1 /usr/local/cuda/lib64/libcudnn.so \\\n && ldconfig\nENV LD_LIBRARY_PATH=\"/usr/local/cuda/lib64:/usr/local/nvidia/lib64\" \\\n    PATH=\"/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n#  python package install\nRUN pip install wheel --no-cache-dir \\\n && pip install pyzmq simplejson msgpack-python uvloop --no-cache-dir \\\n && pip install aiozmq dataclasses tabulate namedlist six \"python-dateutil>=2\" --no-cache-dir \\\n && pip install keras --no-cache-dir \\\n && pip install h5py --no-cache-dir \\\n && pip install Cython --no-cache-dir \\\n && pip install matplotlib bokeh --no-cache-dir \\\n && pip install pyproj --no-cache-dir \\\n && pip install Cartopy --no-cache-dir \\\n && pip install ipython --no-cache-dir \\\n && pip install pandas --no-cache-dir \\\n && pip install seaborn --no-cache-dir \\\n && pip install pillow --no-cache-dir \\\n && pip install networkx cvxpy --no-cache-dir \\\n && pip install scikit-learn scikit-image --no-cache-dir \\\n && pip install scikit-image --no-cache-dir \\\n && pip install pygments --no-cache-dir \\\n && pip install requests --no-cache-dir \\\n && rm -f /tmp/*.whl\n#  Set where models should be stored in the container\n# ENV MODEL_BASE_PATH=/home/work/models\n# RUN mkdir -p ${MODEL_BASE_PATH}\n#  The only required piece is the model name in order to differentiate endpoints\n# ENV MODEL_NAME=model\nRUN apt-get update \\\n && apt-get install libseccomp2 gosu -y \\\n && apt-get clean \\\n && rm -r /var/lib/apt/lists /var/cache/apt/archives \\\n && ln -s /usr/sbin/gosu /usr/sbin/su-exec \\\n && mkdir /home/work \\\n && chmod 755 /home/work ; mkdir /home/backend.ai \\\n && chmod 755 /home/backend.ai\nADD entrypoint.sh /usr/local/bin/entrypoint.sh\nRUN chmod +x /usr/local/bin/entrypoint.sh\n#  Create a script that runs the model server so we can use environment variables\n#  while also passing in arguments from the docker command line\n# RUN echo '#!/bin/bash \\n\\n\\\n#     tensorflow_model_server --port=8500 --rest_api_port=8501 \\\n#         --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \\\n#         \"$@\"' >> /usr/local/bin/entrypoint.sh && \\\n#     chmod +x /usr/local/bin/entrypoint.sh\nENTRYPOINT [\"/usr/local/bin/entrypoint.sh\"]\nCOPY policy.yml /home/backend.ai/policy.yml\n#  Install jail\nCOPY --from=jail-builder /go/src/github.com/lablup/backend.ai-jail/backend.ai-jail /home/backend.ai/jail\nCOPY --from=hook-builder /root/backend.ai-hook/libbaihook.so /home/backend.ai/libbaihook.so\nENV LD_PRELOAD=\"/home/backend.ai/libbaihook.so\"\n#  Install kernel-runner scripts package\nRUN pip install \"backend.ai-kernel-runner[python]~=1.4.0\" --no-cache-dir\n#  Matplotlib configuration and pre-heating\nENV MPLCONFIGDIR=\"/home/backend.ai/.matplotlib\"\nRUN mkdir /home/backend.ai/.matplotlib\nCOPY matplotlibrc /home/backend.ai/.matplotlib/\nRUN echo 'import matplotlib.pyplot' > /tmp/matplotlib-fontcache.py \\\n && python /tmp/matplotlib-fontcache.py \\\n && rm /tmp/matplotlib-fontcache.py\nWORKDIR /home/work\nVOLUME [\"/home/work\"]\nEXPOSE 2000/tcp 2001/tcp 2002/tcp 2003/tcp\nLABEL ai.backend.nvidia.enabled=\"yes\" \\\n      com.nvidia.cuda.version=\"9.0.176\" \\\n      com.nvidia.volumes.needed=\"nvidia_driver\" \\\n      ai.backend.port=\"8500, 8501\" \\\n      ai.backend.timeout=\"0\" \\\n      ai.backend.maxmem=\"8g\" \\\n      ai.backend.maxcores=\"4\" \\\n      ai.backend.envs.corecount=\"OPENBLAS_NUM_THREADS,OMP_NUM_THREADS,NPROC\" \\\n      ai.backend.features=\"batch query uid-match user-input\"\nCMD [\"/home/backend.ai/jail\", \"-policy\", \"/home/backend.ai/policy.yml\", \"/usr/local/bin/python\", \"-m\", \"ai.backend.kernel\", \"python\"]\n#  vim: ft=dockerfile sts=4 sw=4 et tw=0\n","injectedSmells":[],"originalDockerfileHash":"746fe0281b9daedcc7bb9ce5f2ff5184","successfullyInjectedSmells":[],"originalDockerfileUglified":"FROM lablup/kernel-base:jail AS jail-builder\nFROM lablup/kernel-base:hook AS hook-builder\nFROM lablup/kernel-base:python3.6 AS python-binary\nFROM lablup/common-tensorflow:1.12-py36-srv AS tf-serving\nFROM nvidia/cuda:9.0-base-ubuntu16.04\nMAINTAINER Mario Cho \"m.cho@lablup.com\"\nENV LANG=\"C.UTF-8\"\nENV PYTHONUNBUFFERED=\"1\"\nENV NCCL_VERSION=\"2.2.13\"\nENV CUDNN_VERSION=\"7.2.1.38\"\nENV TF_TENSORRT_VERSION=\"4.1.2\"\nARG TF_SERVING_VERSION_GIT_BRANCH=master\nARG TF_SERVING_VERSION_GIT_COMMIT=head\nLABEL tensorflow_serving_github_branchtag=\"${TF_SERVING_VERSION_GIT_BRANCH}\"\nLABEL tensorflow_serving_github_commit=\"${TF_SERVING_VERSION_GIT_COMMIT}\"\nRUN apt-get update \\\n && apt-get install --no-install-recommends ca-certificates cuda-command-line-tools-9-0 cuda-command-line-tools-9-0 cuda-cublas-9-0 cuda-cufft-9-0 cuda-curand-9-0 cuda-cusolver-9-0 cuda-cusparse-9-0 libgomp1 wget libexpat1 libgdbm3 libbz2-dev libffi6 libsqlite3-0 liblzma5 zlib1g libmpdec2 libssl1.0.0 libssl-dev libncursesw5 libtinfo5 libreadline6 proj-bin libgeos-dev mime-support gcc g++ libproj-dev libgeos-dev libzmq3-dev libuv1 libcudnn7=${CUDNN_VERSION}-1+cuda9.0 libnccl2=${NCCL_VERSION}-1+cuda9.0 -y \\\n && apt-get clean \\\n && rm -rf /var/lib/apt/lists/*\nRUN apt-get update \\\n && apt-get install --no-install-recommends nvinfer-runtime-trt-repo-ubuntu1604-4.0.1-ga-cuda9.0 \\\n && apt-get update \\\n && apt-get install --no-install-recommends libnvinfer4=${TF_TENSORRT_VERSION}-1+cuda9.0 \\\n && apt-get clean \\\n && rm -rf /var/lib/apt/lists/* \\\n && rm /usr/lib/x86_64-linux-gnu/libnvinfer_plugin* \\\n && rm /usr/lib/x86_64-linux-gnu/libnvcaffe_parser* \\\n && rm /usr/lib/x86_64-linux-gnu/libnvparsers*\n#   Install TensorFlow-serving\nCOPY --from=tf-serving /usr/local/bin/tensorflow_model_server /usr/bin/tensorflow_model_server\n#   Expose ports\n#   gRPC\nEXPOSE 8500/tcp\n#   REST\nEXPOSE 8501/tcp\n#   Copy the whole Python from the docker library image\nCOPY --from=python-binary /python.tar.gz /\nRUN cd / ; tar xzpf python.tar.gz ; rm python.tar.gz ; ldconfig\nRUN export LD_LIBRARY_PATH=/usr/local/ssl/lib:$LD_LIBRARY_PATH\n#   Test if Python is working\nRUN python -c 'import sys; print(sys.version_info); import ssl'\n#   As we mostly have \"manylinux\" glibc-compatible binary packaes,\n#   we don't have to rebuild these!\nRUN pip install pyzmq simplejson msgpack-python uvloop --no-cache-dir \\\n && pip install aiozmq dataclasses tabulate namedlist six \"python-dateutil>=2\" --no-cache-dir\n#   Install CUDA-9.0 + cuDNN 7.2\nRUN ln -s /usr/local/cuda-9.0 /usr/local/cuda \\\n && ln -s /usr/lib/x86_64-linux-gnu/libcudnn.so.7.2.1 /usr/local/cuda/lib64/libcudnn.so \\\n && ldconfig\nENV LD_LIBRARY_PATH=\"/usr/local/cuda/lib64:/usr/local/nvidia/lib64\" \\\n    PATH=\"/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n#   python package install\nRUN pip install wheel --no-cache-dir \\\n && pip install pyzmq simplejson msgpack-python uvloop --no-cache-dir \\\n && pip install aiozmq dataclasses tabulate namedlist six \"python-dateutil>=2\" --no-cache-dir \\\n && pip install keras --no-cache-dir \\\n && pip install h5py --no-cache-dir \\\n && pip install Cython --no-cache-dir \\\n && pip install matplotlib bokeh --no-cache-dir \\\n && pip install pyproj --no-cache-dir \\\n && pip install Cartopy --no-cache-dir \\\n && pip install ipython --no-cache-dir \\\n && pip install pandas --no-cache-dir \\\n && pip install seaborn --no-cache-dir \\\n && pip install pillow --no-cache-dir \\\n && pip install networkx cvxpy --no-cache-dir \\\n && pip install scikit-learn scikit-image --no-cache-dir \\\n && pip install scikit-image --no-cache-dir \\\n && pip install pygments --no-cache-dir \\\n && pip install requests --no-cache-dir \\\n && rm -f /tmp/*.whl\n#   Set where models should be stored in the container\n#  ENV MODEL_BASE_PATH=/home/work/models\n#  RUN mkdir -p ${MODEL_BASE_PATH}\n#   The only required piece is the model name in order to differentiate endpoints\n#  ENV MODEL_NAME=model\nRUN apt-get update \\\n && apt-get install libseccomp2 gosu -y \\\n && apt-get clean \\\n && rm -r /var/lib/apt/lists /var/cache/apt/archives \\\n && ln -s /usr/sbin/gosu /usr/sbin/su-exec \\\n && mkdir /home/work \\\n && chmod 755 /home/work ; mkdir /home/backend.ai \\\n && chmod 755 /home/backend.ai\nADD entrypoint.sh /usr/local/bin/entrypoint.sh\nRUN chmod +x /usr/local/bin/entrypoint.sh\n#   Create a script that runs the model server so we can use environment variables\n#   while also passing in arguments from the docker command line\n#  RUN echo '#!/bin/bash \\n\\n\\\n#      tensorflow_model_server --port=8500 --rest_api_port=8501 \\\n#          --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \\\n#          \"$@\"' >> /usr/local/bin/entrypoint.sh && \\\n#      chmod +x /usr/local/bin/entrypoint.sh\nENTRYPOINT [\"/usr/local/bin/entrypoint.sh\"]\nCOPY policy.yml /home/backend.ai/policy.yml\n#   Install jail\nCOPY --from=jail-builder /go/src/github.com/lablup/backend.ai-jail/backend.ai-jail /home/backend.ai/jail\nCOPY --from=hook-builder /root/backend.ai-hook/libbaihook.so /home/backend.ai/libbaihook.so\nENV LD_PRELOAD=\"/home/backend.ai/libbaihook.so\"\n#   Install kernel-runner scripts package\nRUN pip install \"backend.ai-kernel-runner[python]~=1.4.0\" --no-cache-dir\n#   Matplotlib configuration and pre-heating\nENV MPLCONFIGDIR=\"/home/backend.ai/.matplotlib\"\nRUN mkdir /home/backend.ai/.matplotlib\nCOPY matplotlibrc /home/backend.ai/.matplotlib/\nRUN echo 'import matplotlib.pyplot' > /tmp/matplotlib-fontcache.py \\\n && python /tmp/matplotlib-fontcache.py \\\n && rm /tmp/matplotlib-fontcache.py\nWORKDIR /home/work\nVOLUME [\"/home/work\"]\nEXPOSE 2000/tcp 2001/tcp 2002/tcp 2003/tcp\nLABEL ai.backend.nvidia.enabled=\"yes\" \\\n      com.nvidia.cuda.version=\"9.0.176\" \\\n      com.nvidia.volumes.needed=\"nvidia_driver\" \\\n      ai.backend.port=\"8500, 8501\" \\\n      ai.backend.timeout=\"0\" \\\n      ai.backend.maxmem=\"8g\" \\\n      ai.backend.maxcores=\"4\" \\\n      ai.backend.envs.corecount=\"OPENBLAS_NUM_THREADS,OMP_NUM_THREADS,NPROC\" \\\n      ai.backend.features=\"batch query uid-match user-input\"\nCMD [\"/home/backend.ai/jail\", \"-policy\", \"/home/backend.ai/policy.yml\", \"/usr/local/bin/python\", \"-m\", \"ai.backend.kernel\", \"python\"]\n#   vim: ft=dockerfile sts=4 sw=4 et tw=0\n","originalDockerfileUglifiedHash":"dab0d69eda719630de545ababf8306c0","fileName":"/ICSME-replicationpackage/dataset/smelly_dockerfiles_bianncle/ab3e0d369621f80660e32a81793d51cd73c17fd9.dockerfile"}