{"seed":1932038553,"processedDockerfileHash":"922734de09d54f06aed21eef29d7fd79","fixedSmells":["use-no-install-recommends","do-not-use-apt-get-update-alone","pin-package-manager-versions-apt-get","pin-package-manager-versions-pip","pin-package-manager-versions-npm","pin-package-manager-versions-gem","pin-package-manager-versions-apk","use-copy-instead-of-add","use-wget-instead-of-add","do-not-have-secrets","have-a-healthcheck","have-a-user"],"successfullyFixedSmells":["do-not-use-apt-get-update-alone","pin-package-manager-versions-pip","have-a-healthcheck","have-a-user"],"processedDockerfile":"FROM python:2.7-stretch AS no-spark\n#   Setup airflow\nRUN set -ex \\\n && (echo 'deb http://deb.debian.org/debian stretch-backports main' > /etc/apt/sources.list.d/backports.list) \\\n && : \\\n && DEBIAN_FRONTEND=noninteractive apt-get install --no-install-recommends -y --force-yes build-essential libkrb5-dev libsasl2-dev libffi-dev default-libmysqlclient-dev vim-tiny gosu krb5-user \\\n && apt-get purge --auto-remove -yqq \\\n && apt-get autoremove -yqq --purge \\\n && apt-get clean \\\n && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/* /usr/share/doc /usr/share/doc-base \\\n && pip install psycopg2==2.9.6 \"apache-airflow[devel_hadoop,crypto,celery,redis,postgres,jdbc,ssh]==1.10.3\" --no-cache-dir\nARG airflow_home=/airflow\nENV AIRFLOW_HOME=\"${airflow_home}\"\nWORKDIR ${AIRFLOW_HOME}\n#   Setup airflow dags path\nENV AIRFLOW_DAG=\"${AIRFLOW_HOME}/dags\"\nRUN mkdir -p ${AIRFLOW_DAG}\nCOPY airflow.cfg ${AIRFLOW_HOME}/airflow.cfg\nCOPY unittests.cfg ${AIRFLOW_HOME}/unittests.cfg\nCOPY webserver_config.py ${AIRFLOW_HOME}/webserver_config.py\n#   Create default user and group\nARG user=afpuser\nENV USER=\"${user}\"\nARG group=hadoop\nENV GROUP=\"${group}\"\nRUN groupadd -r \"${GROUP}\" \\\n && useradd -rmg \"${GROUP}\" \"${USER}\"\n#   Number of times the Airflow scheduler will run before it terminates (and restarts)\nARG scheduler_runs=5\nENV SCHEDULER_RUNS=\"${scheduler_runs}\"\n#   parallelism = number of physical python processes the scheduler can run\nARG airflow__core__parallelism=8\nENV AIRFLOW__CORE__PARALLELISM=\"${airflow__core__parallelism}\"\n#   dag_concurrency = the number of TIs to be allowed to run PER-dag at once\nARG airflow__core__dag_concurrency=6\nENV AIRFLOW__CORE__DAG_CONCURRENCY=\"${airflow__core__dag_concurrency}\"\n#   max_threads = number of processes to parallelize the scheduler over, cannot exceed the cpu count\nARG airflow__scheduler__max_threads=4\nENV AIRFLOW__SCHEDULER__MAX_THREADS=\"${airflow__scheduler__max_threads}\"\nENV AIRFLOW__CORE__EXECUTOR=\"LocalExecutor\"\nWORKDIR ${AIRFLOW_HOME}\n#   Setup pipeline dependencies\nCOPY requirements.txt ${AIRFLOW_HOME}/requirements.txt\nRUN pip install -r \"${AIRFLOW_HOME}/requirements.txt\"\n#   For optional S3 logging\nCOPY ./config/ ${AIRFLOW_HOME}/config/\nCOPY entrypoint.sh /entrypoint.sh\nENTRYPOINT [\"/entrypoint.sh\"]\nFROM no-spark AS with-spark-optional-dag\n#   Install Java\nRUN apt-get update \\\n && (apt-get update ;apt-get install --no-install-recommends openjdk-8-jre-headless -y ) \\\n && rm -rf /var/lib/apt/lists/*\nENV JAVA_HOME=\"/usr/lib/jvm/java-8-openjdk-amd64\"\nARG SPARK_VERSION=2.1.2\nARG HADOOP_VERSION=2.6.5\nARG SPARK_PY4J=python/lib/py4j-0.10.4-src.zip\nARG hadoop_home=/opt/hadoop\nENV HADOOP_HOME=\"${hadoop_home}\"\nENV PATH=\"${PATH}:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin\"\nENV SPARK_HOME=\"/opt/spark-${SPARK_VERSION}\"\nENV PATH=\"$PATH:${SPARK_HOME}/bin\"\nENV PYTHONPATH=\"${SPARK_HOME}/${SPARK_PY4J}:${SPARK_HOME}/python\"\nENV PYSPARK_SUBMIT_ARGS=\"--driver-memory 8g --py-files ${SPARK_HOME}/python/lib/pyspark.zip pyspark-shell\"\n#   Download Spark\nARG SPARK_EXTRACT_LOC=/sparkbin\nRUN [\"/bin/bash\", \"-c\", \"set\", \"-eoux\", \"pipefail\", \"&&\", \"(curl\", \"https://archive.apache.org/dist/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz\", \"|\", \"tar\", \"-xz\", \"-C\", \"/opt/\", \")\", \"&&\", \"mv\", \"/opt/hadoop-${HADOOP_VERSION}\", \"/opt/hadoop\", \"&&\", \"mkdir\", \"-p\", \"${SPARK_EXTRACT_LOC}\", \"&&\", \"(curl\", \"https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION:0:3}.tgz\", \"|\", \"tar\", \"-xz\", \"-C\", \"${SPARK_EXTRACT_LOC}\", \")\", \"&&\", \"mkdir\", \"-p\", \"${SPARK_HOME}\", \"&&\", \"mv\", \"${SPARK_EXTRACT_LOC}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION:0:3}/*\", \"${SPARK_HOME}\", \"&&\", \"rm\", \"-rf\", \"${SPARK_EXTRACT_LOC}\", \"&&\", \"echo\", \"SPARK_HOME\", \"is\", \"${SPARK_HOME}\", \"&&\", \"ls\", \"-al\", \"--g\", \"${SPARK_HOME}\"]\n#   Less verbose logging\nCOPY log4j.properties.production ${SPARK_HOME}/conf/log4j.properties\nFROM with-spark-optional-dag AS with-spark\n#  # To build your own image:\nONBUILD COPY dags/ ${AIRFLOW_DAG}\nRUN groupadd --system docker-user ; useradd --system --gid docker-user docker-user\nUSER docker-user\n# Please add your HEALTHCHECK here!!!\n","originalDockerfile":"FROM python:2.7-stretch AS no-spark\n#  Setup airflow\nRUN set -ex \\\n && (echo 'deb http://deb.debian.org/debian stretch-backports main' > /etc/apt/sources.list.d/backports.list) \\\n && apt-get update \\\n && DEBIAN_FRONTEND=noninteractive apt-get install --no-install-recommends -y --force-yes build-essential libkrb5-dev libsasl2-dev libffi-dev default-libmysqlclient-dev vim-tiny gosu krb5-user \\\n && apt-get purge --auto-remove -yqq \\\n && apt-get autoremove -yqq --purge \\\n && apt-get clean \\\n && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/* /usr/share/doc /usr/share/doc-base \\\n && pip install psycopg2 \"apache-airflow[devel_hadoop,crypto,celery,redis,postgres,jdbc,ssh]==1.10.3\" --no-cache-dir\nARG airflow_home=/airflow\nENV AIRFLOW_HOME=\"${airflow_home}\"\nWORKDIR ${AIRFLOW_HOME}\n#  Setup airflow dags path\nENV AIRFLOW_DAG=\"${AIRFLOW_HOME}/dags\"\nRUN mkdir -p ${AIRFLOW_DAG}\nCOPY airflow.cfg ${AIRFLOW_HOME}/airflow.cfg\nCOPY unittests.cfg ${AIRFLOW_HOME}/unittests.cfg\nCOPY webserver_config.py ${AIRFLOW_HOME}/webserver_config.py\n#  Create default user and group\nARG user=afpuser\nENV USER=\"${user}\"\nARG group=hadoop\nENV GROUP=\"${group}\"\nRUN groupadd -r \"${GROUP}\" \\\n && useradd -rmg \"${GROUP}\" \"${USER}\"\n#  Number of times the Airflow scheduler will run before it terminates (and restarts)\nARG scheduler_runs=5\nENV SCHEDULER_RUNS=\"${scheduler_runs}\"\n#  parallelism = number of physical python processes the scheduler can run\nARG airflow__core__parallelism=8\nENV AIRFLOW__CORE__PARALLELISM=\"${airflow__core__parallelism}\"\n#  dag_concurrency = the number of TIs to be allowed to run PER-dag at once\nARG airflow__core__dag_concurrency=6\nENV AIRFLOW__CORE__DAG_CONCURRENCY=\"${airflow__core__dag_concurrency}\"\n#  max_threads = number of processes to parallelize the scheduler over, cannot exceed the cpu count\nARG airflow__scheduler__max_threads=4\nENV AIRFLOW__SCHEDULER__MAX_THREADS=\"${airflow__scheduler__max_threads}\"\nENV AIRFLOW__CORE__EXECUTOR=\"LocalExecutor\"\nWORKDIR ${AIRFLOW_HOME}\n#  Setup pipeline dependencies\nCOPY requirements.txt ${AIRFLOW_HOME}/requirements.txt\nRUN pip install -r \"${AIRFLOW_HOME}/requirements.txt\"\n#  For optional S3 logging\nCOPY ./config/ ${AIRFLOW_HOME}/config/\nCOPY entrypoint.sh /entrypoint.sh\nENTRYPOINT [\"/entrypoint.sh\"]\nFROM no-spark AS with-spark-optional-dag\n#  Install Java\nRUN apt-get update \\\n && apt-get install --no-install-recommends openjdk-8-jre-headless -y \\\n && rm -rf /var/lib/apt/lists/*\nENV JAVA_HOME=\"/usr/lib/jvm/java-8-openjdk-amd64\"\nARG SPARK_VERSION=2.1.2\nARG HADOOP_VERSION=2.6.5\nARG SPARK_PY4J=python/lib/py4j-0.10.4-src.zip\nARG hadoop_home=/opt/hadoop\nENV HADOOP_HOME=\"${hadoop_home}\"\nENV PATH=\"${PATH}:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin\"\nENV SPARK_HOME=\"/opt/spark-${SPARK_VERSION}\"\nENV PATH=\"$PATH:${SPARK_HOME}/bin\"\nENV PYTHONPATH=\"${SPARK_HOME}/${SPARK_PY4J}:${SPARK_HOME}/python\"\nENV PYSPARK_SUBMIT_ARGS=\"--driver-memory 8g --py-files ${SPARK_HOME}/python/lib/pyspark.zip pyspark-shell\"\n#  Download Spark\nARG SPARK_EXTRACT_LOC=/sparkbin\nRUN [\"/bin/bash\", \"-c\", \"set\", \"-eoux\", \"pipefail\", \"&&\", \"(curl\", \"https://archive.apache.org/dist/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz\", \"|\", \"tar\", \"-xz\", \"-C\", \"/opt/\", \")\", \"&&\", \"mv\", \"/opt/hadoop-${HADOOP_VERSION}\", \"/opt/hadoop\", \"&&\", \"mkdir\", \"-p\", \"${SPARK_EXTRACT_LOC}\", \"&&\", \"(curl\", \"https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION:0:3}.tgz\", \"|\", \"tar\", \"-xz\", \"-C\", \"${SPARK_EXTRACT_LOC}\", \")\", \"&&\", \"mkdir\", \"-p\", \"${SPARK_HOME}\", \"&&\", \"mv\", \"${SPARK_EXTRACT_LOC}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION:0:3}/*\", \"${SPARK_HOME}\", \"&&\", \"rm\", \"-rf\", \"${SPARK_EXTRACT_LOC}\", \"&&\", \"echo\", \"SPARK_HOME\", \"is\", \"${SPARK_HOME}\", \"&&\", \"ls\", \"-al\", \"--g\", \"${SPARK_HOME}\"]\n#  Less verbose logging\nCOPY log4j.properties.production ${SPARK_HOME}/conf/log4j.properties\nFROM with-spark-optional-dag AS with-spark\n# # To build your own image:\nONBUILD COPY dags/ ${AIRFLOW_DAG}\n","injectedSmells":[],"originalDockerfileHash":"c4c255c9cb97ea23908dab4073289199","successfullyInjectedSmells":[],"originalDockerfileUglified":"FROM python:2.7-stretch AS no-spark\n#   Setup airflow\nRUN set -ex \\\n && (echo 'deb http://deb.debian.org/debian stretch-backports main' > /etc/apt/sources.list.d/backports.list) \\\n && apt-get update \\\n && DEBIAN_FRONTEND=noninteractive apt-get install --no-install-recommends -y --force-yes build-essential libkrb5-dev libsasl2-dev libffi-dev default-libmysqlclient-dev vim-tiny gosu krb5-user \\\n && apt-get purge --auto-remove -yqq \\\n && apt-get autoremove -yqq --purge \\\n && apt-get clean \\\n && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/* /usr/share/doc /usr/share/doc-base \\\n && pip install psycopg2 \"apache-airflow[devel_hadoop,crypto,celery,redis,postgres,jdbc,ssh]==1.10.3\" --no-cache-dir\nARG airflow_home=/airflow\nENV AIRFLOW_HOME=\"${airflow_home}\"\nWORKDIR ${AIRFLOW_HOME}\n#   Setup airflow dags path\nENV AIRFLOW_DAG=\"${AIRFLOW_HOME}/dags\"\nRUN mkdir -p ${AIRFLOW_DAG}\nCOPY airflow.cfg ${AIRFLOW_HOME}/airflow.cfg\nCOPY unittests.cfg ${AIRFLOW_HOME}/unittests.cfg\nCOPY webserver_config.py ${AIRFLOW_HOME}/webserver_config.py\n#   Create default user and group\nARG user=afpuser\nENV USER=\"${user}\"\nARG group=hadoop\nENV GROUP=\"${group}\"\nRUN groupadd -r \"${GROUP}\" \\\n && useradd -rmg \"${GROUP}\" \"${USER}\"\n#   Number of times the Airflow scheduler will run before it terminates (and restarts)\nARG scheduler_runs=5\nENV SCHEDULER_RUNS=\"${scheduler_runs}\"\n#   parallelism = number of physical python processes the scheduler can run\nARG airflow__core__parallelism=8\nENV AIRFLOW__CORE__PARALLELISM=\"${airflow__core__parallelism}\"\n#   dag_concurrency = the number of TIs to be allowed to run PER-dag at once\nARG airflow__core__dag_concurrency=6\nENV AIRFLOW__CORE__DAG_CONCURRENCY=\"${airflow__core__dag_concurrency}\"\n#   max_threads = number of processes to parallelize the scheduler over, cannot exceed the cpu count\nARG airflow__scheduler__max_threads=4\nENV AIRFLOW__SCHEDULER__MAX_THREADS=\"${airflow__scheduler__max_threads}\"\nENV AIRFLOW__CORE__EXECUTOR=\"LocalExecutor\"\nWORKDIR ${AIRFLOW_HOME}\n#   Setup pipeline dependencies\nCOPY requirements.txt ${AIRFLOW_HOME}/requirements.txt\nRUN pip install -r \"${AIRFLOW_HOME}/requirements.txt\"\n#   For optional S3 logging\nCOPY ./config/ ${AIRFLOW_HOME}/config/\nCOPY entrypoint.sh /entrypoint.sh\nENTRYPOINT [\"/entrypoint.sh\"]\nFROM no-spark AS with-spark-optional-dag\n#   Install Java\nRUN apt-get update \\\n && apt-get install --no-install-recommends openjdk-8-jre-headless -y \\\n && rm -rf /var/lib/apt/lists/*\nENV JAVA_HOME=\"/usr/lib/jvm/java-8-openjdk-amd64\"\nARG SPARK_VERSION=2.1.2\nARG HADOOP_VERSION=2.6.5\nARG SPARK_PY4J=python/lib/py4j-0.10.4-src.zip\nARG hadoop_home=/opt/hadoop\nENV HADOOP_HOME=\"${hadoop_home}\"\nENV PATH=\"${PATH}:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin\"\nENV SPARK_HOME=\"/opt/spark-${SPARK_VERSION}\"\nENV PATH=\"$PATH:${SPARK_HOME}/bin\"\nENV PYTHONPATH=\"${SPARK_HOME}/${SPARK_PY4J}:${SPARK_HOME}/python\"\nENV PYSPARK_SUBMIT_ARGS=\"--driver-memory 8g --py-files ${SPARK_HOME}/python/lib/pyspark.zip pyspark-shell\"\n#   Download Spark\nARG SPARK_EXTRACT_LOC=/sparkbin\nRUN [\"/bin/bash\", \"-c\", \"set\", \"-eoux\", \"pipefail\", \"&&\", \"(curl\", \"https://archive.apache.org/dist/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz\", \"|\", \"tar\", \"-xz\", \"-C\", \"/opt/\", \")\", \"&&\", \"mv\", \"/opt/hadoop-${HADOOP_VERSION}\", \"/opt/hadoop\", \"&&\", \"mkdir\", \"-p\", \"${SPARK_EXTRACT_LOC}\", \"&&\", \"(curl\", \"https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION:0:3}.tgz\", \"|\", \"tar\", \"-xz\", \"-C\", \"${SPARK_EXTRACT_LOC}\", \")\", \"&&\", \"mkdir\", \"-p\", \"${SPARK_HOME}\", \"&&\", \"mv\", \"${SPARK_EXTRACT_LOC}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION:0:3}/*\", \"${SPARK_HOME}\", \"&&\", \"rm\", \"-rf\", \"${SPARK_EXTRACT_LOC}\", \"&&\", \"echo\", \"SPARK_HOME\", \"is\", \"${SPARK_HOME}\", \"&&\", \"ls\", \"-al\", \"--g\", \"${SPARK_HOME}\"]\n#   Less verbose logging\nCOPY log4j.properties.production ${SPARK_HOME}/conf/log4j.properties\nFROM with-spark-optional-dag AS with-spark\n#  # To build your own image:\nONBUILD COPY dags/ ${AIRFLOW_DAG}\n","originalDockerfileUglifiedHash":"e88c50543969e5c34e40ad4c4ebbe89d","fileName":"/ICSME-replicationpackage/dataset/smelly_dockerfiles_bianncle/e8556239dd29244190c46076146bcad2e30d7297.dockerfile"}